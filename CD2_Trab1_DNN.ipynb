{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "oABS6IdWihVV",
      "metadata": {
        "id": "oABS6IdWihVV"
      },
      "source": [
        "# 1 - Importar Bibliotecas(deve ser rodado para gerar o CSV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_K8Av1MbUmGk",
      "metadata": {
        "id": "_K8Av1MbUmGk"
      },
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import copy\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tqdm\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import (KFold, StratifiedKFold, cross_val_score,\n",
        "                                     train_test_split)\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rp41VhQ87c0A",
      "metadata": {
        "id": "Rp41VhQ87c0A"
      },
      "source": [
        "#2 - Tratamento da entrada(deve ser rodado para gerar o CSV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UsbWTBz88BsM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsbWTBz88BsM",
        "outputId": "a6bef18d-bcf8-4c73-ef1c-8425a99bbd69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1i3YKPA5BqBYvOptIAXqC8X7lIyluf8bI\n",
            "To: /content/dados_treino_alunos.csv\n",
            "100% 2.79M/2.79M [00:00<00:00, 293MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1eWweaJM4jemsM05cC-mmcDxPLRAtt0I_\n",
            "To: /content/dados_teste_competicao_features_POR_TIPO.csv\n",
            "100% 2.81M/2.81M [00:00<00:00, 266MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download dataset\n",
        "!gdown --fuzzy \"https://drive.google.com/file/d/1i3YKPA5BqBYvOptIAXqC8X7lIyluf8bI/view?usp=sharing\"\n",
        "!gdown --fuzzy \"https://drive.google.com/file/d/1eWweaJM4jemsM05cC-mmcDxPLRAtt0I_/view?usp=sharing\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb88225",
      "metadata": {
        "id": "8fb88225"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./dados_treino_alunos.csv')\n",
        "\n",
        "df_comp = pd.read_csv('./dados_teste_competicao_features_POR_TIPO.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f11774ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f11774ec",
        "outputId": "b7de2551-0731-4435-fd95-14de481827b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique Cod_municipio: 3924\n"
          ]
        }
      ],
      "source": [
        "# Verificação do número de municípios\n",
        "unique_cod_municipio = df['Cod_municipio'].nunique()\n",
        "print(f\"Number of unique Cod_municipio: {unique_cod_municipio}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "353e8d53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "353e8d53",
        "outputId": "fc352d01-b358-4089-8a34-ac5ca69aafad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique Ano: 1\n"
          ]
        }
      ],
      "source": [
        "# Verificação de repetição de ano\n",
        "unique_ano = df['Ano'].nunique()\n",
        "print(f\"Number of unique Ano: {unique_ano}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "948686e0",
      "metadata": {
        "id": "948686e0"
      },
      "outputs": [],
      "source": [
        "# Deletando features desnecessárias(Talvez incluir estado como one hot encoding)\n",
        "df.drop(columns=['Nome_UF','Nome_Município', 'Tipo_de_VE', 'Ano'], inplace=True)\n",
        "\n",
        "df_comp.drop(columns=['Nome_UF','Nome_Município', 'Tipo_de_VE', 'Ano'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e88ea2b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "e88ea2b5",
        "outputId": "a7b733bc-46bc-4ed5-ce61-2b8e5145ab39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original rows: 12006, Grouped rows: 3924\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Cod_municipio  Potencia_instalada_fotovoltaica  Tarifa_eletrica  \\\n",
              "0           1100015                          3467.06       686.555813   \n",
              "1           1100023                         28751.08       686.555813   \n",
              "2           1100031                          1322.32       686.555813   \n",
              "3           1100049                         25024.72       686.555813   \n",
              "4           1100056                          4588.86       686.555813   \n",
              "...             ...                              ...              ...   \n",
              "3919        5221908                           548.60       853.737962   \n",
              "3920        5222005                          2271.46       853.737962   \n",
              "3921        5222054                           898.76       853.737962   \n",
              "3922        5222302                           630.70       853.737962   \n",
              "3923        5300108                        141264.46       720.872001   \n",
              "\n",
              "      Densidade_demo      Frota  Num_medio_salariosMin  Num_empregados  \\\n",
              "0               3.04    16100.0                    2.1            3706   \n",
              "1              21.88    82721.0                    2.0           24810   \n",
              "2               4.07     4039.0                    1.9             805   \n",
              "3              22.91    75559.0                    1.9           23243   \n",
              "4               5.71    13012.0                    2.0            3556   \n",
              "...              ...        ...                    ...             ...   \n",
              "3919            7.18     2286.0                    1.6             439   \n",
              "3920           15.68    11355.0                    2.2            3442   \n",
              "3921           11.95     5797.0                    2.1            2500   \n",
              "3922            2.67     1132.0                    2.0            1533   \n",
              "3923          489.06  1982829.0                    4.9         1427471   \n",
              "\n",
              "      Num_empresas       PIB  Populacao  ...  Educacao_superior  Irradiancia  \\\n",
              "0              848  21968.76    21494.0  ...              0.036       4635.0   \n",
              "1             4533  22495.49    96833.0  ...              0.039       4494.0   \n",
              "2              199  27082.16     5351.0  ...              0.038       4795.0   \n",
              "3             4045  25385.70    86887.0  ...              0.047       4622.0   \n",
              "4              773  29139.65    15890.0  ...              0.048       4767.0   \n",
              "...            ...       ...        ...  ...                ...          ...   \n",
              "3919           124  17234.77     3716.0  ...              0.026       5280.0   \n",
              "3920           762  31496.17    14956.0  ...              0.034       5306.0   \n",
              "3921           339  36330.36     8768.0  ...              0.031       5299.0   \n",
              "3922           176  27844.67     5815.0  ...              0.015       5332.0   \n",
              "3923        178067  83405.82  2817381.0  ...              0.124       5252.0   \n",
              "\n",
              "      Num_familias  Verticalizacao_cidades  Familias_area_rural  \\\n",
              "0           7695.0                0.001689             0.375049   \n",
              "1          34768.0                0.062069             0.126754   \n",
              "2           1967.0                0.001525             0.456533   \n",
              "3          31917.0                0.078986             0.162860   \n",
              "4           5873.0                0.009705             0.106419   \n",
              "...            ...                     ...                  ...   \n",
              "3919        1553.0                0.000000             0.396008   \n",
              "3920        5499.0                0.009093             0.216221   \n",
              "3921        3144.0                0.001908             0.098601   \n",
              "3922        2180.0                0.000459             0.410550   \n",
              "3923      988191.0                0.342292             0.029299   \n",
              "\n",
              "      Casas_proprias  Casas_alugadas  Residentes_por_familia  \\\n",
              "0           0.697076        0.106823                    2.79   \n",
              "1           0.515905        0.182984                    2.77   \n",
              "2           0.764616        0.075750                    2.72   \n",
              "3           0.507660        0.167309                    2.71   \n",
              "4           0.674102        0.132641                    2.69   \n",
              "...              ...             ...                     ...   \n",
              "3919        0.504185        0.142949                    2.39   \n",
              "3920        0.485543        0.145481                    2.71   \n",
              "3921        0.451972        0.184160                    2.76   \n",
              "3922        0.425229        0.066972                    2.66   \n",
              "3923        0.462680        0.237849                    2.83   \n",
              "\n",
              "      Familias_mais_3_salarios  Qtd. Veículos  \n",
              "0                     0.030669              5  \n",
              "1                     0.053325            131  \n",
              "2                     0.030503              2  \n",
              "3                     0.052668             66  \n",
              "4                     0.044951             15  \n",
              "...                        ...            ...  \n",
              "3919                  0.056021              1  \n",
              "3920                  0.052737             17  \n",
              "3921                  0.044529              1  \n",
              "3922                  0.017890              1  \n",
              "3923                  0.231002          30241  \n",
              "\n",
              "[3924 rows x 24 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dda038ff-e16c-4960-971e-e2ce7a6c00a6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cod_municipio</th>\n",
              "      <th>Potencia_instalada_fotovoltaica</th>\n",
              "      <th>Tarifa_eletrica</th>\n",
              "      <th>Densidade_demo</th>\n",
              "      <th>Frota</th>\n",
              "      <th>Num_medio_salariosMin</th>\n",
              "      <th>Num_empregados</th>\n",
              "      <th>Num_empresas</th>\n",
              "      <th>PIB</th>\n",
              "      <th>Populacao</th>\n",
              "      <th>...</th>\n",
              "      <th>Educacao_superior</th>\n",
              "      <th>Irradiancia</th>\n",
              "      <th>Num_familias</th>\n",
              "      <th>Verticalizacao_cidades</th>\n",
              "      <th>Familias_area_rural</th>\n",
              "      <th>Casas_proprias</th>\n",
              "      <th>Casas_alugadas</th>\n",
              "      <th>Residentes_por_familia</th>\n",
              "      <th>Familias_mais_3_salarios</th>\n",
              "      <th>Qtd. Veículos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1100015</td>\n",
              "      <td>3467.06</td>\n",
              "      <td>686.555813</td>\n",
              "      <td>3.04</td>\n",
              "      <td>16100.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>3706</td>\n",
              "      <td>848</td>\n",
              "      <td>21968.76</td>\n",
              "      <td>21494.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.036</td>\n",
              "      <td>4635.0</td>\n",
              "      <td>7695.0</td>\n",
              "      <td>0.001689</td>\n",
              "      <td>0.375049</td>\n",
              "      <td>0.697076</td>\n",
              "      <td>0.106823</td>\n",
              "      <td>2.79</td>\n",
              "      <td>0.030669</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1100023</td>\n",
              "      <td>28751.08</td>\n",
              "      <td>686.555813</td>\n",
              "      <td>21.88</td>\n",
              "      <td>82721.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>24810</td>\n",
              "      <td>4533</td>\n",
              "      <td>22495.49</td>\n",
              "      <td>96833.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.039</td>\n",
              "      <td>4494.0</td>\n",
              "      <td>34768.0</td>\n",
              "      <td>0.062069</td>\n",
              "      <td>0.126754</td>\n",
              "      <td>0.515905</td>\n",
              "      <td>0.182984</td>\n",
              "      <td>2.77</td>\n",
              "      <td>0.053325</td>\n",
              "      <td>131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1100031</td>\n",
              "      <td>1322.32</td>\n",
              "      <td>686.555813</td>\n",
              "      <td>4.07</td>\n",
              "      <td>4039.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>805</td>\n",
              "      <td>199</td>\n",
              "      <td>27082.16</td>\n",
              "      <td>5351.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.038</td>\n",
              "      <td>4795.0</td>\n",
              "      <td>1967.0</td>\n",
              "      <td>0.001525</td>\n",
              "      <td>0.456533</td>\n",
              "      <td>0.764616</td>\n",
              "      <td>0.075750</td>\n",
              "      <td>2.72</td>\n",
              "      <td>0.030503</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1100049</td>\n",
              "      <td>25024.72</td>\n",
              "      <td>686.555813</td>\n",
              "      <td>22.91</td>\n",
              "      <td>75559.0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>23243</td>\n",
              "      <td>4045</td>\n",
              "      <td>25385.70</td>\n",
              "      <td>86887.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047</td>\n",
              "      <td>4622.0</td>\n",
              "      <td>31917.0</td>\n",
              "      <td>0.078986</td>\n",
              "      <td>0.162860</td>\n",
              "      <td>0.507660</td>\n",
              "      <td>0.167309</td>\n",
              "      <td>2.71</td>\n",
              "      <td>0.052668</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1100056</td>\n",
              "      <td>4588.86</td>\n",
              "      <td>686.555813</td>\n",
              "      <td>5.71</td>\n",
              "      <td>13012.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3556</td>\n",
              "      <td>773</td>\n",
              "      <td>29139.65</td>\n",
              "      <td>15890.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.048</td>\n",
              "      <td>4767.0</td>\n",
              "      <td>5873.0</td>\n",
              "      <td>0.009705</td>\n",
              "      <td>0.106419</td>\n",
              "      <td>0.674102</td>\n",
              "      <td>0.132641</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.044951</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3919</th>\n",
              "      <td>5221908</td>\n",
              "      <td>548.60</td>\n",
              "      <td>853.737962</td>\n",
              "      <td>7.18</td>\n",
              "      <td>2286.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>439</td>\n",
              "      <td>124</td>\n",
              "      <td>17234.77</td>\n",
              "      <td>3716.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026</td>\n",
              "      <td>5280.0</td>\n",
              "      <td>1553.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.396008</td>\n",
              "      <td>0.504185</td>\n",
              "      <td>0.142949</td>\n",
              "      <td>2.39</td>\n",
              "      <td>0.056021</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3920</th>\n",
              "      <td>5222005</td>\n",
              "      <td>2271.46</td>\n",
              "      <td>853.737962</td>\n",
              "      <td>15.68</td>\n",
              "      <td>11355.0</td>\n",
              "      <td>2.2</td>\n",
              "      <td>3442</td>\n",
              "      <td>762</td>\n",
              "      <td>31496.17</td>\n",
              "      <td>14956.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.034</td>\n",
              "      <td>5306.0</td>\n",
              "      <td>5499.0</td>\n",
              "      <td>0.009093</td>\n",
              "      <td>0.216221</td>\n",
              "      <td>0.485543</td>\n",
              "      <td>0.145481</td>\n",
              "      <td>2.71</td>\n",
              "      <td>0.052737</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3921</th>\n",
              "      <td>5222054</td>\n",
              "      <td>898.76</td>\n",
              "      <td>853.737962</td>\n",
              "      <td>11.95</td>\n",
              "      <td>5797.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>2500</td>\n",
              "      <td>339</td>\n",
              "      <td>36330.36</td>\n",
              "      <td>8768.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031</td>\n",
              "      <td>5299.0</td>\n",
              "      <td>3144.0</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>0.098601</td>\n",
              "      <td>0.451972</td>\n",
              "      <td>0.184160</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.044529</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3922</th>\n",
              "      <td>5222302</td>\n",
              "      <td>630.70</td>\n",
              "      <td>853.737962</td>\n",
              "      <td>2.67</td>\n",
              "      <td>1132.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1533</td>\n",
              "      <td>176</td>\n",
              "      <td>27844.67</td>\n",
              "      <td>5815.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015</td>\n",
              "      <td>5332.0</td>\n",
              "      <td>2180.0</td>\n",
              "      <td>0.000459</td>\n",
              "      <td>0.410550</td>\n",
              "      <td>0.425229</td>\n",
              "      <td>0.066972</td>\n",
              "      <td>2.66</td>\n",
              "      <td>0.017890</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3923</th>\n",
              "      <td>5300108</td>\n",
              "      <td>141264.46</td>\n",
              "      <td>720.872001</td>\n",
              "      <td>489.06</td>\n",
              "      <td>1982829.0</td>\n",
              "      <td>4.9</td>\n",
              "      <td>1427471</td>\n",
              "      <td>178067</td>\n",
              "      <td>83405.82</td>\n",
              "      <td>2817381.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.124</td>\n",
              "      <td>5252.0</td>\n",
              "      <td>988191.0</td>\n",
              "      <td>0.342292</td>\n",
              "      <td>0.029299</td>\n",
              "      <td>0.462680</td>\n",
              "      <td>0.237849</td>\n",
              "      <td>2.83</td>\n",
              "      <td>0.231002</td>\n",
              "      <td>30241</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3924 rows × 24 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dda038ff-e16c-4960-971e-e2ce7a6c00a6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dda038ff-e16c-4960-971e-e2ce7a6c00a6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dda038ff-e16c-4960-971e-e2ce7a6c00a6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-03ba4f81-efb3-4084-ad03-b71d85446fe1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-03ba4f81-efb3-4084-ad03-b71d85446fe1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-03ba4f81-efb3-4084-ad03-b71d85446fe1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_aa80553e-944b-4a05-9322-bcccdf3c7b5e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_grouped')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_aa80553e-944b-4a05-9322-bcccdf3c7b5e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_grouped');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_grouped"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Somando a quantidade de veículos para cada município\n",
        "df_grouped = df.groupby('Cod_municipio', as_index=False).agg({\n",
        "    'Potencia_instalada_fotovoltaica': 'first',\n",
        "    'Tarifa_eletrica': 'first',\n",
        "    'Densidade_demo': 'first',\n",
        "    'Frota': 'first',\n",
        "    'Num_medio_salariosMin': 'first',\n",
        "    'Num_empregados': 'first',\n",
        "    'Num_empresas': 'first',\n",
        "    'PIB': 'first',\n",
        "    'Populacao': 'first',\n",
        "    'Area': 'first',\n",
        "    'IDH_educacao': 'first',\n",
        "    'IDH_saude': 'first',\n",
        "    'IDH_renda': 'first',\n",
        "    'Educacao_superior': 'first',\n",
        "    'Irradiancia': 'first',\n",
        "    'Num_familias': 'first',\n",
        "    'Verticalizacao_cidades': 'first',\n",
        "    'Familias_area_rural': 'first',\n",
        "    'Casas_proprias': 'first',\n",
        "    'Casas_alugadas': 'first',\n",
        "    'Residentes_por_familia': 'first',\n",
        "    'Familias_mais_3_salarios': 'first',\n",
        "    'Qtd. Veículos': 'sum'\n",
        "})\n",
        "\n",
        "df_grouped_comp = df_comp.groupby('Cod_municipio', as_index=False).agg({\n",
        "    'Potencia_instalada_fotovoltaica': 'first',\n",
        "    'Tarifa_eletrica': 'first',\n",
        "    'Densidade_demo': 'first',\n",
        "    'Frota': 'first',\n",
        "    'Num_medio_salariosMin': 'first',\n",
        "    'Num_empregados': 'first',\n",
        "    'Num_empresas': 'first',\n",
        "    'PIB': 'first',\n",
        "    'Populacao': 'first',\n",
        "    'Area': 'first',\n",
        "    'IDH_educacao': 'first',\n",
        "    'IDH_saude': 'first',\n",
        "    'IDH_renda': 'first',\n",
        "    'Educacao_superior': 'first',\n",
        "    'Irradiancia': 'first',\n",
        "    'Num_familias': 'first',\n",
        "    'Verticalizacao_cidades': 'first',\n",
        "    'Familias_area_rural': 'first',\n",
        "    'Casas_proprias': 'first',\n",
        "    'Casas_alugadas': 'first',\n",
        "    'Residentes_por_familia': 'first',\n",
        "    'Familias_mais_3_salarios': 'first',\n",
        "})\n",
        "\n",
        "print(f\"Original rows: {len(df)}, Grouped rows: {len(df_grouped)}\")\n",
        "df_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v-wAl7KxEN26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-wAl7KxEN26",
        "outputId": "33e9cd39-d3f0-4ea9-8a33-fce47c41b5d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Has Null: False\n",
            "Has NaN: False\n",
            "Train data: 3139\n",
            "Test data: 785\n"
          ]
        }
      ],
      "source": [
        "# Convertendo todos os valores para float\n",
        "df_grouped_remv = df_grouped\n",
        "for col in df_grouped.columns:\n",
        "    df_grouped_remv[col] = pd.to_numeric(df_grouped[col]).astype('float64')\n",
        "\n",
        "X = df_grouped_remv.drop(columns=['Cod_municipio', 'Qtd. Veículos'])\n",
        "y = df_grouped_remv['Qtd. Veículos']\n",
        "\n",
        "# Embaralhando e separando em treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "\n",
        "# Normalizando os dados\n",
        "x_scaler = StandardScaler()\n",
        "# O fit faz o cálculo da média e desvio para normalizar\n",
        "X_train_norm = x_scaler.fit_transform(X_train.values)\n",
        "# Usar a média e desvio já calculados para evitar data leakage\n",
        "X_test_norm = x_scaler.transform(X_test.values)\n",
        "\n",
        "# Normalizando o Y\n",
        "# Deve se considerar que quanto maior os valores de y\n",
        "# Maior vai ser a loss e logo maior vai ser a atualização dos pesos\n",
        "# Resultando em explosão de gradientes\n",
        "y_scaler = StandardScaler()\n",
        "y_train_norm = y_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_test_norm = y_scaler.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "#y_pred_original = y_scaler.inverse_transform(\n",
        "#    y_pred.cpu().numpy().reshape(-1, 1))\n",
        "\n",
        "# Verificando valores NULL ou NAN\n",
        "print(\"Has Null:\", df_grouped_remv.isnull().any().any())\n",
        "print(\"Has NaN:\", df_grouped_remv.isna().any().any())\n",
        "\n",
        "# Verificando quantidade de dados\n",
        "print('Train data:', len(X_train))\n",
        "print('Test data:', len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "247zhUjboLkO",
      "metadata": {
        "id": "247zhUjboLkO"
      },
      "source": [
        "# 3 - Modelos(deve ser rodado para gerar o CSV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pq0U_lUwJ0bo",
      "metadata": {
        "id": "pq0U_lUwJ0bo"
      },
      "outputs": [],
      "source": [
        "class LinearMostSimple(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(22, 18)\n",
        "        self.norm = nn.BatchNorm1d(18)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.drop = nn.Dropout(p=0.1)\n",
        "        self.l2 = nn.Linear(18, 12)\n",
        "        self.norm1 = nn.BatchNorm1d(12)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.l3 = nn.Linear(12, 6)\n",
        "        self.norm2 = nn.BatchNorm1d(6)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.l4 = nn.Linear(6, 1)\n",
        "\n",
        "        # Inicialização\n",
        "        nn.init.xavier_uniform_(self.l1.weight)\n",
        "        nn.init.xavier_uniform_(self.l2.weight)\n",
        "        nn.init.xavier_uniform_(self.l3.weight)\n",
        "        nn.init.xavier_uniform_(self.l4.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.l2(x)\n",
        "        #x = self.norm1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.l3(x)\n",
        "        #x = self.norm2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.l4(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AoOHbvQbHMyB",
      "metadata": {
        "id": "AoOHbvQbHMyB"
      },
      "outputs": [],
      "source": [
        "# Cria o modelo e manda para a GPU\n",
        "model = LinearMostSimple().to('cuda')\n",
        "# Treina mais rápido se compilado\n",
        "model = torch.compile(model)\n",
        "\n",
        "# Loss MSE\n",
        "loss_fn = torch.nn.MSELoss()#reduction='mean')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3s8nsm6DoCkb",
      "metadata": {
        "id": "3s8nsm6DoCkb"
      },
      "source": [
        "#4 - Treinamento DNN\n",
        "*   Nem sempre o melhor modelo é conseguido, por isso deixei salvo os pesos do melhor modelo que eu consegui.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_oIP9WRWpJKL",
      "metadata": {
        "id": "_oIP9WRWpJKL"
      },
      "outputs": [],
      "source": [
        "# Transformando os dados para tensores\n",
        "X_train_tensor_norm = torch.tensor(X_train_norm,\n",
        "                                   dtype=torch.float32).to('cuda')\n",
        "y_train_tensor_norm = torch.tensor(\n",
        "    y_train_norm,\n",
        "    dtype=torch.float32).reshape(-1, 1).to('cuda')\n",
        "X_test_tensor_norm = torch.tensor(X_test_norm,\n",
        "                                  dtype=torch.float32).to('cuda')\n",
        "y_test_tensor_norm = torch.tensor(y_test_norm,\n",
        "                                  dtype=torch.float32).reshape(-1, 1).to('cuda')\n",
        "\n",
        "# Dados não torch não normalizados\n",
        "X_train_tensor = torch.tensor(X_train.values,\n",
        "                              dtype=torch.float32).to('cuda')\n",
        "y_train_tensor = torch.tensor(y_train.values,\n",
        "                              dtype=torch.float32).reshape(-1, 1).to('cuda')\n",
        "X_test_tensor = torch.tensor(X_test.values,\n",
        "                             dtype=torch.float32).to('cuda')\n",
        "y_test_tensor = torch.tensor(y_test.values,\n",
        "                             dtype=torch.float32).reshape(-1, 1).to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PjIKP64UGJjx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjIKP64UGJjx",
        "outputId": "3e8f15c3-40ce-44dc-8519-ca2586f1b690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train Loss RMSE: 195.2096\n",
            "Test Loss MSE: 112876.4531\n",
            "Test RMSE: 335.9709\n",
            "Test R²: 0.9099\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 193/100\n",
            "\n",
            "Epoch 447/1000\n",
            "Train Loss MSE: 55663.3637\n",
            "Train Loss RMSE: 235.9308\n",
            "Test Loss MSE: 275731.5625\n",
            "Test RMSE: 525.1015\n",
            "Test R²: 0.7800\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 194/100\n",
            "\n",
            "Epoch 448/1000\n",
            "Train Loss MSE: 88632.0587\n",
            "Train Loss RMSE: 297.7114\n",
            "Test Loss MSE: 140080.9375\n",
            "Test RMSE: 374.2739\n",
            "Test R²: 0.8882\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 195/100\n",
            "\n",
            "Epoch 449/1000\n",
            "Train Loss MSE: 68026.4123\n",
            "Train Loss RMSE: 260.8187\n",
            "Test Loss MSE: 507585.1562\n",
            "Test RMSE: 712.4501\n",
            "Test R²: 0.5949\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 196/100\n",
            "\n",
            "Epoch 450/1000\n",
            "Train Loss MSE: 339493.8364\n",
            "Train Loss RMSE: 582.6610\n",
            "Test Loss MSE: 80342.4297\n",
            "Test RMSE: 283.4474\n",
            "Test R²: 0.9359\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 197/100\n",
            "\n",
            "Epoch 451/1000\n",
            "Train Loss MSE: 76134.4718\n",
            "Train Loss RMSE: 275.9248\n",
            "Test Loss MSE: 10696.9707\n",
            "Test RMSE: 103.4262\n",
            "Test R²: 0.9915\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 198/100\n",
            "\n",
            "Epoch 452/1000\n",
            "Train Loss MSE: 100228.9647\n",
            "Train Loss RMSE: 316.5896\n",
            "Test Loss MSE: 59730.6094\n",
            "Test RMSE: 244.3985\n",
            "Test R²: 0.9523\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 199/100\n",
            "\n",
            "Epoch 453/1000\n",
            "Train Loss MSE: 97817.2743\n",
            "Train Loss RMSE: 312.7575\n",
            "Test Loss MSE: 73809.0234\n",
            "Test RMSE: 271.6782\n",
            "Test R²: 0.9411\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 200/100\n",
            "\n",
            "Epoch 454/1000\n",
            "Train Loss MSE: 56486.8297\n",
            "Train Loss RMSE: 237.6696\n",
            "Test Loss MSE: 20777.3301\n",
            "Test RMSE: 144.1434\n",
            "Test R²: 0.9834\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 201/100\n",
            "\n",
            "Epoch 455/1000\n",
            "Train Loss MSE: 28909.4255\n",
            "Train Loss RMSE: 170.0277\n",
            "Test Loss MSE: 27611.5117\n",
            "Test RMSE: 166.1671\n",
            "Test R²: 0.9780\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 202/100\n",
            "\n",
            "Epoch 456/1000\n",
            "Train Loss MSE: 29664.2399\n",
            "Train Loss RMSE: 172.2331\n",
            "Test Loss MSE: 676916.8125\n",
            "Test RMSE: 822.7495\n",
            "Test R²: 0.4598\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 203/100\n",
            "\n",
            "Epoch 457/1000\n",
            "Train Loss MSE: 67673.9267\n",
            "Train Loss RMSE: 260.1421\n",
            "Test Loss MSE: 305501.2812\n",
            "Test RMSE: 552.7217\n",
            "Test R²: 0.7562\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 204/100\n",
            "\n",
            "Epoch 458/1000\n",
            "Train Loss MSE: 74599.5606\n",
            "Train Loss RMSE: 273.1292\n",
            "Test Loss MSE: 192093.6406\n",
            "Test RMSE: 438.2849\n",
            "Test R²: 0.8467\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 205/100\n",
            "\n",
            "Epoch 459/1000\n",
            "Train Loss MSE: 18924.0430\n",
            "Train Loss RMSE: 137.5647\n",
            "Test Loss MSE: 858630.1875\n",
            "Test RMSE: 926.6230\n",
            "Test R²: 0.3148\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 206/100\n",
            "\n",
            "Epoch 460/1000\n",
            "Train Loss MSE: 60555.9882\n",
            "Train Loss RMSE: 246.0813\n",
            "Test Loss MSE: 345205.4062\n",
            "Test RMSE: 587.5418\n",
            "Test R²: 0.7245\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 207/100\n",
            "\n",
            "Epoch 461/1000\n",
            "Train Loss MSE: 134576.9371\n",
            "Train Loss RMSE: 366.8473\n",
            "Test Loss MSE: 13971.7812\n",
            "Test RMSE: 118.2023\n",
            "Test R²: 0.9888\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 208/100\n",
            "\n",
            "Epoch 462/1000\n",
            "Train Loss MSE: 87308.7277\n",
            "Train Loss RMSE: 295.4805\n",
            "Test Loss MSE: 901488.0625\n",
            "Test RMSE: 949.4673\n",
            "Test R²: 0.2806\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 209/100\n",
            "\n",
            "Epoch 463/1000\n",
            "Train Loss MSE: 33668.7833\n",
            "Train Loss RMSE: 183.4906\n",
            "Test Loss MSE: 239066.6875\n",
            "Test RMSE: 488.9445\n",
            "Test R²: 0.8092\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 210/100\n",
            "\n",
            "Epoch 464/1000\n",
            "Train Loss MSE: 24980.8123\n",
            "Train Loss RMSE: 158.0532\n",
            "Test Loss MSE: 140539.0000\n",
            "Test RMSE: 374.8853\n",
            "Test R²: 0.8878\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 211/100\n",
            "\n",
            "Epoch 465/1000\n",
            "Train Loss MSE: 45637.6011\n",
            "Train Loss RMSE: 213.6296\n",
            "Test Loss MSE: 60723.5234\n",
            "Test RMSE: 246.4214\n",
            "Test R²: 0.9515\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 212/100\n",
            "\n",
            "Epoch 466/1000\n",
            "Train Loss MSE: 37066.7671\n",
            "Train Loss RMSE: 192.5273\n",
            "Test Loss MSE: 77392.9922\n",
            "Test RMSE: 278.1960\n",
            "Test R²: 0.9382\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 213/100\n",
            "\n",
            "Epoch 467/1000\n",
            "Train Loss MSE: 387420.9761\n",
            "Train Loss RMSE: 622.4315\n",
            "Test Loss MSE: 21436.4258\n",
            "Test RMSE: 146.4118\n",
            "Test R²: 0.9829\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 214/100\n",
            "\n",
            "Epoch 468/1000\n",
            "Train Loss MSE: 45946.4269\n",
            "Train Loss RMSE: 214.3512\n",
            "Test Loss MSE: 94867.5391\n",
            "Test RMSE: 308.0057\n",
            "Test R²: 0.9243\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 215/100\n",
            "\n",
            "Epoch 469/1000\n",
            "Train Loss MSE: 25523.9367\n",
            "Train Loss RMSE: 159.7621\n",
            "Test Loss MSE: 107431.1328\n",
            "Test RMSE: 327.7669\n",
            "Test R²: 0.9143\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 216/100\n",
            "\n",
            "Epoch 470/1000\n",
            "Train Loss MSE: 18098.5607\n",
            "Train Loss RMSE: 134.5309\n",
            "Test Loss MSE: 34767.1172\n",
            "Test RMSE: 186.4594\n",
            "Test R²: 0.9723\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 217/100\n",
            "\n",
            "Epoch 471/1000\n",
            "Train Loss MSE: 47370.1769\n",
            "Train Loss RMSE: 217.6469\n",
            "Test Loss MSE: 465046.7188\n",
            "Test RMSE: 681.9433\n",
            "Test R²: 0.6289\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 218/100\n",
            "\n",
            "Epoch 472/1000\n",
            "Train Loss MSE: 69077.5115\n",
            "Train Loss RMSE: 262.8260\n",
            "Test Loss MSE: 132274.6094\n",
            "Test RMSE: 363.6958\n",
            "Test R²: 0.8944\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 219/100\n",
            "\n",
            "Epoch 473/1000\n",
            "Train Loss MSE: 35731.0374\n",
            "Train Loss RMSE: 189.0266\n",
            "Test Loss MSE: 25517.7969\n",
            "Test RMSE: 159.7429\n",
            "Test R²: 0.9796\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 220/100\n",
            "\n",
            "Epoch 474/1000\n",
            "Train Loss MSE: 52359.0217\n",
            "Train Loss RMSE: 228.8209\n",
            "Test Loss MSE: 26000.3418\n",
            "Test RMSE: 161.2462\n",
            "Test R²: 0.9793\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 221/100\n",
            "\n",
            "Epoch 475/1000\n",
            "Train Loss MSE: 20320.0767\n",
            "Train Loss RMSE: 142.5485\n",
            "Test Loss MSE: 50955.4102\n",
            "Test RMSE: 225.7331\n",
            "Test R²: 0.9593\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 222/100\n",
            "\n",
            "Epoch 476/1000\n",
            "Train Loss MSE: 64156.8244\n",
            "Train Loss RMSE: 253.2920\n",
            "Test Loss MSE: 27531.5918\n",
            "Test RMSE: 165.9265\n",
            "Test R²: 0.9780\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 223/100\n",
            "\n",
            "Epoch 477/1000\n",
            "Train Loss MSE: 42904.5726\n",
            "Train Loss RMSE: 207.1342\n",
            "Test Loss MSE: 101082.3125\n",
            "Test RMSE: 317.9344\n",
            "Test R²: 0.9193\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 224/100\n",
            "\n",
            "Epoch 478/1000\n",
            "Train Loss MSE: 135160.3736\n",
            "Train Loss RMSE: 367.6416\n",
            "Test Loss MSE: 10690.9170\n",
            "Test RMSE: 103.3969\n",
            "Test R²: 0.9915\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 225/100\n",
            "\n",
            "Epoch 479/1000\n",
            "Train Loss MSE: 34372.1808\n",
            "Train Loss RMSE: 185.3974\n",
            "Test Loss MSE: 96691.3125\n",
            "Test RMSE: 310.9523\n",
            "Test R²: 0.9228\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 226/100\n",
            "\n",
            "Epoch 480/1000\n",
            "Train Loss MSE: 1012484.4030\n",
            "Train Loss RMSE: 1006.2228\n",
            "Test Loss MSE: 68221.5625\n",
            "Test RMSE: 261.1926\n",
            "Test R²: 0.9456\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 227/100\n",
            "\n",
            "Epoch 481/1000\n",
            "Train Loss MSE: 40405.6993\n",
            "Train Loss RMSE: 201.0117\n",
            "Test Loss MSE: 48598.0703\n",
            "Test RMSE: 220.4497\n",
            "Test R²: 0.9612\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 228/100\n",
            "\n",
            "Epoch 482/1000\n",
            "Train Loss MSE: 955079.7370\n",
            "Train Loss RMSE: 977.2818\n",
            "Test Loss MSE: 110612.4062\n",
            "Test RMSE: 332.5844\n",
            "Test R²: 0.9117\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 229/100\n",
            "\n",
            "Epoch 483/1000\n",
            "Train Loss MSE: 28501.7087\n",
            "Train Loss RMSE: 168.8245\n",
            "Test Loss MSE: 8092.6997\n",
            "Test RMSE: 89.9594\n",
            "Test R²: 0.9935\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 230/100\n",
            "\n",
            "Epoch 484/1000\n",
            "Train Loss MSE: 155007.2833\n",
            "Train Loss RMSE: 393.7096\n",
            "Test Loss MSE: 488027.3125\n",
            "Test RMSE: 698.5895\n",
            "Test R²: 0.6105\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 231/100\n",
            "\n",
            "Epoch 485/1000\n",
            "Train Loss MSE: 773607.6473\n",
            "Train Loss RMSE: 879.5497\n",
            "Test Loss MSE: 184519.6875\n",
            "Test RMSE: 429.5575\n",
            "Test R²: 0.8527\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 232/100\n",
            "\n",
            "Epoch 486/1000\n",
            "Train Loss MSE: 158940.6182\n",
            "Train Loss RMSE: 398.6736\n",
            "Test Loss MSE: 235263.0312\n",
            "Test RMSE: 485.0392\n",
            "Test R²: 0.8122\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 233/100\n",
            "\n",
            "Epoch 487/1000\n",
            "Train Loss MSE: 44962.9924\n",
            "Train Loss RMSE: 212.0448\n",
            "Test Loss MSE: 31307.5605\n",
            "Test RMSE: 176.9394\n",
            "Test R²: 0.9750\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 234/100\n",
            "\n",
            "Epoch 488/1000\n",
            "Train Loss MSE: 56902.9766\n",
            "Train Loss RMSE: 238.5434\n",
            "Test Loss MSE: 10992.6992\n",
            "Test RMSE: 104.8461\n",
            "Test R²: 0.9912\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 235/100\n",
            "\n",
            "Epoch 489/1000\n",
            "Train Loss MSE: 592810.0842\n",
            "Train Loss RMSE: 769.9416\n",
            "Test Loss MSE: 565670.7500\n",
            "Test RMSE: 752.1109\n",
            "Test R²: 0.5486\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 236/100\n",
            "\n",
            "Epoch 490/1000\n",
            "Train Loss MSE: 72943.6281\n",
            "Train Loss RMSE: 270.0808\n",
            "Test Loss MSE: 811094.0000\n",
            "Test RMSE: 900.6076\n",
            "Test R²: 0.3527\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 237/100\n",
            "\n",
            "Epoch 491/1000\n",
            "Train Loss MSE: 613702.1843\n",
            "Train Loss RMSE: 783.3915\n",
            "Test Loss MSE: 32140.8301\n",
            "Test RMSE: 179.2786\n",
            "Test R²: 0.9743\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 238/100\n",
            "\n",
            "Epoch 492/1000\n",
            "Train Loss MSE: 99743.0286\n",
            "Train Loss RMSE: 315.8212\n",
            "Test Loss MSE: 55951.7266\n",
            "Test RMSE: 236.5412\n",
            "Test R²: 0.9553\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 239/100\n",
            "\n",
            "Epoch 493/1000\n",
            "Train Loss MSE: 18018.0663\n",
            "Train Loss RMSE: 134.2314\n",
            "Test Loss MSE: 15212.7979\n",
            "Test RMSE: 123.3402\n",
            "Test R²: 0.9879\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 240/100\n",
            "\n",
            "Epoch 494/1000\n",
            "Train Loss MSE: 113768.3659\n",
            "Train Loss RMSE: 337.2957\n",
            "Test Loss MSE: 298910.7812\n",
            "Test RMSE: 546.7273\n",
            "Test R²: 0.7615\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 241/100\n",
            "\n",
            "Epoch 495/1000\n",
            "Train Loss MSE: 275128.4598\n",
            "Train Loss RMSE: 524.5269\n",
            "Test Loss MSE: 801615.0000\n",
            "Test RMSE: 895.3295\n",
            "Test R²: 0.3603\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 242/100\n",
            "\n",
            "Epoch 496/1000\n",
            "Train Loss MSE: 195479.1391\n",
            "Train Loss RMSE: 442.1302\n",
            "Test Loss MSE: 862241.5000\n",
            "Test RMSE: 928.5696\n",
            "Test R²: 0.3119\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 243/100\n",
            "\n",
            "Epoch 497/1000\n",
            "Train Loss MSE: 38794.4992\n",
            "Train Loss RMSE: 196.9632\n",
            "Test Loss MSE: 16917.2559\n",
            "Test RMSE: 130.0664\n",
            "Test R²: 0.9865\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 244/100\n",
            "\n",
            "Epoch 498/1000\n",
            "Train Loss MSE: 47535.6698\n",
            "Train Loss RMSE: 218.0268\n",
            "Test Loss MSE: 79538.5703\n",
            "Test RMSE: 282.0258\n",
            "Test R²: 0.9365\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 245/100\n",
            "\n",
            "Epoch 499/1000\n",
            "Train Loss MSE: 21202.8019\n",
            "Train Loss RMSE: 145.6118\n",
            "Test Loss MSE: 27424.2559\n",
            "Test RMSE: 165.6027\n",
            "Test R²: 0.9781\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 246/100\n",
            "\n",
            "Epoch 500/1000\n",
            "Train Loss MSE: 36684.8955\n",
            "Train Loss RMSE: 191.5330\n",
            "Test Loss MSE: 54518.0664\n",
            "Test RMSE: 233.4910\n",
            "Test R²: 0.9565\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 247/100\n",
            "\n",
            "Epoch 501/1000\n",
            "Train Loss MSE: 33935.9631\n",
            "Train Loss RMSE: 184.2172\n",
            "Test Loss MSE: 62737.6094\n",
            "Test RMSE: 250.4748\n",
            "Test R²: 0.9499\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 248/100\n",
            "\n",
            "Epoch 502/1000\n",
            "Train Loss MSE: 33789.4424\n",
            "Train Loss RMSE: 183.8190\n",
            "Test Loss MSE: 197707.5781\n",
            "Test RMSE: 444.6432\n",
            "Test R²: 0.8422\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 249/100\n",
            "\n",
            "Epoch 503/1000\n",
            "Train Loss MSE: 39521.1684\n",
            "Train Loss RMSE: 198.7993\n",
            "Test Loss MSE: 57389.7500\n",
            "Test RMSE: 239.5616\n",
            "Test R²: 0.9542\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 250/100\n",
            "\n",
            "Epoch 504/1000\n",
            "Train Loss MSE: 32813.7378\n",
            "Train Loss RMSE: 181.1456\n",
            "Test Loss MSE: 150544.5781\n",
            "Test RMSE: 388.0007\n",
            "Test R²: 0.8799\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 251/100\n",
            "\n",
            "Epoch 505/1000\n",
            "Train Loss MSE: 980547.6753\n",
            "Train Loss RMSE: 990.2261\n",
            "Test Loss MSE: 24517.8809\n",
            "Test RMSE: 156.5819\n",
            "Test R²: 0.9804\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 252/100\n",
            "\n",
            "Epoch 506/1000\n",
            "Train Loss MSE: 371908.5016\n",
            "Train Loss RMSE: 609.8430\n",
            "Test Loss MSE: 143635.1562\n",
            "Test RMSE: 378.9923\n",
            "Test R²: 0.8854\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 253/100\n",
            "\n",
            "Epoch 507/1000\n",
            "Train Loss MSE: 39959.6678\n",
            "Train Loss RMSE: 199.8991\n",
            "Test Loss MSE: 193034.6250\n",
            "Test RMSE: 439.3571\n",
            "Test R²: 0.8459\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 254/100\n",
            "\n",
            "Epoch 508/1000\n",
            "Train Loss MSE: 35046.2256\n",
            "Train Loss RMSE: 187.2064\n",
            "Test Loss MSE: 118353.2578\n",
            "Test RMSE: 344.0251\n",
            "Test R²: 0.9055\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 255/100\n",
            "\n",
            "Epoch 509/1000\n",
            "Train Loss MSE: 66923.4728\n",
            "Train Loss RMSE: 258.6957\n",
            "Test Loss MSE: 66047.6328\n",
            "Test RMSE: 256.9973\n",
            "Test R²: 0.9473\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 256/100\n",
            "\n",
            "Epoch 510/1000\n",
            "Train Loss MSE: 197608.7028\n",
            "Train Loss RMSE: 444.5320\n",
            "Test Loss MSE: 792131.7500\n",
            "Test RMSE: 890.0178\n",
            "Test R²: 0.3678\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 257/100\n",
            "\n",
            "Epoch 511/1000\n",
            "Train Loss MSE: 404095.7397\n",
            "Train Loss RMSE: 635.6853\n",
            "Test Loss MSE: 13063.1211\n",
            "Test RMSE: 114.2940\n",
            "Test R²: 0.9896\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 258/100\n",
            "\n",
            "Epoch 512/1000\n",
            "Train Loss MSE: 45332.4019\n",
            "Train Loss RMSE: 212.9141\n",
            "Test Loss MSE: 22725.9160\n",
            "Test RMSE: 150.7512\n",
            "Test R²: 0.9819\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 259/100\n",
            "\n",
            "Epoch 513/1000\n",
            "Train Loss MSE: 362688.1799\n",
            "Train Loss RMSE: 602.2360\n",
            "Test Loss MSE: 100677.6953\n",
            "Test RMSE: 317.2975\n",
            "Test R²: 0.9197\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 260/100\n",
            "\n",
            "Epoch 514/1000\n",
            "Train Loss MSE: 65627.4156\n",
            "Train Loss RMSE: 256.1785\n",
            "Test Loss MSE: 551954.5625\n",
            "Test RMSE: 742.9364\n",
            "Test R²: 0.5595\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 261/100\n",
            "\n",
            "Epoch 515/1000\n",
            "Train Loss MSE: 22220.1703\n",
            "Train Loss RMSE: 149.0643\n",
            "Test Loss MSE: 515484.4688\n",
            "Test RMSE: 717.9725\n",
            "Test R²: 0.5886\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 262/100\n",
            "\n",
            "Epoch 516/1000\n",
            "Train Loss MSE: 17615.7910\n",
            "Train Loss RMSE: 132.7245\n",
            "Test Loss MSE: 227483.8906\n",
            "Test RMSE: 476.9527\n",
            "Test R²: 0.8185\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 263/100\n",
            "\n",
            "Epoch 517/1000\n",
            "Train Loss MSE: 989593.2027\n",
            "Train Loss RMSE: 994.7830\n",
            "Test Loss MSE: 30899.7051\n",
            "Test RMSE: 175.7831\n",
            "Test R²: 0.9753\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 264/100\n",
            "\n",
            "Epoch 518/1000\n",
            "Train Loss MSE: 75072.1792\n",
            "Train Loss RMSE: 273.9930\n",
            "Test Loss MSE: 11206.2051\n",
            "Test RMSE: 105.8594\n",
            "Test R²: 0.9911\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 265/100\n",
            "\n",
            "Epoch 519/1000\n",
            "Train Loss MSE: 15025.1124\n",
            "Train Loss RMSE: 122.5770\n",
            "Test Loss MSE: 107089.1250\n",
            "Test RMSE: 327.2447\n",
            "Test R²: 0.9145\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 266/100\n",
            "\n",
            "Epoch 520/1000\n",
            "Train Loss MSE: 37451.9925\n",
            "Train Loss RMSE: 193.5252\n",
            "Test Loss MSE: 125796.1094\n",
            "Test RMSE: 354.6775\n",
            "Test R²: 0.8996\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 267/100\n",
            "\n",
            "Epoch 521/1000\n",
            "Train Loss MSE: 237584.9633\n",
            "Train Loss RMSE: 487.4269\n",
            "Test Loss MSE: 79266.7188\n",
            "Test RMSE: 281.5435\n",
            "Test R²: 0.9367\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 268/100\n",
            "\n",
            "Epoch 522/1000\n",
            "Train Loss MSE: 39513.8051\n",
            "Train Loss RMSE: 198.7808\n",
            "Test Loss MSE: 53355.7617\n",
            "Test RMSE: 230.9887\n",
            "Test R²: 0.9574\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 269/100\n",
            "\n",
            "Epoch 523/1000\n",
            "Train Loss MSE: 250034.7878\n",
            "Train Loss RMSE: 500.0348\n",
            "Test Loss MSE: 151212.3125\n",
            "Test RMSE: 388.8603\n",
            "Test R²: 0.8793\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 270/100\n",
            "\n",
            "Epoch 524/1000\n",
            "Train Loss MSE: 22782.0715\n",
            "Train Loss RMSE: 150.9373\n",
            "Test Loss MSE: 378297.0625\n",
            "Test RMSE: 615.0586\n",
            "Test R²: 0.6981\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 271/100\n",
            "\n",
            "Epoch 525/1000\n",
            "Train Loss MSE: 50989.2266\n",
            "Train Loss RMSE: 225.8079\n",
            "Test Loss MSE: 75587.2656\n",
            "Test RMSE: 274.9314\n",
            "Test R²: 0.9397\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 272/100\n",
            "\n",
            "Epoch 526/1000\n",
            "Train Loss MSE: 31921.4340\n",
            "Train Loss RMSE: 178.6657\n",
            "Test Loss MSE: 16765.6035\n",
            "Test RMSE: 129.4821\n",
            "Test R²: 0.9866\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 273/100\n",
            "\n",
            "Epoch 527/1000\n",
            "Train Loss MSE: 65543.2854\n",
            "Train Loss RMSE: 256.0142\n",
            "Test Loss MSE: 372719.2500\n",
            "Test RMSE: 610.5074\n",
            "Test R²: 0.7025\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 274/100\n",
            "\n",
            "Epoch 528/1000\n",
            "Train Loss MSE: 40039.0088\n",
            "Train Loss RMSE: 200.0975\n",
            "Test Loss MSE: 21046.3086\n",
            "Test RMSE: 145.0735\n",
            "Test R²: 0.9832\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 275/100\n",
            "\n",
            "Epoch 529/1000\n",
            "Train Loss MSE: 42567.7943\n",
            "Train Loss RMSE: 206.3196\n",
            "Test Loss MSE: 707217.1875\n",
            "Test RMSE: 840.9621\n",
            "Test R²: 0.4356\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 276/100\n",
            "\n",
            "Epoch 530/1000\n",
            "Train Loss MSE: 32106.6634\n",
            "Train Loss RMSE: 179.1833\n",
            "Test Loss MSE: 52895.3594\n",
            "Test RMSE: 229.9899\n",
            "Test R²: 0.9578\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 277/100\n",
            "\n",
            "Epoch 531/1000\n",
            "Train Loss MSE: 1149553.9092\n",
            "Train Loss RMSE: 1072.1725\n",
            "Test Loss MSE: 30277.7051\n",
            "Test RMSE: 174.0049\n",
            "Test R²: 0.9758\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 278/100\n",
            "\n",
            "Epoch 532/1000\n",
            "Train Loss MSE: 95151.8507\n",
            "Train Loss RMSE: 308.4669\n",
            "Test Loss MSE: 76854.9219\n",
            "Test RMSE: 277.2272\n",
            "Test R²: 0.9387\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 279/100\n",
            "\n",
            "Epoch 533/1000\n",
            "Train Loss MSE: 1098250.9568\n",
            "Train Loss RMSE: 1047.9747\n",
            "Test Loss MSE: 1127466.6250\n",
            "Test RMSE: 1061.8223\n",
            "Test R²: 0.1002\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 280/100\n",
            "\n",
            "Epoch 534/1000\n",
            "Train Loss MSE: 57053.0899\n",
            "Train Loss RMSE: 238.8579\n",
            "Test Loss MSE: 32852.7031\n",
            "Test RMSE: 181.2531\n",
            "Test R²: 0.9738\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 281/100\n",
            "\n",
            "Epoch 535/1000\n",
            "Train Loss MSE: 24095.8426\n",
            "Train Loss RMSE: 155.2284\n",
            "Test Loss MSE: 411614.0625\n",
            "Test RMSE: 641.5716\n",
            "Test R²: 0.6715\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 282/100\n",
            "\n",
            "Epoch 536/1000\n",
            "Train Loss MSE: 317135.5828\n",
            "Train Loss RMSE: 563.1479\n",
            "Test Loss MSE: 44382.6133\n",
            "Test RMSE: 210.6718\n",
            "Test R²: 0.9646\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 283/100\n",
            "\n",
            "Epoch 537/1000\n",
            "Train Loss MSE: 40283.3069\n",
            "Train Loss RMSE: 200.7070\n",
            "Test Loss MSE: 25716.5508\n",
            "Test RMSE: 160.3638\n",
            "Test R²: 0.9795\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 284/100\n",
            "\n",
            "Epoch 538/1000\n",
            "Train Loss MSE: 17126.4091\n",
            "Train Loss RMSE: 130.8679\n",
            "Test Loss MSE: 26197.1816\n",
            "Test RMSE: 161.8554\n",
            "Test R²: 0.9791\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 285/100\n",
            "\n",
            "Epoch 539/1000\n",
            "Train Loss MSE: 33876.3735\n",
            "Train Loss RMSE: 184.0554\n",
            "Test Loss MSE: 102863.5156\n",
            "Test RMSE: 320.7234\n",
            "Test R²: 0.9179\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 286/100\n",
            "\n",
            "Epoch 540/1000\n",
            "Train Loss MSE: 257553.8414\n",
            "Train Loss RMSE: 507.4976\n",
            "Test Loss MSE: 33140.0977\n",
            "Test RMSE: 182.0442\n",
            "Test R²: 0.9736\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 287/100\n",
            "\n",
            "Epoch 541/1000\n",
            "Train Loss MSE: 72545.0776\n",
            "Train Loss RMSE: 269.3419\n",
            "Test Loss MSE: 560194.3125\n",
            "Test RMSE: 748.4613\n",
            "Test R²: 0.5529\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 288/100\n",
            "\n",
            "Epoch 542/1000\n",
            "Train Loss MSE: 29537.6153\n",
            "Train Loss RMSE: 171.8651\n",
            "Test Loss MSE: 864679.0625\n",
            "Test RMSE: 929.8812\n",
            "Test R²: 0.3099\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 289/100\n",
            "\n",
            "Epoch 543/1000\n",
            "Train Loss MSE: 23815.6838\n",
            "Train Loss RMSE: 154.3233\n",
            "Test Loss MSE: 66969.4297\n",
            "Test RMSE: 258.7845\n",
            "Test R²: 0.9466\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 290/100\n",
            "\n",
            "Epoch 544/1000\n",
            "Train Loss MSE: 26612.5401\n",
            "Train Loss RMSE: 163.1335\n",
            "Test Loss MSE: 9420.0342\n",
            "Test RMSE: 97.0569\n",
            "Test R²: 0.9925\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 291/100\n",
            "\n",
            "Epoch 545/1000\n",
            "Train Loss MSE: 17312.7616\n",
            "Train Loss RMSE: 131.5780\n",
            "Test Loss MSE: 718004.1875\n",
            "Test RMSE: 847.3513\n",
            "Test R²: 0.4270\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 292/100\n",
            "\n",
            "Epoch 546/1000\n",
            "Train Loss MSE: 110266.1388\n",
            "Train Loss RMSE: 332.0635\n",
            "Test Loss MSE: 97272.8672\n",
            "Test RMSE: 311.8860\n",
            "Test R²: 0.9224\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 293/100\n",
            "\n",
            "Epoch 547/1000\n",
            "Train Loss MSE: 339129.4828\n",
            "Train Loss RMSE: 582.3482\n",
            "Test Loss MSE: 775818.5000\n",
            "Test RMSE: 880.8056\n",
            "Test R²: 0.3809\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 294/100\n",
            "\n",
            "Epoch 548/1000\n",
            "Train Loss MSE: 41472.4220\n",
            "Train Loss RMSE: 203.6478\n",
            "Test Loss MSE: 282605.4688\n",
            "Test RMSE: 531.6065\n",
            "Test R²: 0.7745\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 295/100\n",
            "\n",
            "Epoch 549/1000\n",
            "Train Loss MSE: 102477.8476\n",
            "Train Loss RMSE: 320.1216\n",
            "Test Loss MSE: 11915.5479\n",
            "Test RMSE: 109.1584\n",
            "Test R²: 0.9905\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 296/100\n",
            "\n",
            "Epoch 550/1000\n",
            "Train Loss MSE: 315744.6270\n",
            "Train Loss RMSE: 561.9116\n",
            "Test Loss MSE: 52423.4961\n",
            "Test RMSE: 228.9618\n",
            "Test R²: 0.9582\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 297/100\n",
            "\n",
            "Epoch 551/1000\n",
            "Train Loss MSE: 64346.3936\n",
            "Train Loss RMSE: 253.6659\n",
            "Test Loss MSE: 81768.4922\n",
            "Test RMSE: 285.9519\n",
            "Test R²: 0.9347\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 298/100\n",
            "\n",
            "Epoch 552/1000\n",
            "Train Loss MSE: 72991.2430\n",
            "Train Loss RMSE: 270.1689\n",
            "Test Loss MSE: 71438.2812\n",
            "Test RMSE: 267.2794\n",
            "Test R²: 0.9430\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 299/100\n",
            "\n",
            "Epoch 553/1000\n",
            "Train Loss MSE: 164569.9177\n",
            "Train Loss RMSE: 405.6722\n",
            "Test Loss MSE: 15278.8652\n",
            "Test RMSE: 123.6077\n",
            "Test R²: 0.9878\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 300/100\n",
            "\n",
            "Epoch 554/1000\n",
            "Train Loss MSE: 34665.5156\n",
            "Train Loss RMSE: 186.1868\n",
            "Test Loss MSE: 45628.1602\n",
            "Test RMSE: 213.6075\n",
            "Test R²: 0.9636\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 301/100\n",
            "\n",
            "Epoch 555/1000\n",
            "Train Loss MSE: 83115.7091\n",
            "Train Loss RMSE: 288.2980\n",
            "Test Loss MSE: 26677.9668\n",
            "Test RMSE: 163.3339\n",
            "Test R²: 0.9787\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 302/100\n",
            "\n",
            "Epoch 556/1000\n",
            "Train Loss MSE: 330944.7435\n",
            "Train Loss RMSE: 575.2780\n",
            "Test Loss MSE: 7099.2178\n",
            "Test RMSE: 84.2569\n",
            "Test R²: 0.9943\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 303/100\n",
            "\n",
            "Epoch 557/1000\n",
            "Train Loss MSE: 36741.7318\n",
            "Train Loss RMSE: 191.6813\n",
            "Test Loss MSE: 39743.6133\n",
            "Test RMSE: 199.3580\n",
            "Test R²: 0.9683\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 304/100\n",
            "\n",
            "Epoch 558/1000\n",
            "Train Loss MSE: 105014.9933\n",
            "Train Loss RMSE: 324.0602\n",
            "Test Loss MSE: 96196.1250\n",
            "Test RMSE: 310.1550\n",
            "Test R²: 0.9232\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 305/100\n",
            "\n",
            "Epoch 559/1000\n",
            "Train Loss MSE: 30805.2175\n",
            "Train Loss RMSE: 175.5142\n",
            "Test Loss MSE: 123401.3047\n",
            "Test RMSE: 351.2852\n",
            "Test R²: 0.9015\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 306/100\n",
            "\n",
            "Epoch 560/1000\n",
            "Train Loss MSE: 29111.1905\n",
            "Train Loss RMSE: 170.6200\n",
            "Test Loss MSE: 109198.6797\n",
            "Test RMSE: 330.4522\n",
            "Test R²: 0.9129\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 307/100\n",
            "\n",
            "Epoch 561/1000\n",
            "Train Loss MSE: 93850.3990\n",
            "Train Loss RMSE: 306.3501\n",
            "Test Loss MSE: 25392.3262\n",
            "Test RMSE: 159.3497\n",
            "Test R²: 0.9797\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 308/100\n",
            "\n",
            "Epoch 562/1000\n",
            "Train Loss MSE: 66410.4737\n",
            "Train Loss RMSE: 257.7023\n",
            "Test Loss MSE: 9395.1533\n",
            "Test RMSE: 96.9286\n",
            "Test R²: 0.9925\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 309/100\n",
            "\n",
            "Epoch 563/1000\n",
            "Train Loss MSE: 29428.3247\n",
            "Train Loss RMSE: 171.5469\n",
            "Test Loss MSE: 37746.2539\n",
            "Test RMSE: 194.2840\n",
            "Test R²: 0.9699\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 310/100\n",
            "\n",
            "Epoch 564/1000\n",
            "Train Loss MSE: 928402.7777\n",
            "Train Loss RMSE: 963.5366\n",
            "Test Loss MSE: 297292.9062\n",
            "Test RMSE: 545.2457\n",
            "Test R²: 0.7627\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 311/100\n",
            "\n",
            "Epoch 565/1000\n",
            "Train Loss MSE: 35747.7824\n",
            "Train Loss RMSE: 189.0708\n",
            "Test Loss MSE: 14478.4609\n",
            "Test RMSE: 120.3265\n",
            "Test R²: 0.9884\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 312/100\n",
            "\n",
            "Epoch 566/1000\n",
            "Train Loss MSE: 35457.5661\n",
            "Train Loss RMSE: 188.3018\n",
            "Test Loss MSE: 16771.9062\n",
            "Test RMSE: 129.5064\n",
            "Test R²: 0.9866\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 313/100\n",
            "\n",
            "Epoch 567/1000\n",
            "Train Loss MSE: 641439.6157\n",
            "Train Loss RMSE: 800.8993\n",
            "Test Loss MSE: 410626.7812\n",
            "Test RMSE: 640.8017\n",
            "Test R²: 0.6723\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 314/100\n",
            "\n",
            "Epoch 568/1000\n",
            "Train Loss MSE: 47686.3858\n",
            "Train Loss RMSE: 218.3721\n",
            "Test Loss MSE: 96266.9375\n",
            "Test RMSE: 310.2691\n",
            "Test R²: 0.9232\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 315/100\n",
            "\n",
            "Epoch 569/1000\n",
            "Train Loss MSE: 38443.9897\n",
            "Train Loss RMSE: 196.0714\n",
            "Test Loss MSE: 94252.1719\n",
            "Test RMSE: 307.0052\n",
            "Test R²: 0.9248\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 316/100\n",
            "\n",
            "Epoch 570/1000\n",
            "Train Loss MSE: 883086.8026\n",
            "Train Loss RMSE: 939.7270\n",
            "Test Loss MSE: 16275.5605\n",
            "Test RMSE: 127.5757\n",
            "Test R²: 0.9870\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 317/100\n",
            "\n",
            "Epoch 571/1000\n",
            "Train Loss MSE: 32512.0973\n",
            "Train Loss RMSE: 180.3111\n",
            "Test Loss MSE: 825989.2500\n",
            "Test RMSE: 908.8395\n",
            "Test R²: 0.3408\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 318/100\n",
            "\n",
            "Epoch 572/1000\n",
            "Train Loss MSE: 83074.8810\n",
            "Train Loss RMSE: 288.2271\n",
            "Test Loss MSE: 19462.2988\n",
            "Test RMSE: 139.5073\n",
            "Test R²: 0.9845\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 319/100\n",
            "\n",
            "Epoch 573/1000\n",
            "Train Loss MSE: 63123.9341\n",
            "Train Loss RMSE: 251.2448\n",
            "Test Loss MSE: 99024.2656\n",
            "Test RMSE: 314.6812\n",
            "Test R²: 0.9210\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 320/100\n",
            "\n",
            "Epoch 574/1000\n",
            "Train Loss MSE: 59375.4508\n",
            "Train Loss RMSE: 243.6708\n",
            "Test Loss MSE: 49429.9414\n",
            "Test RMSE: 222.3285\n",
            "Test R²: 0.9606\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 321/100\n",
            "\n",
            "Epoch 575/1000\n",
            "Train Loss MSE: 90339.5877\n",
            "Train Loss RMSE: 300.5654\n",
            "Test Loss MSE: 16317.0547\n",
            "Test RMSE: 127.7382\n",
            "Test R²: 0.9870\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 322/100\n",
            "\n",
            "Epoch 576/1000\n",
            "Train Loss MSE: 41370.3720\n",
            "Train Loss RMSE: 203.3971\n",
            "Test Loss MSE: 125506.3984\n",
            "Test RMSE: 354.2688\n",
            "Test R²: 0.8998\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 323/100\n",
            "\n",
            "Epoch 577/1000\n",
            "Train Loss MSE: 39794.1264\n",
            "Train Loss RMSE: 199.4847\n",
            "Test Loss MSE: 40444.5625\n",
            "Test RMSE: 201.1083\n",
            "Test R²: 0.9677\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 324/100\n",
            "\n",
            "Epoch 578/1000\n",
            "Train Loss MSE: 29971.4293\n",
            "Train Loss RMSE: 173.1226\n",
            "Test Loss MSE: 485111.6875\n",
            "Test RMSE: 696.4996\n",
            "Test R²: 0.6129\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 325/100\n",
            "\n",
            "Epoch 579/1000\n",
            "Train Loss MSE: 71754.2930\n",
            "Train Loss RMSE: 267.8699\n",
            "Test Loss MSE: 18131.0332\n",
            "Test RMSE: 134.6515\n",
            "Test R²: 0.9855\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 326/100\n",
            "\n",
            "Epoch 580/1000\n",
            "Train Loss MSE: 32308.7751\n",
            "Train Loss RMSE: 179.7464\n",
            "Test Loss MSE: 256160.1719\n",
            "Test RMSE: 506.1227\n",
            "Test R²: 0.7956\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 327/100\n",
            "\n",
            "Epoch 581/1000\n",
            "Train Loss MSE: 81240.6369\n",
            "Train Loss RMSE: 285.0274\n",
            "Test Loss MSE: 60565.3750\n",
            "Test RMSE: 246.1003\n",
            "Test R²: 0.9517\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 328/100\n",
            "\n",
            "Epoch 582/1000\n",
            "Train Loss MSE: 22636.4586\n",
            "Train Loss RMSE: 150.4542\n",
            "Test Loss MSE: 14377.3887\n",
            "Test RMSE: 119.9057\n",
            "Test R²: 0.9885\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 329/100\n",
            "\n",
            "Epoch 583/1000\n",
            "Train Loss MSE: 801800.3062\n",
            "Train Loss RMSE: 895.4330\n",
            "Test Loss MSE: 68119.3906\n",
            "Test RMSE: 260.9969\n",
            "Test R²: 0.9456\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 330/100\n",
            "\n",
            "Epoch 584/1000\n",
            "Train Loss MSE: 98453.8294\n",
            "Train Loss RMSE: 313.7735\n",
            "Test Loss MSE: 9998.3428\n",
            "Test RMSE: 99.9917\n",
            "Test R²: 0.9920\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 331/100\n",
            "\n",
            "Epoch 585/1000\n",
            "Train Loss MSE: 48704.9340\n",
            "Train Loss RMSE: 220.6919\n",
            "Test Loss MSE: 95467.0703\n",
            "Test RMSE: 308.9775\n",
            "Test R²: 0.9238\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 332/100\n",
            "\n",
            "Epoch 586/1000\n",
            "Train Loss MSE: 186595.8406\n",
            "Train Loss RMSE: 431.9674\n",
            "Test Loss MSE: 198728.9375\n",
            "Test RMSE: 445.7902\n",
            "Test R²: 0.8414\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 333/100\n",
            "\n",
            "Epoch 587/1000\n",
            "Train Loss MSE: 41218.9532\n",
            "Train Loss RMSE: 203.0245\n",
            "Test Loss MSE: 114090.7500\n",
            "Test RMSE: 337.7732\n",
            "Test R²: 0.9089\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 334/100\n",
            "\n",
            "Epoch 588/1000\n",
            "Train Loss MSE: 26887.3445\n",
            "Train Loss RMSE: 163.9736\n",
            "Test Loss MSE: 150102.2500\n",
            "Test RMSE: 387.4303\n",
            "Test R²: 0.8802\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 335/100\n",
            "\n",
            "Epoch 589/1000\n",
            "Train Loss MSE: 30471.7577\n",
            "Train Loss RMSE: 174.5616\n",
            "Test Loss MSE: 106144.4375\n",
            "Test RMSE: 325.7982\n",
            "Test R²: 0.9153\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 336/100\n",
            "\n",
            "Epoch 590/1000\n",
            "Train Loss MSE: 16489.4093\n",
            "Train Loss RMSE: 128.4111\n",
            "Test Loss MSE: 838683.8750\n",
            "Test RMSE: 915.7969\n",
            "Test R²: 0.3307\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 337/100\n",
            "\n",
            "Epoch 591/1000\n",
            "Train Loss MSE: 78361.6007\n",
            "Train Loss RMSE: 279.9314\n",
            "Test Loss MSE: 48918.2031\n",
            "Test RMSE: 221.1746\n",
            "Test R²: 0.9610\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 338/100\n",
            "\n",
            "Epoch 592/1000\n",
            "Train Loss MSE: 58887.6297\n",
            "Train Loss RMSE: 242.6677\n",
            "Test Loss MSE: 285719.4375\n",
            "Test RMSE: 534.5273\n",
            "Test R²: 0.7720\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 339/100\n",
            "\n",
            "Epoch 593/1000\n",
            "Train Loss MSE: 36113.4555\n",
            "Train Loss RMSE: 190.0354\n",
            "Test Loss MSE: 299491.8750\n",
            "Test RMSE: 547.2585\n",
            "Test R²: 0.7610\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 340/100\n",
            "\n",
            "Epoch 594/1000\n",
            "Train Loss MSE: 35591.1321\n",
            "Train Loss RMSE: 188.6561\n",
            "Test Loss MSE: 30395.1191\n",
            "Test RMSE: 174.3420\n",
            "Test R²: 0.9757\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 341/100\n",
            "\n",
            "Epoch 595/1000\n",
            "Train Loss MSE: 714855.1715\n",
            "Train Loss RMSE: 845.4911\n",
            "Test Loss MSE: 233773.0938\n",
            "Test RMSE: 483.5009\n",
            "Test R²: 0.8134\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 342/100\n",
            "\n",
            "Epoch 596/1000\n",
            "Train Loss MSE: 32912.5807\n",
            "Train Loss RMSE: 181.4182\n",
            "Test Loss MSE: 13727.2969\n",
            "Test RMSE: 117.1635\n",
            "Test R²: 0.9890\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 343/100\n",
            "\n",
            "Epoch 597/1000\n",
            "Train Loss MSE: 42491.5717\n",
            "Train Loss RMSE: 206.1348\n",
            "Test Loss MSE: 139215.3906\n",
            "Test RMSE: 373.1158\n",
            "Test R²: 0.8889\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 344/100\n",
            "\n",
            "Epoch 598/1000\n",
            "Train Loss MSE: 183854.0412\n",
            "Train Loss RMSE: 428.7820\n",
            "Test Loss MSE: 82847.7188\n",
            "Test RMSE: 287.8328\n",
            "Test R²: 0.9339\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 345/100\n",
            "\n",
            "Epoch 599/1000\n",
            "Train Loss MSE: 41505.9230\n",
            "Train Loss RMSE: 203.7300\n",
            "Test Loss MSE: 145489.1406\n",
            "Test RMSE: 381.4304\n",
            "Test R²: 0.8839\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 346/100\n",
            "\n",
            "Epoch 600/1000\n",
            "Train Loss MSE: 52518.5682\n",
            "Train Loss RMSE: 229.1693\n",
            "Test Loss MSE: 65888.2344\n",
            "Test RMSE: 256.6870\n",
            "Test R²: 0.9474\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 347/100\n",
            "\n",
            "Epoch 601/1000\n",
            "Train Loss MSE: 46798.1290\n",
            "Train Loss RMSE: 216.3288\n",
            "Test Loss MSE: 210611.2188\n",
            "Test RMSE: 458.9240\n",
            "Test R²: 0.8319\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 348/100\n",
            "\n",
            "Epoch 602/1000\n",
            "Train Loss MSE: 753365.2123\n",
            "Train Loss RMSE: 867.9661\n",
            "Test Loss MSE: 304799.8438\n",
            "Test RMSE: 552.0868\n",
            "Test R²: 0.7568\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 349/100\n",
            "\n",
            "Epoch 603/1000\n",
            "Train Loss MSE: 28415.2850\n",
            "Train Loss RMSE: 168.5683\n",
            "Test Loss MSE: 13058.7520\n",
            "Test RMSE: 114.2749\n",
            "Test R²: 0.9896\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 350/100\n",
            "\n",
            "Epoch 604/1000\n",
            "Train Loss MSE: 74823.9893\n",
            "Train Loss RMSE: 273.5397\n",
            "Test Loss MSE: 359699.4062\n",
            "Test RMSE: 599.7495\n",
            "Test R²: 0.7129\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 351/100\n",
            "\n",
            "Epoch 605/1000\n",
            "Train Loss MSE: 22778.0100\n",
            "Train Loss RMSE: 150.9239\n",
            "Test Loss MSE: 115842.7422\n",
            "Test RMSE: 340.3568\n",
            "Test R²: 0.9076\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 352/100\n",
            "\n",
            "Epoch 606/1000\n",
            "Train Loss MSE: 94720.7725\n",
            "Train Loss RMSE: 307.7674\n",
            "Test Loss MSE: 19684.5938\n",
            "Test RMSE: 140.3018\n",
            "Test R²: 0.9843\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 353/100\n",
            "\n",
            "Epoch 607/1000\n",
            "Train Loss MSE: 251900.0958\n",
            "Train Loss RMSE: 501.8965\n",
            "Test Loss MSE: 45973.4531\n",
            "Test RMSE: 214.4142\n",
            "Test R²: 0.9633\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 354/100\n",
            "\n",
            "Epoch 608/1000\n",
            "Train Loss MSE: 28681.7400\n",
            "Train Loss RMSE: 169.3568\n",
            "Test Loss MSE: 279225.8750\n",
            "Test RMSE: 528.4183\n",
            "Test R²: 0.7772\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 355/100\n",
            "\n",
            "Epoch 609/1000\n",
            "Train Loss MSE: 730715.0731\n",
            "Train Loss RMSE: 854.8187\n",
            "Test Loss MSE: 73981.5234\n",
            "Test RMSE: 271.9954\n",
            "Test R²: 0.9410\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 356/100\n",
            "\n",
            "Epoch 610/1000\n",
            "Train Loss MSE: 77098.2900\n",
            "Train Loss RMSE: 277.6658\n",
            "Test Loss MSE: 10621.4990\n",
            "Test RMSE: 103.0607\n",
            "Test R²: 0.9915\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 357/100\n",
            "\n",
            "Epoch 611/1000\n",
            "Train Loss MSE: 20450.9017\n",
            "Train Loss RMSE: 143.0066\n",
            "Test Loss MSE: 11695.9336\n",
            "Test RMSE: 108.1477\n",
            "Test R²: 0.9907\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 358/100\n",
            "\n",
            "Epoch 612/1000\n",
            "Train Loss MSE: 71569.9262\n",
            "Train Loss RMSE: 267.5256\n",
            "Test Loss MSE: 49768.6484\n",
            "Test RMSE: 223.0889\n",
            "Test R²: 0.9603\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 359/100\n",
            "\n",
            "Epoch 613/1000\n",
            "Train Loss MSE: 41128.6098\n",
            "Train Loss RMSE: 202.8019\n",
            "Test Loss MSE: 7845.0981\n",
            "Test RMSE: 88.5726\n",
            "Test R²: 0.9937\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 360/100\n",
            "\n",
            "Epoch 614/1000\n",
            "Train Loss MSE: 18944.6878\n",
            "Train Loss RMSE: 137.6397\n",
            "Test Loss MSE: 10301.3252\n",
            "Test RMSE: 101.4954\n",
            "Test R²: 0.9918\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 361/100\n",
            "\n",
            "Epoch 615/1000\n",
            "Train Loss MSE: 46542.9229\n",
            "Train Loss RMSE: 215.7381\n",
            "Test Loss MSE: 27210.5820\n",
            "Test RMSE: 164.9563\n",
            "Test R²: 0.9783\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 362/100\n",
            "\n",
            "Epoch 616/1000\n",
            "Train Loss MSE: 656404.9234\n",
            "Train Loss RMSE: 810.1882\n",
            "Test Loss MSE: 145107.1094\n",
            "Test RMSE: 380.9293\n",
            "Test R²: 0.8842\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 363/100\n",
            "\n",
            "Epoch 617/1000\n",
            "Train Loss MSE: 81633.3237\n",
            "Train Loss RMSE: 285.7155\n",
            "Test Loss MSE: 125075.3750\n",
            "Test RMSE: 353.6600\n",
            "Test R²: 0.9002\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 364/100\n",
            "\n",
            "Epoch 618/1000\n",
            "Train Loss MSE: 22203.7083\n",
            "Train Loss RMSE: 149.0091\n",
            "Test Loss MSE: 121137.9453\n",
            "Test RMSE: 348.0488\n",
            "Test R²: 0.9033\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 365/100\n",
            "\n",
            "Epoch 619/1000\n",
            "Train Loss MSE: 36078.5543\n",
            "Train Loss RMSE: 189.9436\n",
            "Test Loss MSE: 108460.6484\n",
            "Test RMSE: 329.3336\n",
            "Test R²: 0.9134\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 366/100\n",
            "\n",
            "Epoch 620/1000\n",
            "Train Loss MSE: 35871.2997\n",
            "Train Loss RMSE: 189.3972\n",
            "Test Loss MSE: 105863.0156\n",
            "Test RMSE: 325.3660\n",
            "Test R²: 0.9155\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 367/100\n",
            "\n",
            "Epoch 621/1000\n",
            "Train Loss MSE: 21564.5366\n",
            "Train Loss RMSE: 146.8487\n",
            "Test Loss MSE: 228538.7188\n",
            "Test RMSE: 478.0572\n",
            "Test R²: 0.8176\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 368/100\n",
            "\n",
            "Epoch 622/1000\n",
            "Train Loss MSE: 805803.5392\n",
            "Train Loss RMSE: 897.6656\n",
            "Test Loss MSE: 74461.0547\n",
            "Test RMSE: 272.8755\n",
            "Test R²: 0.9406\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 369/100\n",
            "\n",
            "Epoch 623/1000\n",
            "Train Loss MSE: 376682.4618\n",
            "Train Loss RMSE: 613.7446\n",
            "Test Loss MSE: 239095.2656\n",
            "Test RMSE: 488.9737\n",
            "Test R²: 0.8092\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 370/100\n",
            "\n",
            "Epoch 624/1000\n",
            "Train Loss MSE: 107046.0332\n",
            "Train Loss RMSE: 327.1789\n",
            "Test Loss MSE: 36967.1484\n",
            "Test RMSE: 192.2684\n",
            "Test R²: 0.9705\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 371/100\n",
            "\n",
            "Epoch 625/1000\n",
            "Train Loss MSE: 23042.0713\n",
            "Train Loss RMSE: 151.7962\n",
            "Test Loss MSE: 107269.3281\n",
            "Test RMSE: 327.5200\n",
            "Test R²: 0.9144\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 372/100\n",
            "\n",
            "Epoch 626/1000\n",
            "Train Loss MSE: 870954.9274\n",
            "Train Loss RMSE: 933.2497\n",
            "Test Loss MSE: 35407.6836\n",
            "Test RMSE: 188.1693\n",
            "Test R²: 0.9717\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 373/100\n",
            "\n",
            "Epoch 627/1000\n",
            "Train Loss MSE: 19015.0121\n",
            "Train Loss RMSE: 137.8949\n",
            "Test Loss MSE: 74159.8359\n",
            "Test RMSE: 272.3230\n",
            "Test R²: 0.9408\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 374/100\n",
            "\n",
            "Epoch 628/1000\n",
            "Train Loss MSE: 27183.5336\n",
            "Train Loss RMSE: 164.8743\n",
            "Test Loss MSE: 949623.7500\n",
            "Test RMSE: 974.4864\n",
            "Test R²: 0.2421\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 375/100\n",
            "\n",
            "Epoch 629/1000\n",
            "Train Loss MSE: 18744.9138\n",
            "Train Loss RMSE: 136.9121\n",
            "Test Loss MSE: 371814.1562\n",
            "Test RMSE: 609.7657\n",
            "Test R²: 0.7033\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 376/100\n",
            "\n",
            "Epoch 630/1000\n",
            "Train Loss MSE: 54573.5133\n",
            "Train Loss RMSE: 233.6097\n",
            "Test Loss MSE: 80434.2266\n",
            "Test RMSE: 283.6093\n",
            "Test R²: 0.9358\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 377/100\n",
            "\n",
            "Epoch 631/1000\n",
            "Train Loss MSE: 45850.0228\n",
            "Train Loss RMSE: 214.1262\n",
            "Test Loss MSE: 399898.3125\n",
            "Test RMSE: 632.3751\n",
            "Test R²: 0.6809\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 378/100\n",
            "\n",
            "Epoch 632/1000\n",
            "Train Loss MSE: 145011.0998\n",
            "Train Loss RMSE: 380.8032\n",
            "Test Loss MSE: 123235.9062\n",
            "Test RMSE: 351.0497\n",
            "Test R²: 0.9017\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 379/100\n",
            "\n",
            "Epoch 633/1000\n",
            "Train Loss MSE: 769785.6212\n",
            "Train Loss RMSE: 877.3743\n",
            "Test Loss MSE: 35322.4766\n",
            "Test RMSE: 187.9427\n",
            "Test R²: 0.9718\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 380/100\n",
            "\n",
            "Epoch 634/1000\n",
            "Train Loss MSE: 72986.9591\n",
            "Train Loss RMSE: 270.1610\n",
            "Test Loss MSE: 62540.2344\n",
            "Test RMSE: 250.0805\n",
            "Test R²: 0.9501\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 381/100\n",
            "\n",
            "Epoch 635/1000\n",
            "Train Loss MSE: 31183.3244\n",
            "Train Loss RMSE: 176.5880\n",
            "Test Loss MSE: 45430.6758\n",
            "Test RMSE: 213.1447\n",
            "Test R²: 0.9637\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 382/100\n",
            "\n",
            "Epoch 636/1000\n",
            "Train Loss MSE: 39530.0887\n",
            "Train Loss RMSE: 198.8218\n",
            "Test Loss MSE: 203036.9688\n",
            "Test RMSE: 450.5962\n",
            "Test R²: 0.8380\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 383/100\n",
            "\n",
            "Epoch 637/1000\n",
            "Train Loss MSE: 290950.8196\n",
            "Train Loss RMSE: 539.3986\n",
            "Test Loss MSE: 36507.2578\n",
            "Test RMSE: 191.0687\n",
            "Test R²: 0.9709\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 384/100\n",
            "\n",
            "Epoch 638/1000\n",
            "Train Loss MSE: 17984.2913\n",
            "Train Loss RMSE: 134.1055\n",
            "Test Loss MSE: 110616.9062\n",
            "Test RMSE: 332.5912\n",
            "Test R²: 0.9117\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 385/100\n",
            "\n",
            "Epoch 639/1000\n",
            "Train Loss MSE: 19892.9148\n",
            "Train Loss RMSE: 141.0422\n",
            "Test Loss MSE: 135093.9062\n",
            "Test RMSE: 367.5512\n",
            "Test R²: 0.8922\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 386/100\n",
            "\n",
            "Epoch 640/1000\n",
            "Train Loss MSE: 50897.9043\n",
            "Train Loss RMSE: 225.6056\n",
            "Test Loss MSE: 97551.6016\n",
            "Test RMSE: 312.3325\n",
            "Test R²: 0.9221\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 387/100\n",
            "\n",
            "Epoch 641/1000\n",
            "Train Loss MSE: 11591.1518\n",
            "Train Loss RMSE: 107.6622\n",
            "Test Loss MSE: 520407.6875\n",
            "Test RMSE: 721.3929\n",
            "Test R²: 0.5847\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 388/100\n",
            "\n",
            "Epoch 642/1000\n",
            "Train Loss MSE: 284127.9399\n",
            "Train Loss RMSE: 533.0365\n",
            "Test Loss MSE: 539222.3125\n",
            "Test RMSE: 734.3176\n",
            "Test R²: 0.5697\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 389/100\n",
            "\n",
            "Epoch 643/1000\n",
            "Train Loss MSE: 830319.8178\n",
            "Train Loss RMSE: 911.2189\n",
            "Test Loss MSE: 125058.9297\n",
            "Test RMSE: 353.6367\n",
            "Test R²: 0.9002\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 390/100\n",
            "\n",
            "Epoch 644/1000\n",
            "Train Loss MSE: 36413.0374\n",
            "Train Loss RMSE: 190.8220\n",
            "Test Loss MSE: 26832.2637\n",
            "Test RMSE: 163.8056\n",
            "Test R²: 0.9786\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 391/100\n",
            "\n",
            "Epoch 645/1000\n",
            "Train Loss MSE: 29933.3741\n",
            "Train Loss RMSE: 173.0126\n",
            "Test Loss MSE: 37787.5156\n",
            "Test RMSE: 194.3901\n",
            "Test R²: 0.9698\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 392/100\n",
            "\n",
            "Epoch 646/1000\n",
            "Train Loss MSE: 48312.3737\n",
            "Train Loss RMSE: 219.8008\n",
            "Test Loss MSE: 46569.2148\n",
            "Test RMSE: 215.7990\n",
            "Test R²: 0.9628\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 393/100\n",
            "\n",
            "Epoch 647/1000\n",
            "Train Loss MSE: 54033.7942\n",
            "Train Loss RMSE: 232.4517\n",
            "Test Loss MSE: 887567.2500\n",
            "Test RMSE: 942.1079\n",
            "Test R²: 0.2917\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 394/100\n",
            "\n",
            "Epoch 648/1000\n",
            "Train Loss MSE: 19916.7239\n",
            "Train Loss RMSE: 141.1266\n",
            "Test Loss MSE: 80576.6875\n",
            "Test RMSE: 283.8603\n",
            "Test R²: 0.9357\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 395/100\n",
            "\n",
            "Epoch 649/1000\n",
            "Train Loss MSE: 32158.0371\n",
            "Train Loss RMSE: 179.3266\n",
            "Test Loss MSE: 35647.8320\n",
            "Test RMSE: 188.8063\n",
            "Test R²: 0.9716\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 396/100\n",
            "\n",
            "Epoch 650/1000\n",
            "Train Loss MSE: 20608.3170\n",
            "Train Loss RMSE: 143.5560\n",
            "Test Loss MSE: 182137.8438\n",
            "Test RMSE: 426.7761\n",
            "Test R²: 0.8546\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 397/100\n",
            "\n",
            "Epoch 651/1000\n",
            "Train Loss MSE: 198823.7442\n",
            "Train Loss RMSE: 445.8966\n",
            "Test Loss MSE: 39408.5195\n",
            "Test RMSE: 198.5158\n",
            "Test R²: 0.9685\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 398/100\n",
            "\n",
            "Epoch 652/1000\n",
            "Train Loss MSE: 156245.9301\n",
            "Train Loss RMSE: 395.2796\n",
            "Test Loss MSE: 219139.6562\n",
            "Test RMSE: 468.1235\n",
            "Test R²: 0.8251\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 399/100\n",
            "\n",
            "Epoch 653/1000\n",
            "Train Loss MSE: 54114.3120\n",
            "Train Loss RMSE: 232.6248\n",
            "Test Loss MSE: 725521.0625\n",
            "Test RMSE: 851.7752\n",
            "Test R²: 0.4210\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 400/100\n",
            "\n",
            "Epoch 654/1000\n",
            "Train Loss MSE: 480688.8688\n",
            "Train Loss RMSE: 693.3173\n",
            "Test Loss MSE: 22240.1270\n",
            "Test RMSE: 149.1312\n",
            "Test R²: 0.9823\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 401/100\n",
            "\n",
            "Epoch 655/1000\n",
            "Train Loss MSE: 823260.1785\n",
            "Train Loss RMSE: 907.3369\n",
            "Test Loss MSE: 79448.1016\n",
            "Test RMSE: 281.8654\n",
            "Test R²: 0.9366\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 402/100\n",
            "\n",
            "Epoch 656/1000\n",
            "Train Loss MSE: 28972.2623\n",
            "Train Loss RMSE: 170.2124\n",
            "Test Loss MSE: 496536.1250\n",
            "Test RMSE: 704.6532\n",
            "Test R²: 0.6037\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 403/100\n",
            "\n",
            "Epoch 657/1000\n",
            "Train Loss MSE: 644152.4089\n",
            "Train Loss RMSE: 802.5911\n",
            "Test Loss MSE: 20282.4941\n",
            "Test RMSE: 142.4166\n",
            "Test R²: 0.9838\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 404/100\n",
            "\n",
            "Epoch 658/1000\n",
            "Train Loss MSE: 32742.0705\n",
            "Train Loss RMSE: 180.9477\n",
            "Test Loss MSE: 159581.2656\n",
            "Test RMSE: 399.4762\n",
            "Test R²: 0.8726\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 405/100\n",
            "\n",
            "Epoch 659/1000\n",
            "Train Loss MSE: 24584.6855\n",
            "Train Loss RMSE: 156.7950\n",
            "Test Loss MSE: 355600.0312\n",
            "Test RMSE: 596.3221\n",
            "Test R²: 0.7162\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 406/100\n",
            "\n",
            "Epoch 660/1000\n",
            "Train Loss MSE: 34279.9501\n",
            "Train Loss RMSE: 185.1485\n",
            "Test Loss MSE: 89212.5781\n",
            "Test RMSE: 298.6847\n",
            "Test R²: 0.9288\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 407/100\n",
            "\n",
            "Epoch 661/1000\n",
            "Train Loss MSE: 37688.3522\n",
            "Train Loss RMSE: 194.1349\n",
            "Test Loss MSE: 523697.6875\n",
            "Test RMSE: 723.6696\n",
            "Test R²: 0.5821\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 408/100\n",
            "\n",
            "Epoch 662/1000\n",
            "Train Loss MSE: 40836.2127\n",
            "Train Loss RMSE: 202.0797\n",
            "Test Loss MSE: 73626.3672\n",
            "Test RMSE: 271.3418\n",
            "Test R²: 0.9412\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 409/100\n",
            "\n",
            "Epoch 663/1000\n",
            "Train Loss MSE: 23173.0054\n",
            "Train Loss RMSE: 152.2268\n",
            "Test Loss MSE: 10291.1758\n",
            "Test RMSE: 101.4454\n",
            "Test R²: 0.9918\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 410/100\n",
            "\n",
            "Epoch 664/1000\n",
            "Train Loss MSE: 23699.2265\n",
            "Train Loss RMSE: 153.9455\n",
            "Test Loss MSE: 214986.7812\n",
            "Test RMSE: 463.6667\n",
            "Test R²: 0.8284\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 411/100\n",
            "\n",
            "Epoch 665/1000\n",
            "Train Loss MSE: 23816.0580\n",
            "Train Loss RMSE: 154.3245\n",
            "Test Loss MSE: 336593.3438\n",
            "Test RMSE: 580.1667\n",
            "Test R²: 0.7314\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 412/100\n",
            "\n",
            "Epoch 666/1000\n",
            "Train Loss MSE: 56786.5521\n",
            "Train Loss RMSE: 238.2993\n",
            "Test Loss MSE: 119910.1953\n",
            "Test RMSE: 346.2805\n",
            "Test R²: 0.9043\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 413/100\n",
            "\n",
            "Epoch 667/1000\n",
            "Train Loss MSE: 623048.3187\n",
            "Train Loss RMSE: 789.3341\n",
            "Test Loss MSE: 482347.0938\n",
            "Test RMSE: 694.5121\n",
            "Test R²: 0.6151\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 414/100\n",
            "\n",
            "Epoch 668/1000\n",
            "Train Loss MSE: 47412.5696\n",
            "Train Loss RMSE: 217.7443\n",
            "Test Loss MSE: 8228.9707\n",
            "Test RMSE: 90.7137\n",
            "Test R²: 0.9934\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 415/100\n",
            "\n",
            "Epoch 669/1000\n",
            "Train Loss MSE: 18808.3220\n",
            "Train Loss RMSE: 137.1434\n",
            "Test Loss MSE: 771238.0625\n",
            "Test RMSE: 878.2016\n",
            "Test R²: 0.3845\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 416/100\n",
            "\n",
            "Epoch 670/1000\n",
            "Train Loss MSE: 91866.4070\n",
            "Train Loss RMSE: 303.0947\n",
            "Test Loss MSE: 160854.5000\n",
            "Test RMSE: 401.0667\n",
            "Test R²: 0.8716\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 417/100\n",
            "\n",
            "Epoch 671/1000\n",
            "Train Loss MSE: 207684.8812\n",
            "Train Loss RMSE: 455.7246\n",
            "Test Loss MSE: 77817.7266\n",
            "Test RMSE: 278.9583\n",
            "Test R²: 0.9379\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 418/100\n",
            "\n",
            "Epoch 672/1000\n",
            "Train Loss MSE: 23157.3654\n",
            "Train Loss RMSE: 152.1754\n",
            "Test Loss MSE: 34853.2734\n",
            "Test RMSE: 186.6903\n",
            "Test R²: 0.9722\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 419/100\n",
            "\n",
            "Epoch 673/1000\n",
            "Train Loss MSE: 34484.9472\n",
            "Train Loss RMSE: 185.7012\n",
            "Test Loss MSE: 233990.0000\n",
            "Test RMSE: 483.7251\n",
            "Test R²: 0.8133\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 420/100\n",
            "\n",
            "Epoch 674/1000\n",
            "Train Loss MSE: 65775.5496\n",
            "Train Loss RMSE: 256.4674\n",
            "Test Loss MSE: 151015.2812\n",
            "Test RMSE: 388.6068\n",
            "Test R²: 0.8795\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 421/100\n",
            "\n",
            "Epoch 675/1000\n",
            "Train Loss MSE: 250779.5424\n",
            "Train Loss RMSE: 500.7789\n",
            "Test Loss MSE: 65553.5000\n",
            "Test RMSE: 256.0342\n",
            "Test R²: 0.9477\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 422/100\n",
            "\n",
            "Epoch 676/1000\n",
            "Train Loss MSE: 23741.2520\n",
            "Train Loss RMSE: 154.0820\n",
            "Test Loss MSE: 28471.9395\n",
            "Test RMSE: 168.7363\n",
            "Test R²: 0.9773\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 423/100\n",
            "\n",
            "Epoch 677/1000\n",
            "Train Loss MSE: 88927.9284\n",
            "Train Loss RMSE: 298.2079\n",
            "Test Loss MSE: 613441.5000\n",
            "Test RMSE: 783.2251\n",
            "Test R²: 0.5104\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 424/100\n",
            "\n",
            "Epoch 678/1000\n",
            "Train Loss MSE: 15998499.3460\n",
            "Train Loss RMSE: 3999.8124\n",
            "Test Loss MSE: 1235265.0000\n",
            "Test RMSE: 1111.4248\n",
            "Test R²: 0.0142\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 425/100\n",
            "\n",
            "Epoch 679/1000\n",
            "Train Loss MSE: 18589.2199\n",
            "Train Loss RMSE: 136.3423\n",
            "Test Loss MSE: 24807.1523\n",
            "Test RMSE: 157.5029\n",
            "Test R²: 0.9802\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 426/100\n",
            "\n",
            "Epoch 680/1000\n",
            "Train Loss MSE: 267341.6567\n",
            "Train Loss RMSE: 517.0509\n",
            "Test Loss MSE: 289985.5625\n",
            "Test RMSE: 538.5031\n",
            "Test R²: 0.7686\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 427/100\n",
            "\n",
            "Epoch 681/1000\n",
            "Train Loss MSE: 21824.0350\n",
            "Train Loss RMSE: 147.7296\n",
            "Test Loss MSE: 117967.6875\n",
            "Test RMSE: 343.4642\n",
            "Test R²: 0.9059\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 428/100\n",
            "\n",
            "Epoch 682/1000\n",
            "Train Loss MSE: 41175.4715\n",
            "Train Loss RMSE: 202.9174\n",
            "Test Loss MSE: 9478.9785\n",
            "Test RMSE: 97.3600\n",
            "Test R²: 0.9924\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 429/100\n",
            "\n",
            "Epoch 683/1000\n",
            "Train Loss MSE: 296579.5102\n",
            "Train Loss RMSE: 544.5911\n",
            "Test Loss MSE: 355086.4375\n",
            "Test RMSE: 595.8913\n",
            "Test R²: 0.7166\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 430/100\n",
            "\n",
            "Epoch 684/1000\n",
            "Train Loss MSE: 25127.1665\n",
            "Train Loss RMSE: 158.5155\n",
            "Test Loss MSE: 79677.7188\n",
            "Test RMSE: 282.2724\n",
            "Test R²: 0.9364\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 431/100\n",
            "\n",
            "Epoch 685/1000\n",
            "Train Loss MSE: 17724.7272\n",
            "Train Loss RMSE: 133.1342\n",
            "Test Loss MSE: 50568.9023\n",
            "Test RMSE: 224.8753\n",
            "Test R²: 0.9596\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 432/100\n",
            "\n",
            "Epoch 686/1000\n",
            "Train Loss MSE: 64364.8512\n",
            "Train Loss RMSE: 253.7023\n",
            "Test Loss MSE: 172251.1719\n",
            "Test RMSE: 415.0315\n",
            "Test R²: 0.8625\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 433/100\n",
            "\n",
            "Epoch 687/1000\n",
            "Train Loss MSE: 81036.6237\n",
            "Train Loss RMSE: 284.6693\n",
            "Test Loss MSE: 95060.6094\n",
            "Test RMSE: 308.3190\n",
            "Test R²: 0.9241\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 434/100\n",
            "\n",
            "Epoch 688/1000\n",
            "Train Loss MSE: 34500.0168\n",
            "Train Loss RMSE: 185.7418\n",
            "Test Loss MSE: 438762.9688\n",
            "Test RMSE: 662.3919\n",
            "Test R²: 0.6498\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 435/100\n",
            "\n",
            "Epoch 689/1000\n",
            "Train Loss MSE: 136120.8587\n",
            "Train Loss RMSE: 368.9456\n",
            "Test Loss MSE: 40445.3477\n",
            "Test RMSE: 201.1103\n",
            "Test R²: 0.9677\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 436/100\n",
            "\n",
            "Epoch 690/1000\n",
            "Train Loss MSE: 27898.9411\n",
            "Train Loss RMSE: 167.0298\n",
            "Test Loss MSE: 166899.9531\n",
            "Test RMSE: 408.5339\n",
            "Test R²: 0.8668\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 437/100\n",
            "\n",
            "Epoch 691/1000\n",
            "Train Loss MSE: 81493.3295\n",
            "Train Loss RMSE: 285.4704\n",
            "Test Loss MSE: 45826.5977\n",
            "Test RMSE: 214.0715\n",
            "Test R²: 0.9634\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 438/100\n",
            "\n",
            "Epoch 692/1000\n",
            "Train Loss MSE: 171617.6607\n",
            "Train Loss RMSE: 414.2676\n",
            "Test Loss MSE: 76490.4531\n",
            "Test RMSE: 276.5691\n",
            "Test R²: 0.9390\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 439/100\n",
            "\n",
            "Epoch 693/1000\n",
            "Train Loss MSE: 239717.8463\n",
            "Train Loss RMSE: 489.6099\n",
            "Test Loss MSE: 175720.2031\n",
            "Test RMSE: 419.1899\n",
            "Test R²: 0.8598\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 440/100\n",
            "\n",
            "Epoch 694/1000\n",
            "Train Loss MSE: 100661.4764\n",
            "Train Loss RMSE: 317.2719\n",
            "Test Loss MSE: 10188.5518\n",
            "Test RMSE: 100.9384\n",
            "Test R²: 0.9919\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 441/100\n",
            "\n",
            "Epoch 695/1000\n",
            "Train Loss MSE: 44657.8608\n",
            "Train Loss RMSE: 211.3241\n",
            "Test Loss MSE: 255548.1094\n",
            "Test RMSE: 505.5177\n",
            "Test R²: 0.7961\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 442/100\n",
            "\n",
            "Epoch 696/1000\n",
            "Train Loss MSE: 33846.9960\n",
            "Train Loss RMSE: 183.9755\n",
            "Test Loss MSE: 851482.8125\n",
            "Test RMSE: 922.7583\n",
            "Test R²: 0.3205\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 443/100\n",
            "\n",
            "Epoch 697/1000\n",
            "Train Loss MSE: 54677.3053\n",
            "Train Loss RMSE: 233.8318\n",
            "Test Loss MSE: 48787.3633\n",
            "Test RMSE: 220.8786\n",
            "Test R²: 0.9611\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 444/100\n",
            "\n",
            "Epoch 698/1000\n",
            "Train Loss MSE: 232050.3158\n",
            "Train Loss RMSE: 481.7160\n",
            "Test Loss MSE: 52275.5977\n",
            "Test RMSE: 228.6386\n",
            "Test R²: 0.9583\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 445/100\n",
            "\n",
            "Epoch 699/1000\n",
            "Train Loss MSE: 32488.7396\n",
            "Train Loss RMSE: 180.2463\n",
            "Test Loss MSE: 102503.3281\n",
            "Test RMSE: 320.1614\n",
            "Test R²: 0.9182\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 446/100\n",
            "\n",
            "Epoch 700/1000\n",
            "Train Loss MSE: 832669.5359\n",
            "Train Loss RMSE: 912.5073\n",
            "Test Loss MSE: 168958.7656\n",
            "Test RMSE: 411.0459\n",
            "Test R²: 0.8652\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 447/100\n",
            "\n",
            "Epoch 701/1000\n",
            "Train Loss MSE: 972803.2401\n",
            "Train Loss RMSE: 986.3079\n",
            "Test Loss MSE: 454868.6250\n",
            "Test RMSE: 674.4395\n",
            "Test R²: 0.6370\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 448/100\n",
            "\n",
            "Epoch 702/1000\n",
            "Train Loss MSE: 707448.5918\n",
            "Train Loss RMSE: 841.0996\n",
            "Test Loss MSE: 12122.2393\n",
            "Test RMSE: 110.1010\n",
            "Test R²: 0.9903\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 449/100\n",
            "\n",
            "Epoch 703/1000\n",
            "Train Loss MSE: 19930.7377\n",
            "Train Loss RMSE: 141.1763\n",
            "Test Loss MSE: 141980.1250\n",
            "Test RMSE: 376.8025\n",
            "Test R²: 0.8867\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 450/100\n",
            "\n",
            "Epoch 704/1000\n",
            "Train Loss MSE: 159332.6061\n",
            "Train Loss RMSE: 399.1649\n",
            "Test Loss MSE: 60777.4375\n",
            "Test RMSE: 246.5308\n",
            "Test R²: 0.9515\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 451/100\n",
            "\n",
            "Epoch 705/1000\n",
            "Train Loss MSE: 42806.9538\n",
            "Train Loss RMSE: 206.8984\n",
            "Test Loss MSE: 115229.2109\n",
            "Test RMSE: 339.4543\n",
            "Test R²: 0.9080\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 452/100\n",
            "\n",
            "Epoch 706/1000\n",
            "Train Loss MSE: 35263.5816\n",
            "Train Loss RMSE: 187.7860\n",
            "Test Loss MSE: 465451.4062\n",
            "Test RMSE: 682.2400\n",
            "Test R²: 0.6285\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 453/100\n",
            "\n",
            "Epoch 707/1000\n",
            "Train Loss MSE: 70918.1466\n",
            "Train Loss RMSE: 266.3046\n",
            "Test Loss MSE: 52386.0234\n",
            "Test RMSE: 228.8799\n",
            "Test R²: 0.9582\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 454/100\n",
            "\n",
            "Epoch 708/1000\n",
            "Train Loss MSE: 896619.6001\n",
            "Train Loss RMSE: 946.9000\n",
            "Test Loss MSE: 80811.5781\n",
            "Test RMSE: 284.2738\n",
            "Test R²: 0.9355\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 455/100\n",
            "\n",
            "Epoch 709/1000\n",
            "Train Loss MSE: 1000907.2861\n",
            "Train Loss RMSE: 1000.4535\n",
            "Test Loss MSE: 33228.2852\n",
            "Test RMSE: 182.2863\n",
            "Test R²: 0.9735\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 456/100\n",
            "\n",
            "Epoch 710/1000\n",
            "Train Loss MSE: 176360.0431\n",
            "Train Loss RMSE: 419.9524\n",
            "Test Loss MSE: 72825.6484\n",
            "Test RMSE: 269.8623\n",
            "Test R²: 0.9419\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 457/100\n",
            "\n",
            "Epoch 711/1000\n",
            "Train Loss MSE: 30264.5046\n",
            "Train Loss RMSE: 173.9670\n",
            "Test Loss MSE: 143872.9062\n",
            "Test RMSE: 379.3058\n",
            "Test R²: 0.8852\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 458/100\n",
            "\n",
            "Epoch 712/1000\n",
            "Train Loss MSE: 27445.2660\n",
            "Train Loss RMSE: 165.6661\n",
            "Test Loss MSE: 161006.4375\n",
            "Test RMSE: 401.2561\n",
            "Test R²: 0.8715\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 459/100\n",
            "\n",
            "Epoch 713/1000\n",
            "Train Loss MSE: 16181.3500\n",
            "Train Loss RMSE: 127.2059\n",
            "Test Loss MSE: 506352.0625\n",
            "Test RMSE: 711.5842\n",
            "Test R²: 0.5959\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 460/100\n",
            "\n",
            "Epoch 714/1000\n",
            "Train Loss MSE: 22493.0814\n",
            "Train Loss RMSE: 149.9769\n",
            "Test Loss MSE: 883825.0000\n",
            "Test RMSE: 940.1197\n",
            "Test R²: 0.2947\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 461/100\n",
            "\n",
            "Epoch 715/1000\n",
            "Train Loss MSE: 83711.9002\n",
            "Train Loss RMSE: 289.3301\n",
            "Test Loss MSE: 164106.2031\n",
            "Test RMSE: 405.1002\n",
            "Test R²: 0.8690\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 462/100\n",
            "\n",
            "Epoch 716/1000\n",
            "Train Loss MSE: 65323.0507\n",
            "Train Loss RMSE: 255.5837\n",
            "Test Loss MSE: 88065.5312\n",
            "Test RMSE: 296.7584\n",
            "Test R²: 0.9297\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 463/100\n",
            "\n",
            "Epoch 717/1000\n",
            "Train Loss MSE: 23517.3007\n",
            "Train Loss RMSE: 153.3535\n",
            "Test Loss MSE: 120453.3047\n",
            "Test RMSE: 347.0638\n",
            "Test R²: 0.9039\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 464/100\n",
            "\n",
            "Epoch 718/1000\n",
            "Train Loss MSE: 129316.0479\n",
            "Train Loss RMSE: 359.6054\n",
            "Test Loss MSE: 89857.3594\n",
            "Test RMSE: 299.7622\n",
            "Test R²: 0.9283\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 465/100\n",
            "\n",
            "Epoch 719/1000\n",
            "Train Loss MSE: 18420.5869\n",
            "Train Loss RMSE: 135.7225\n",
            "Test Loss MSE: 37604.2109\n",
            "Test RMSE: 193.9181\n",
            "Test R²: 0.9700\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 466/100\n",
            "\n",
            "Epoch 720/1000\n",
            "Train Loss MSE: 85420.3495\n",
            "Train Loss RMSE: 292.2676\n",
            "Test Loss MSE: 81849.0625\n",
            "Test RMSE: 286.0928\n",
            "Test R²: 0.9347\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 467/100\n",
            "\n",
            "Epoch 721/1000\n",
            "Train Loss MSE: 143255.0179\n",
            "Train Loss RMSE: 378.4904\n",
            "Test Loss MSE: 20080.4766\n",
            "Test RMSE: 141.7056\n",
            "Test R²: 0.9840\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 468/100\n",
            "\n",
            "Epoch 722/1000\n",
            "Train Loss MSE: 24670.6162\n",
            "Train Loss RMSE: 157.0688\n",
            "Test Loss MSE: 871728.4375\n",
            "Test RMSE: 933.6640\n",
            "Test R²: 0.3043\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 469/100\n",
            "\n",
            "Epoch 723/1000\n",
            "Train Loss MSE: 882070.3713\n",
            "Train Loss RMSE: 939.1860\n",
            "Test Loss MSE: 133764.0312\n",
            "Test RMSE: 365.7377\n",
            "Test R²: 0.8932\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 470/100\n",
            "\n",
            "Epoch 724/1000\n",
            "Train Loss MSE: 65974.5350\n",
            "Train Loss RMSE: 256.8551\n",
            "Test Loss MSE: 43118.0508\n",
            "Test RMSE: 207.6489\n",
            "Test R²: 0.9656\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 471/100\n",
            "\n",
            "Epoch 725/1000\n",
            "Train Loss MSE: 147567.7573\n",
            "Train Loss RMSE: 384.1455\n",
            "Test Loss MSE: 13372.1074\n",
            "Test RMSE: 115.6378\n",
            "Test R²: 0.9893\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 472/100\n",
            "\n",
            "Epoch 726/1000\n",
            "Train Loss MSE: 327573.8848\n",
            "Train Loss RMSE: 572.3407\n",
            "Test Loss MSE: 107887.9375\n",
            "Test RMSE: 328.4630\n",
            "Test R²: 0.9139\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 473/100\n",
            "\n",
            "Epoch 727/1000\n",
            "Train Loss MSE: 60478.5260\n",
            "Train Loss RMSE: 245.9238\n",
            "Test Loss MSE: 62065.9570\n",
            "Test RMSE: 249.1304\n",
            "Test R²: 0.9505\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 474/100\n",
            "\n",
            "Epoch 728/1000\n",
            "Train Loss MSE: 24526.0277\n",
            "Train Loss RMSE: 156.6079\n",
            "Test Loss MSE: 53726.0742\n",
            "Test RMSE: 231.7889\n",
            "Test R²: 0.9571\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 475/100\n",
            "\n",
            "Epoch 729/1000\n",
            "Train Loss MSE: 42419.4826\n",
            "Train Loss RMSE: 205.9599\n",
            "Test Loss MSE: 306388.2500\n",
            "Test RMSE: 553.5235\n",
            "Test R²: 0.7555\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 476/100\n",
            "\n",
            "Epoch 730/1000\n",
            "Train Loss MSE: 129344.0067\n",
            "Train Loss RMSE: 359.6443\n",
            "Test Loss MSE: 14491.6660\n",
            "Test RMSE: 120.3813\n",
            "Test R²: 0.9884\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 477/100\n",
            "\n",
            "Epoch 731/1000\n",
            "Train Loss MSE: 60050.7263\n",
            "Train Loss RMSE: 245.0525\n",
            "Test Loss MSE: 65544.2578\n",
            "Test RMSE: 256.0161\n",
            "Test R²: 0.9477\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 478/100\n",
            "\n",
            "Epoch 732/1000\n",
            "Train Loss MSE: 110749.9940\n",
            "Train Loss RMSE: 332.7912\n",
            "Test Loss MSE: 146686.7031\n",
            "Test RMSE: 382.9970\n",
            "Test R²: 0.8829\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 479/100\n",
            "\n",
            "Epoch 733/1000\n",
            "Train Loss MSE: 28768.4015\n",
            "Train Loss RMSE: 169.6125\n",
            "Test Loss MSE: 255486.4062\n",
            "Test RMSE: 505.4566\n",
            "Test R²: 0.7961\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 480/100\n",
            "\n",
            "Epoch 734/1000\n",
            "Train Loss MSE: 127562.1448\n",
            "Train Loss RMSE: 357.1584\n",
            "Test Loss MSE: 225596.1562\n",
            "Test RMSE: 474.9696\n",
            "Test R²: 0.8200\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 481/100\n",
            "\n",
            "Epoch 735/1000\n",
            "Train Loss MSE: 42260.8042\n",
            "Train Loss RMSE: 205.5743\n",
            "Test Loss MSE: 28506.7109\n",
            "Test RMSE: 168.8393\n",
            "Test R²: 0.9772\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 482/100\n",
            "\n",
            "Epoch 736/1000\n",
            "Train Loss MSE: 994981.5425\n",
            "Train Loss RMSE: 997.4876\n",
            "Test Loss MSE: 72546.7109\n",
            "Test RMSE: 269.3450\n",
            "Test R²: 0.9421\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 483/100\n",
            "\n",
            "Epoch 737/1000\n",
            "Train Loss MSE: 38682.9754\n",
            "Train Loss RMSE: 196.6799\n",
            "Test Loss MSE: 25954.5195\n",
            "Test RMSE: 161.1041\n",
            "Test R²: 0.9793\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 484/100\n",
            "\n",
            "Epoch 738/1000\n",
            "Train Loss MSE: 257032.1043\n",
            "Train Loss RMSE: 506.9833\n",
            "Test Loss MSE: 28412.5000\n",
            "Test RMSE: 168.5601\n",
            "Test R²: 0.9773\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 485/100\n",
            "\n",
            "Epoch 739/1000\n",
            "Train Loss MSE: 24515.2909\n",
            "Train Loss RMSE: 156.5736\n",
            "Test Loss MSE: 623311.1875\n",
            "Test RMSE: 789.5006\n",
            "Test R²: 0.5026\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 486/100\n",
            "\n",
            "Epoch 740/1000\n",
            "Train Loss MSE: 32348.5826\n",
            "Train Loss RMSE: 179.8571\n",
            "Test Loss MSE: 733834.5000\n",
            "Test RMSE: 856.6414\n",
            "Test R²: 0.4144\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 487/100\n",
            "\n",
            "Epoch 741/1000\n",
            "Train Loss MSE: 16085.7736\n",
            "Train Loss RMSE: 126.8297\n",
            "Test Loss MSE: 51456.6133\n",
            "Test RMSE: 226.8405\n",
            "Test R²: 0.9589\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 488/100\n",
            "\n",
            "Epoch 742/1000\n",
            "Train Loss MSE: 84216.5744\n",
            "Train Loss RMSE: 290.2009\n",
            "Test Loss MSE: 428675.8438\n",
            "Test RMSE: 654.7334\n",
            "Test R²: 0.6579\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 489/100\n",
            "\n",
            "Epoch 743/1000\n",
            "Train Loss MSE: 40216.9829\n",
            "Train Loss RMSE: 200.5417\n",
            "Test Loss MSE: 25361.7246\n",
            "Test RMSE: 159.2536\n",
            "Test R²: 0.9798\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 490/100\n",
            "\n",
            "Epoch 744/1000\n",
            "Train Loss MSE: 171274.2083\n",
            "Train Loss RMSE: 413.8529\n",
            "Test Loss MSE: 575645.5625\n",
            "Test RMSE: 758.7131\n",
            "Test R²: 0.5406\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 491/100\n",
            "\n",
            "Epoch 745/1000\n",
            "Train Loss MSE: 25816.8055\n",
            "Train Loss RMSE: 160.6761\n",
            "Test Loss MSE: 89643.2969\n",
            "Test RMSE: 299.4049\n",
            "Test R²: 0.9285\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 492/100\n",
            "\n",
            "Epoch 746/1000\n",
            "Train Loss MSE: 54966.7447\n",
            "Train Loss RMSE: 234.4499\n",
            "Test Loss MSE: 380093.1562\n",
            "Test RMSE: 616.5170\n",
            "Test R²: 0.6967\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 493/100\n",
            "\n",
            "Epoch 747/1000\n",
            "Train Loss MSE: 847244.5615\n",
            "Train Loss RMSE: 920.4589\n",
            "Test Loss MSE: 270289.1562\n",
            "Test RMSE: 519.8934\n",
            "Test R²: 0.7843\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 494/100\n",
            "\n",
            "Epoch 748/1000\n",
            "Train Loss MSE: 26608.4751\n",
            "Train Loss RMSE: 163.1210\n",
            "Test Loss MSE: 77338.0938\n",
            "Test RMSE: 278.0973\n",
            "Test R²: 0.9383\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 495/100\n",
            "\n",
            "Epoch 749/1000\n",
            "Train Loss MSE: 129885.1750\n",
            "Train Loss RMSE: 360.3959\n",
            "Test Loss MSE: 28755.1055\n",
            "Test RMSE: 169.5733\n",
            "Test R²: 0.9771\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 496/100\n",
            "\n",
            "Epoch 750/1000\n",
            "Train Loss MSE: 26520.0770\n",
            "Train Loss RMSE: 162.8499\n",
            "Test Loss MSE: 98124.6797\n",
            "Test RMSE: 313.2486\n",
            "Test R²: 0.9217\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 497/100\n",
            "\n",
            "Epoch 751/1000\n",
            "Train Loss MSE: 23461.3660\n",
            "Train Loss RMSE: 153.1710\n",
            "Test Loss MSE: 10187.7393\n",
            "Test RMSE: 100.9343\n",
            "Test R²: 0.9919\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 498/100\n",
            "\n",
            "Epoch 752/1000\n",
            "Train Loss MSE: 42531.5841\n",
            "Train Loss RMSE: 206.2319\n",
            "Test Loss MSE: 128725.3906\n",
            "Test RMSE: 358.7832\n",
            "Test R²: 0.8973\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 499/100\n",
            "\n",
            "Epoch 753/1000\n",
            "Train Loss MSE: 77189.6971\n",
            "Train Loss RMSE: 277.8303\n",
            "Test Loss MSE: 86535.0156\n",
            "Test RMSE: 294.1683\n",
            "Test R²: 0.9309\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 500/100\n",
            "\n",
            "Epoch 754/1000\n",
            "Train Loss MSE: 74686.9133\n",
            "Train Loss RMSE: 273.2891\n",
            "Test Loss MSE: 155703.4062\n",
            "Test RMSE: 394.5927\n",
            "Test R²: 0.8757\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 501/100\n",
            "\n",
            "Epoch 755/1000\n",
            "Train Loss MSE: 26487.1236\n",
            "Train Loss RMSE: 162.7487\n",
            "Test Loss MSE: 891586.0000\n",
            "Test RMSE: 944.2383\n",
            "Test R²: 0.2885\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 502/100\n",
            "\n",
            "Epoch 756/1000\n",
            "Train Loss MSE: 31645.7887\n",
            "Train Loss RMSE: 177.8926\n",
            "Test Loss MSE: 72232.5781\n",
            "Test RMSE: 268.7612\n",
            "Test R²: 0.9424\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 503/100\n",
            "\n",
            "Epoch 757/1000\n",
            "Train Loss MSE: 64828.5899\n",
            "Train Loss RMSE: 254.6146\n",
            "Test Loss MSE: 28402.0156\n",
            "Test RMSE: 168.5290\n",
            "Test R²: 0.9773\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 504/100\n",
            "\n",
            "Epoch 758/1000\n",
            "Train Loss MSE: 108188.7463\n",
            "Train Loss RMSE: 328.9206\n",
            "Test Loss MSE: 127481.6562\n",
            "Test RMSE: 357.0457\n",
            "Test R²: 0.8983\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 505/100\n",
            "\n",
            "Epoch 759/1000\n",
            "Train Loss MSE: 780909.6724\n",
            "Train Loss RMSE: 883.6909\n",
            "Test Loss MSE: 119363.0391\n",
            "Test RMSE: 345.4896\n",
            "Test R²: 0.9047\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 506/100\n",
            "\n",
            "Epoch 760/1000\n",
            "Train Loss MSE: 28823.6274\n",
            "Train Loss RMSE: 169.7752\n",
            "Test Loss MSE: 337314.2812\n",
            "Test RMSE: 580.7876\n",
            "Test R²: 0.7308\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 507/100\n",
            "\n",
            "Epoch 761/1000\n",
            "Train Loss MSE: 31176.8107\n",
            "Train Loss RMSE: 176.5696\n",
            "Test Loss MSE: 830305.1250\n",
            "Test RMSE: 911.2108\n",
            "Test R²: 0.3374\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 508/100\n",
            "\n",
            "Epoch 762/1000\n",
            "Train Loss MSE: 34452.1889\n",
            "Train Loss RMSE: 185.6130\n",
            "Test Loss MSE: 179905.9219\n",
            "Test RMSE: 424.1532\n",
            "Test R²: 0.8564\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 509/100\n",
            "\n",
            "Epoch 763/1000\n",
            "Train Loss MSE: 1019154.6178\n",
            "Train Loss RMSE: 1009.5319\n",
            "Test Loss MSE: 536528.6250\n",
            "Test RMSE: 732.4811\n",
            "Test R²: 0.5718\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 510/100\n",
            "\n",
            "Epoch 764/1000\n",
            "Train Loss MSE: 62798.6090\n",
            "Train Loss RMSE: 250.5965\n",
            "Test Loss MSE: 203666.8438\n",
            "Test RMSE: 451.2946\n",
            "Test R²: 0.8375\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 511/100\n",
            "\n",
            "Epoch 765/1000\n",
            "Train Loss MSE: 42037.4278\n",
            "Train Loss RMSE: 205.0303\n",
            "Test Loss MSE: 44293.4375\n",
            "Test RMSE: 210.4601\n",
            "Test R²: 0.9647\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 512/100\n",
            "\n",
            "Epoch 766/1000\n",
            "Train Loss MSE: 836056.1385\n",
            "Train Loss RMSE: 914.3611\n",
            "Test Loss MSE: 101813.4609\n",
            "Test RMSE: 319.0822\n",
            "Test R²: 0.9187\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 513/100\n",
            "\n",
            "Epoch 767/1000\n",
            "Train Loss MSE: 963804.8395\n",
            "Train Loss RMSE: 981.7356\n",
            "Test Loss MSE: 25500.1211\n",
            "Test RMSE: 159.6876\n",
            "Test R²: 0.9796\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 514/100\n",
            "\n",
            "Epoch 768/1000\n",
            "Train Loss MSE: 56504.1960\n",
            "Train Loss RMSE: 237.7061\n",
            "Test Loss MSE: 108990.7188\n",
            "Test RMSE: 330.1374\n",
            "Test R²: 0.9130\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 515/100\n",
            "\n",
            "Epoch 769/1000\n",
            "Train Loss MSE: 19741.5326\n",
            "Train Loss RMSE: 140.5046\n",
            "Test Loss MSE: 104386.7109\n",
            "Test RMSE: 323.0893\n",
            "Test R²: 0.9167\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 516/100\n",
            "\n",
            "Epoch 770/1000\n",
            "Train Loss MSE: 829249.4772\n",
            "Train Loss RMSE: 910.6314\n",
            "Test Loss MSE: 142099.6719\n",
            "Test RMSE: 376.9611\n",
            "Test R²: 0.8866\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 517/100\n",
            "\n",
            "Epoch 771/1000\n",
            "Train Loss MSE: 32426.2151\n",
            "Train Loss RMSE: 180.0728\n",
            "Test Loss MSE: 7477.4688\n",
            "Test RMSE: 86.4724\n",
            "Test R²: 0.9940\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 518/100\n",
            "\n",
            "Epoch 772/1000\n",
            "Train Loss MSE: 77093.2629\n",
            "Train Loss RMSE: 277.6567\n",
            "Test Loss MSE: 6761.6206\n",
            "Test RMSE: 82.2291\n",
            "Test R²: 0.9946\n",
            "Learning Rate: 1.00e-02\n",
            "✓ New best model! Improvement: 85.9907\n",
            "\n",
            "Epoch 773/1000\n",
            "Train Loss MSE: 67057.5304\n",
            "Train Loss RMSE: 258.9547\n",
            "Test Loss MSE: 165331.2500\n",
            "Test RMSE: 406.6095\n",
            "Test R²: 0.8681\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 1/100\n",
            "\n",
            "Epoch 774/1000\n",
            "Train Loss MSE: 71522.3855\n",
            "Train Loss RMSE: 267.4367\n",
            "Test Loss MSE: 95068.8750\n",
            "Test RMSE: 308.3324\n",
            "Test R²: 0.9241\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 2/100\n",
            "\n",
            "Epoch 775/1000\n",
            "Train Loss MSE: 372363.7145\n",
            "Train Loss RMSE: 610.2161\n",
            "Test Loss MSE: 17979.3652\n",
            "Test RMSE: 134.0872\n",
            "Test R²: 0.9857\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 3/100\n",
            "\n",
            "Epoch 776/1000\n",
            "Train Loss MSE: 60281.4306\n",
            "Train Loss RMSE: 245.5228\n",
            "Test Loss MSE: 170911.7656\n",
            "Test RMSE: 413.4148\n",
            "Test R²: 0.8636\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 4/100\n",
            "\n",
            "Epoch 777/1000\n",
            "Train Loss MSE: 31159.7316\n",
            "Train Loss RMSE: 176.5212\n",
            "Test Loss MSE: 142774.9219\n",
            "Test RMSE: 377.8557\n",
            "Test R²: 0.8861\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 5/100\n",
            "\n",
            "Epoch 778/1000\n",
            "Train Loss MSE: 19869.5158\n",
            "Train Loss RMSE: 140.9593\n",
            "Test Loss MSE: 26293.2988\n",
            "Test RMSE: 162.1521\n",
            "Test R²: 0.9790\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 6/100\n",
            "\n",
            "Epoch 779/1000\n",
            "Train Loss MSE: 26494.2533\n",
            "Train Loss RMSE: 162.7706\n",
            "Test Loss MSE: 116309.5625\n",
            "Test RMSE: 341.0419\n",
            "Test R²: 0.9072\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 7/100\n",
            "\n",
            "Epoch 780/1000\n",
            "Train Loss MSE: 45733.3668\n",
            "Train Loss RMSE: 213.8536\n",
            "Test Loss MSE: 107844.6562\n",
            "Test RMSE: 328.3971\n",
            "Test R²: 0.9139\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 8/100\n",
            "\n",
            "Epoch 781/1000\n",
            "Train Loss MSE: 99321.5738\n",
            "Train Loss RMSE: 315.1533\n",
            "Test Loss MSE: 37781.3320\n",
            "Test RMSE: 194.3742\n",
            "Test R²: 0.9698\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 9/100\n",
            "\n",
            "Epoch 782/1000\n",
            "Train Loss MSE: 744382.3566\n",
            "Train Loss RMSE: 862.7760\n",
            "Test Loss MSE: 643343.5000\n",
            "Test RMSE: 802.0870\n",
            "Test R²: 0.4866\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 10/100\n",
            "\n",
            "Epoch 783/1000\n",
            "Train Loss MSE: 45250.2773\n",
            "Train Loss RMSE: 212.7211\n",
            "Test Loss MSE: 9439.0000\n",
            "Test RMSE: 97.1545\n",
            "Test R²: 0.9925\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 11/100\n",
            "\n",
            "Epoch 784/1000\n",
            "Train Loss MSE: 906366.6882\n",
            "Train Loss RMSE: 952.0329\n",
            "Test Loss MSE: 10748.5312\n",
            "Test RMSE: 103.6751\n",
            "Test R²: 0.9914\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 12/100\n",
            "\n",
            "Epoch 785/1000\n",
            "Train Loss MSE: 801257.6996\n",
            "Train Loss RMSE: 895.1300\n",
            "Test Loss MSE: 317197.7500\n",
            "Test RMSE: 563.2031\n",
            "Test R²: 0.7469\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 13/100\n",
            "\n",
            "Epoch 786/1000\n",
            "Train Loss MSE: 35652.6905\n",
            "Train Loss RMSE: 188.8192\n",
            "Test Loss MSE: 123254.7109\n",
            "Test RMSE: 351.0765\n",
            "Test R²: 0.9016\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 14/100\n",
            "\n",
            "Epoch 787/1000\n",
            "Train Loss MSE: 48637.8082\n",
            "Train Loss RMSE: 220.5398\n",
            "Test Loss MSE: 59755.2969\n",
            "Test RMSE: 244.4490\n",
            "Test R²: 0.9523\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 15/100\n",
            "\n",
            "Epoch 788/1000\n",
            "Train Loss MSE: 67300.2525\n",
            "Train Loss RMSE: 259.4229\n",
            "Test Loss MSE: 9316.6318\n",
            "Test RMSE: 96.5227\n",
            "Test R²: 0.9926\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 16/100\n",
            "\n",
            "Epoch 789/1000\n",
            "Train Loss MSE: 43114.3262\n",
            "Train Loss RMSE: 207.6399\n",
            "Test Loss MSE: 124135.8047\n",
            "Test RMSE: 352.3291\n",
            "Test R²: 0.9009\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 17/100\n",
            "\n",
            "Epoch 790/1000\n",
            "Train Loss MSE: 20861.2507\n",
            "Train Loss RMSE: 144.4342\n",
            "Test Loss MSE: 32850.5195\n",
            "Test RMSE: 181.2471\n",
            "Test R²: 0.9738\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 18/100\n",
            "\n",
            "Epoch 791/1000\n",
            "Train Loss MSE: 78917.1419\n",
            "Train Loss RMSE: 280.9219\n",
            "Test Loss MSE: 51428.5391\n",
            "Test RMSE: 226.7786\n",
            "Test R²: 0.9590\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 19/100\n",
            "\n",
            "Epoch 792/1000\n",
            "Train Loss MSE: 28133.9969\n",
            "Train Loss RMSE: 167.7319\n",
            "Test Loss MSE: 42654.0859\n",
            "Test RMSE: 206.5287\n",
            "Test R²: 0.9660\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 20/100\n",
            "\n",
            "Epoch 793/1000\n",
            "Train Loss MSE: 16250.2563\n",
            "Train Loss RMSE: 127.4765\n",
            "Test Loss MSE: 255026.1094\n",
            "Test RMSE: 505.0011\n",
            "Test R²: 0.7965\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 21/100\n",
            "\n",
            "Epoch 794/1000\n",
            "Train Loss MSE: 32432.0445\n",
            "Train Loss RMSE: 180.0890\n",
            "Test Loss MSE: 522591.1875\n",
            "Test RMSE: 722.9047\n",
            "Test R²: 0.5829\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 22/100\n",
            "\n",
            "Epoch 795/1000\n",
            "Train Loss MSE: 36013.6696\n",
            "Train Loss RMSE: 189.7727\n",
            "Test Loss MSE: 709024.8125\n",
            "Test RMSE: 842.0361\n",
            "Test R²: 0.4342\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 23/100\n",
            "\n",
            "Epoch 796/1000\n",
            "Train Loss MSE: 802776.0804\n",
            "Train Loss RMSE: 895.9777\n",
            "Test Loss MSE: 62263.4844\n",
            "Test RMSE: 249.5265\n",
            "Test R²: 0.9503\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 24/100\n",
            "\n",
            "Epoch 797/1000\n",
            "Train Loss MSE: 26785.1855\n",
            "Train Loss RMSE: 163.6618\n",
            "Test Loss MSE: 13240.2246\n",
            "Test RMSE: 115.0662\n",
            "Test R²: 0.9894\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 25/100\n",
            "\n",
            "Epoch 798/1000\n",
            "Train Loss MSE: 53196.4305\n",
            "Train Loss RMSE: 230.6435\n",
            "Test Loss MSE: 8682.9297\n",
            "Test RMSE: 93.1822\n",
            "Test R²: 0.9931\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 26/100\n",
            "\n",
            "Epoch 799/1000\n",
            "Train Loss MSE: 111873.8310\n",
            "Train Loss RMSE: 334.4755\n",
            "Test Loss MSE: 141249.0781\n",
            "Test RMSE: 375.8312\n",
            "Test R²: 0.8873\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 27/100\n",
            "\n",
            "Epoch 800/1000\n",
            "Train Loss MSE: 30728.3629\n",
            "Train Loss RMSE: 175.2951\n",
            "Test Loss MSE: 8206.7324\n",
            "Test RMSE: 90.5910\n",
            "Test R²: 0.9935\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 28/100\n",
            "\n",
            "Epoch 801/1000\n",
            "Train Loss MSE: 1062096.2029\n",
            "Train Loss RMSE: 1030.5805\n",
            "Test Loss MSE: 862702.8750\n",
            "Test RMSE: 928.8180\n",
            "Test R²: 0.3115\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 29/100\n",
            "\n",
            "Epoch 802/1000\n",
            "Train Loss MSE: 56711.0105\n",
            "Train Loss RMSE: 238.1407\n",
            "Test Loss MSE: 152807.2969\n",
            "Test RMSE: 390.9057\n",
            "Test R²: 0.8781\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 30/100\n",
            "\n",
            "Epoch 803/1000\n",
            "Train Loss MSE: 28717.5332\n",
            "Train Loss RMSE: 169.4625\n",
            "Test Loss MSE: 533505.5000\n",
            "Test RMSE: 730.4146\n",
            "Test R²: 0.5742\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 31/100\n",
            "\n",
            "Epoch 804/1000\n",
            "Train Loss MSE: 67989.0185\n",
            "Train Loss RMSE: 260.7470\n",
            "Test Loss MSE: 147312.2188\n",
            "Test RMSE: 383.8127\n",
            "Test R²: 0.8824\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 32/100\n",
            "\n",
            "Epoch 805/1000\n",
            "Train Loss MSE: 24220.6361\n",
            "Train Loss RMSE: 155.6298\n",
            "Test Loss MSE: 83586.3984\n",
            "Test RMSE: 289.1131\n",
            "Test R²: 0.9333\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 33/100\n",
            "\n",
            "Epoch 806/1000\n",
            "Train Loss MSE: 30318.5699\n",
            "Train Loss RMSE: 174.1223\n",
            "Test Loss MSE: 42295.1250\n",
            "Test RMSE: 205.6578\n",
            "Test R²: 0.9662\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 34/100\n",
            "\n",
            "Epoch 807/1000\n",
            "Train Loss MSE: 63127.7061\n",
            "Train Loss RMSE: 251.2523\n",
            "Test Loss MSE: 292235.5312\n",
            "Test RMSE: 540.5881\n",
            "Test R²: 0.7668\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 35/100\n",
            "\n",
            "Epoch 808/1000\n",
            "Train Loss MSE: 1209508.3472\n",
            "Train Loss RMSE: 1099.7765\n",
            "Test Loss MSE: 33970.0938\n",
            "Test RMSE: 184.3098\n",
            "Test R²: 0.9729\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 36/100\n",
            "\n",
            "Epoch 809/1000\n",
            "Train Loss MSE: 56213.4145\n",
            "Train Loss RMSE: 237.0937\n",
            "Test Loss MSE: 42959.8125\n",
            "Test RMSE: 207.2675\n",
            "Test R²: 0.9657\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 37/100\n",
            "\n",
            "Epoch 810/1000\n",
            "Train Loss MSE: 21247.6125\n",
            "Train Loss RMSE: 145.7656\n",
            "Test Loss MSE: 159246.4375\n",
            "Test RMSE: 399.0569\n",
            "Test R²: 0.8729\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 38/100\n",
            "\n",
            "Epoch 811/1000\n",
            "Train Loss MSE: 16945.1456\n",
            "Train Loss RMSE: 130.1735\n",
            "Test Loss MSE: 82921.6719\n",
            "Test RMSE: 287.9612\n",
            "Test R²: 0.9338\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 39/100\n",
            "\n",
            "Epoch 812/1000\n",
            "Train Loss MSE: 68215.6472\n",
            "Train Loss RMSE: 261.1813\n",
            "Test Loss MSE: 98500.4531\n",
            "Test RMSE: 313.8478\n",
            "Test R²: 0.9214\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 40/100\n",
            "\n",
            "Epoch 813/1000\n",
            "Train Loss MSE: 43411.1081\n",
            "Train Loss RMSE: 208.3533\n",
            "Test Loss MSE: 101174.5781\n",
            "Test RMSE: 318.0795\n",
            "Test R²: 0.9193\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 41/100\n",
            "\n",
            "Epoch 814/1000\n",
            "Train Loss MSE: 67292.5368\n",
            "Train Loss RMSE: 259.4081\n",
            "Test Loss MSE: 463289.4062\n",
            "Test RMSE: 680.6537\n",
            "Test R²: 0.6303\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 42/100\n",
            "\n",
            "Epoch 815/1000\n",
            "Train Loss MSE: 318388.0777\n",
            "Train Loss RMSE: 564.2589\n",
            "Test Loss MSE: 53286.3867\n",
            "Test RMSE: 230.8384\n",
            "Test R²: 0.9575\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 43/100\n",
            "\n",
            "Epoch 816/1000\n",
            "Train Loss MSE: 50327.9629\n",
            "Train Loss RMSE: 224.3389\n",
            "Test Loss MSE: 108487.2891\n",
            "Test RMSE: 329.3741\n",
            "Test R²: 0.9134\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 44/100\n",
            "\n",
            "Epoch 817/1000\n",
            "Train Loss MSE: 21788.0768\n",
            "Train Loss RMSE: 147.6078\n",
            "Test Loss MSE: 51178.5391\n",
            "Test RMSE: 226.2267\n",
            "Test R²: 0.9592\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 45/100\n",
            "\n",
            "Epoch 818/1000\n",
            "Train Loss MSE: 24651.5250\n",
            "Train Loss RMSE: 157.0080\n",
            "Test Loss MSE: 55286.0781\n",
            "Test RMSE: 235.1299\n",
            "Test R²: 0.9559\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 46/100\n",
            "\n",
            "Epoch 819/1000\n",
            "Train Loss MSE: 23714.2839\n",
            "Train Loss RMSE: 153.9944\n",
            "Test Loss MSE: 206399.5938\n",
            "Test RMSE: 454.3122\n",
            "Test R²: 0.8353\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 47/100\n",
            "\n",
            "Epoch 820/1000\n",
            "Train Loss MSE: 48584.0686\n",
            "Train Loss RMSE: 220.4179\n",
            "Test Loss MSE: 383890.9688\n",
            "Test RMSE: 619.5894\n",
            "Test R²: 0.6936\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 48/100\n",
            "\n",
            "Epoch 821/1000\n",
            "Train Loss MSE: 24317.2243\n",
            "Train Loss RMSE: 155.9398\n",
            "Test Loss MSE: 55814.0078\n",
            "Test RMSE: 236.2499\n",
            "Test R²: 0.9555\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 49/100\n",
            "\n",
            "Epoch 822/1000\n",
            "Train Loss MSE: 33821.7493\n",
            "Train Loss RMSE: 183.9069\n",
            "Test Loss MSE: 250638.0312\n",
            "Test RMSE: 500.6376\n",
            "Test R²: 0.8000\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 50/100\n",
            "\n",
            "Epoch 823/1000\n",
            "Train Loss MSE: 340670.5838\n",
            "Train Loss RMSE: 583.6699\n",
            "Test Loss MSE: 882614.9375\n",
            "Test RMSE: 939.4759\n",
            "Test R²: 0.2956\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 51/100\n",
            "\n",
            "Epoch 824/1000\n",
            "Train Loss MSE: 69132.7892\n",
            "Train Loss RMSE: 262.9311\n",
            "Test Loss MSE: 33725.8125\n",
            "Test RMSE: 183.6459\n",
            "Test R²: 0.9731\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 52/100\n",
            "\n",
            "Epoch 825/1000\n",
            "Train Loss MSE: 69938.5498\n",
            "Train Loss RMSE: 264.4590\n",
            "Test Loss MSE: 190446.2031\n",
            "Test RMSE: 436.4014\n",
            "Test R²: 0.8480\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 53/100\n",
            "\n",
            "Epoch 826/1000\n",
            "Train Loss MSE: 56172.5273\n",
            "Train Loss RMSE: 237.0074\n",
            "Test Loss MSE: 23532.9805\n",
            "Test RMSE: 153.4046\n",
            "Test R²: 0.9812\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 54/100\n",
            "\n",
            "Epoch 827/1000\n",
            "Train Loss MSE: 166102.9602\n",
            "Train Loss RMSE: 407.5573\n",
            "Test Loss MSE: 534892.2500\n",
            "Test RMSE: 731.3633\n",
            "Test R²: 0.5731\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 55/100\n",
            "\n",
            "Epoch 828/1000\n",
            "Train Loss MSE: 177241.3759\n",
            "Train Loss RMSE: 421.0004\n",
            "Test Loss MSE: 78725.6250\n",
            "Test RMSE: 280.5809\n",
            "Test R²: 0.9372\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 56/100\n",
            "\n",
            "Epoch 829/1000\n",
            "Train Loss MSE: 26720.6775\n",
            "Train Loss RMSE: 163.4646\n",
            "Test Loss MSE: 8444.6445\n",
            "Test RMSE: 91.8947\n",
            "Test R²: 0.9933\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 57/100\n",
            "\n",
            "Epoch 830/1000\n",
            "Train Loss MSE: 162359.0124\n",
            "Train Loss RMSE: 402.9380\n",
            "Test Loss MSE: 47416.3438\n",
            "Test RMSE: 217.7529\n",
            "Test R²: 0.9622\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 58/100\n",
            "\n",
            "Epoch 831/1000\n",
            "Train Loss MSE: 16713.2383\n",
            "Train Loss RMSE: 129.2797\n",
            "Test Loss MSE: 71036.0469\n",
            "Test RMSE: 266.5259\n",
            "Test R²: 0.9433\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 59/100\n",
            "\n",
            "Epoch 832/1000\n",
            "Train Loss MSE: 25062.5841\n",
            "Train Loss RMSE: 158.3117\n",
            "Test Loss MSE: 50554.4688\n",
            "Test RMSE: 224.8432\n",
            "Test R²: 0.9597\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 60/100\n",
            "\n",
            "Epoch 833/1000\n",
            "Train Loss MSE: 39842.8260\n",
            "Train Loss RMSE: 199.6067\n",
            "Test Loss MSE: 89908.3594\n",
            "Test RMSE: 299.8472\n",
            "Test R²: 0.9282\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 61/100\n",
            "\n",
            "Epoch 834/1000\n",
            "Train Loss MSE: 33263.8442\n",
            "Train Loss RMSE: 182.3838\n",
            "Test Loss MSE: 909855.5625\n",
            "Test RMSE: 953.8635\n",
            "Test R²: 0.2739\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 62/100\n",
            "\n",
            "Epoch 835/1000\n",
            "Train Loss MSE: 918018.8801\n",
            "Train Loss RMSE: 958.1330\n",
            "Test Loss MSE: 256985.2344\n",
            "Test RMSE: 506.9371\n",
            "Test R²: 0.7949\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 63/100\n",
            "\n",
            "Epoch 836/1000\n",
            "Train Loss MSE: 130402.0268\n",
            "Train Loss RMSE: 361.1122\n",
            "Test Loss MSE: 213592.3438\n",
            "Test RMSE: 462.1605\n",
            "Test R²: 0.8295\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 64/100\n",
            "\n",
            "Epoch 837/1000\n",
            "Train Loss MSE: 28124.9400\n",
            "Train Loss RMSE: 167.7049\n",
            "Test Loss MSE: 32431.5527\n",
            "Test RMSE: 180.0876\n",
            "Test R²: 0.9741\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 65/100\n",
            "\n",
            "Epoch 838/1000\n",
            "Train Loss MSE: 48130.5894\n",
            "Train Loss RMSE: 219.3868\n",
            "Test Loss MSE: 341333.6875\n",
            "Test RMSE: 584.2377\n",
            "Test R²: 0.7276\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 66/100\n",
            "\n",
            "Epoch 839/1000\n",
            "Train Loss MSE: 51857.1661\n",
            "Train Loss RMSE: 227.7217\n",
            "Test Loss MSE: 178373.7500\n",
            "Test RMSE: 422.3432\n",
            "Test R²: 0.8576\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 67/100\n",
            "\n",
            "Epoch 840/1000\n",
            "Train Loss MSE: 26764.5302\n",
            "Train Loss RMSE: 163.5987\n",
            "Test Loss MSE: 73724.8594\n",
            "Test RMSE: 271.5232\n",
            "Test R²: 0.9412\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 68/100\n",
            "\n",
            "Epoch 841/1000\n",
            "Train Loss MSE: 53385.0246\n",
            "Train Loss RMSE: 231.0520\n",
            "Test Loss MSE: 56649.8203\n",
            "Test RMSE: 238.0122\n",
            "Test R²: 0.9548\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 69/100\n",
            "\n",
            "Epoch 842/1000\n",
            "Train Loss MSE: 21728.7955\n",
            "Train Loss RMSE: 147.4069\n",
            "Test Loss MSE: 89625.5781\n",
            "Test RMSE: 299.3753\n",
            "Test R²: 0.9285\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 70/100\n",
            "\n",
            "Epoch 843/1000\n",
            "Train Loss MSE: 139272.5253\n",
            "Train Loss RMSE: 373.1923\n",
            "Test Loss MSE: 59171.6836\n",
            "Test RMSE: 243.2523\n",
            "Test R²: 0.9528\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 71/100\n",
            "\n",
            "Epoch 844/1000\n",
            "Train Loss MSE: 299444.4488\n",
            "Train Loss RMSE: 547.2152\n",
            "Test Loss MSE: 514658.3438\n",
            "Test RMSE: 717.3969\n",
            "Test R²: 0.5893\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 72/100\n",
            "\n",
            "Epoch 845/1000\n",
            "Train Loss MSE: 51014.4825\n",
            "Train Loss RMSE: 225.8639\n",
            "Test Loss MSE: 592986.6250\n",
            "Test RMSE: 770.0562\n",
            "Test R²: 0.5268\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 73/100\n",
            "\n",
            "Epoch 846/1000\n",
            "Train Loss MSE: 42589.9322\n",
            "Train Loss RMSE: 206.3733\n",
            "Test Loss MSE: 884741.7500\n",
            "Test RMSE: 940.6071\n",
            "Test R²: 0.2939\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 74/100\n",
            "\n",
            "Epoch 847/1000\n",
            "Train Loss MSE: 69105.5389\n",
            "Train Loss RMSE: 262.8793\n",
            "Test Loss MSE: 31176.2402\n",
            "Test RMSE: 176.5679\n",
            "Test R²: 0.9751\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 75/100\n",
            "\n",
            "Epoch 848/1000\n",
            "Train Loss MSE: 33583.5940\n",
            "Train Loss RMSE: 183.2583\n",
            "Test Loss MSE: 199838.0469\n",
            "Test RMSE: 447.0325\n",
            "Test R²: 0.8405\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 76/100\n",
            "\n",
            "Epoch 849/1000\n",
            "Train Loss MSE: 42261.9075\n",
            "Train Loss RMSE: 205.5770\n",
            "Test Loss MSE: 53467.1094\n",
            "Test RMSE: 231.2296\n",
            "Test R²: 0.9573\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 77/100\n",
            "\n",
            "Epoch 850/1000\n",
            "Train Loss MSE: 13014.8002\n",
            "Train Loss RMSE: 114.0824\n",
            "Test Loss MSE: 290300.5625\n",
            "Test RMSE: 538.7955\n",
            "Test R²: 0.7683\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 78/100\n",
            "\n",
            "Epoch 851/1000\n",
            "Train Loss MSE: 36111.2009\n",
            "Train Loss RMSE: 190.0295\n",
            "Test Loss MSE: 37471.4922\n",
            "Test RMSE: 193.5755\n",
            "Test R²: 0.9701\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 79/100\n",
            "\n",
            "Epoch 852/1000\n",
            "Train Loss MSE: 17128.4369\n",
            "Train Loss RMSE: 130.8757\n",
            "Test Loss MSE: 408719.6875\n",
            "Test RMSE: 639.3119\n",
            "Test R²: 0.6738\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 80/100\n",
            "\n",
            "Epoch 853/1000\n",
            "Train Loss MSE: 51222.0954\n",
            "Train Loss RMSE: 226.3230\n",
            "Test Loss MSE: 26380.9121\n",
            "Test RMSE: 162.4220\n",
            "Test R²: 0.9789\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 81/100\n",
            "\n",
            "Epoch 854/1000\n",
            "Train Loss MSE: 221859.7966\n",
            "Train Loss RMSE: 471.0200\n",
            "Test Loss MSE: 115805.1484\n",
            "Test RMSE: 340.3016\n",
            "Test R²: 0.9076\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 82/100\n",
            "\n",
            "Epoch 855/1000\n",
            "Train Loss MSE: 39238.8863\n",
            "Train Loss RMSE: 198.0881\n",
            "Test Loss MSE: 202210.8750\n",
            "Test RMSE: 449.6786\n",
            "Test R²: 0.8386\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 83/100\n",
            "\n",
            "Epoch 856/1000\n",
            "Train Loss MSE: 25248.8430\n",
            "Train Loss RMSE: 158.8988\n",
            "Test Loss MSE: 47657.5352\n",
            "Test RMSE: 218.3061\n",
            "Test R²: 0.9620\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 84/100\n",
            "\n",
            "Epoch 857/1000\n",
            "Train Loss MSE: 48157.0177\n",
            "Train Loss RMSE: 219.4471\n",
            "Test Loss MSE: 40964.0586\n",
            "Test RMSE: 202.3958\n",
            "Test R²: 0.9673\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 85/100\n",
            "\n",
            "Epoch 858/1000\n",
            "Train Loss MSE: 94094.0596\n",
            "Train Loss RMSE: 306.7476\n",
            "Test Loss MSE: 556251.2500\n",
            "Test RMSE: 745.8225\n",
            "Test R²: 0.5561\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 86/100\n",
            "\n",
            "Epoch 859/1000\n",
            "Train Loss MSE: 52733.9413\n",
            "Train Loss RMSE: 229.6387\n",
            "Test Loss MSE: 449446.4062\n",
            "Test RMSE: 670.4076\n",
            "Test R²: 0.6413\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 87/100\n",
            "\n",
            "Epoch 860/1000\n",
            "Train Loss MSE: 54155.0154\n",
            "Train Loss RMSE: 232.7123\n",
            "Test Loss MSE: 261378.8594\n",
            "Test RMSE: 511.2522\n",
            "Test R²: 0.7914\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 88/100\n",
            "\n",
            "Epoch 861/1000\n",
            "Train Loss MSE: 45265.3529\n",
            "Train Loss RMSE: 212.7566\n",
            "Test Loss MSE: 166217.7500\n",
            "Test RMSE: 407.6981\n",
            "Test R²: 0.8673\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 89/100\n",
            "\n",
            "Epoch 862/1000\n",
            "Train Loss MSE: 255876.0127\n",
            "Train Loss RMSE: 505.8419\n",
            "Test Loss MSE: 324461.4375\n",
            "Test RMSE: 569.6152\n",
            "Test R²: 0.7411\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 90/100\n",
            "\n",
            "Epoch 863/1000\n",
            "Train Loss MSE: 22845.3702\n",
            "Train Loss RMSE: 151.1468\n",
            "Test Loss MSE: 87916.8438\n",
            "Test RMSE: 296.5077\n",
            "Test R²: 0.9298\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 91/100\n",
            "\n",
            "Epoch 864/1000\n",
            "Train Loss MSE: 915416.1692\n",
            "Train Loss RMSE: 956.7738\n",
            "Test Loss MSE: 25529.4941\n",
            "Test RMSE: 159.7795\n",
            "Test R²: 0.9796\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 92/100\n",
            "\n",
            "Epoch 865/1000\n",
            "Train Loss MSE: 34561.2394\n",
            "Train Loss RMSE: 185.9065\n",
            "Test Loss MSE: 176042.0469\n",
            "Test RMSE: 419.5736\n",
            "Test R²: 0.8595\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 93/100\n",
            "\n",
            "Epoch 866/1000\n",
            "Train Loss MSE: 32106.3445\n",
            "Train Loss RMSE: 179.1824\n",
            "Test Loss MSE: 28988.7344\n",
            "Test RMSE: 170.2608\n",
            "Test R²: 0.9769\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 94/100\n",
            "\n",
            "Epoch 867/1000\n",
            "Train Loss MSE: 94571.3166\n",
            "Train Loss RMSE: 307.5245\n",
            "Test Loss MSE: 88090.6094\n",
            "Test RMSE: 296.8006\n",
            "Test R²: 0.9297\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 95/100\n",
            "\n",
            "Epoch 868/1000\n",
            "Train Loss MSE: 23700.4572\n",
            "Train Loss RMSE: 153.9495\n",
            "Test Loss MSE: 70549.7188\n",
            "Test RMSE: 265.6120\n",
            "Test R²: 0.9437\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 96/100\n",
            "\n",
            "Epoch 869/1000\n",
            "Train Loss MSE: 668795.1968\n",
            "Train Loss RMSE: 817.7990\n",
            "Test Loss MSE: 33821.4102\n",
            "Test RMSE: 183.9060\n",
            "Test R²: 0.9730\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 97/100\n",
            "\n",
            "Epoch 870/1000\n",
            "Train Loss MSE: 226869.2696\n",
            "Train Loss RMSE: 476.3080\n",
            "Test Loss MSE: 515587.2188\n",
            "Test RMSE: 718.0440\n",
            "Test R²: 0.5885\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 98/100\n",
            "\n",
            "Epoch 871/1000\n",
            "Train Loss MSE: 28335.8564\n",
            "Train Loss RMSE: 168.3326\n",
            "Test Loss MSE: 378065.9062\n",
            "Test RMSE: 614.8706\n",
            "Test R²: 0.6983\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 99/100\n",
            "\n",
            "Epoch 872/1000\n",
            "Train Loss MSE: 13777.6119\n",
            "Train Loss RMSE: 117.3781\n",
            "Test Loss MSE: 260305.3438\n",
            "Test RMSE: 510.2013\n",
            "Test R²: 0.7923\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 100/100\n",
            "\n",
            "Epoch 873/1000\n",
            "Train Loss MSE: 863310.9872\n",
            "Train Loss RMSE: 929.1453\n",
            "Test Loss MSE: 59514.0781\n",
            "Test RMSE: 243.9551\n",
            "Test R²: 0.9525\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 101/100\n",
            "\n",
            "Epoch 874/1000\n",
            "Train Loss MSE: 43418.6770\n",
            "Train Loss RMSE: 208.3715\n",
            "Test Loss MSE: 185195.4531\n",
            "Test RMSE: 430.3434\n",
            "Test R²: 0.8522\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 102/100\n",
            "\n",
            "Epoch 875/1000\n",
            "Train Loss MSE: 36263.0076\n",
            "Train Loss RMSE: 190.4285\n",
            "Test Loss MSE: 56980.9961\n",
            "Test RMSE: 238.7069\n",
            "Test R²: 0.9545\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 103/100\n",
            "\n",
            "Epoch 876/1000\n",
            "Train Loss MSE: 881300.7093\n",
            "Train Loss RMSE: 938.7762\n",
            "Test Loss MSE: 67686.0078\n",
            "Test RMSE: 260.1653\n",
            "Test R²: 0.9460\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 104/100\n",
            "\n",
            "Epoch 877/1000\n",
            "Train Loss MSE: 68892.0260\n",
            "Train Loss RMSE: 262.4729\n",
            "Test Loss MSE: 118035.4844\n",
            "Test RMSE: 343.5629\n",
            "Test R²: 0.9058\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 105/100\n",
            "\n",
            "Epoch 878/1000\n",
            "Train Loss MSE: 64742.8620\n",
            "Train Loss RMSE: 254.4462\n",
            "Test Loss MSE: 17472.5098\n",
            "Test RMSE: 132.1836\n",
            "Test R²: 0.9861\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 106/100\n",
            "\n",
            "Epoch 879/1000\n",
            "Train Loss MSE: 27981.8234\n",
            "Train Loss RMSE: 167.2777\n",
            "Test Loss MSE: 140257.4844\n",
            "Test RMSE: 374.5097\n",
            "Test R²: 0.8881\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 107/100\n",
            "\n",
            "Epoch 880/1000\n",
            "Train Loss MSE: 42297.1987\n",
            "Train Loss RMSE: 205.6628\n",
            "Test Loss MSE: 215381.6875\n",
            "Test RMSE: 464.0923\n",
            "Test R²: 0.8281\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 108/100\n",
            "\n",
            "Epoch 881/1000\n",
            "Train Loss MSE: 139904.3646\n",
            "Train Loss RMSE: 374.0379\n",
            "Test Loss MSE: 6822.9224\n",
            "Test RMSE: 82.6010\n",
            "Test R²: 0.9946\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 109/100\n",
            "\n",
            "Epoch 882/1000\n",
            "Train Loss MSE: 21131.5251\n",
            "Train Loss RMSE: 145.3669\n",
            "Test Loss MSE: 245774.7031\n",
            "Test RMSE: 495.7567\n",
            "Test R²: 0.8039\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 110/100\n",
            "\n",
            "Epoch 883/1000\n",
            "Train Loss MSE: 46704.3531\n",
            "Train Loss RMSE: 216.1119\n",
            "Test Loss MSE: 37808.5039\n",
            "Test RMSE: 194.4441\n",
            "Test R²: 0.9698\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 111/100\n",
            "\n",
            "Epoch 884/1000\n",
            "Train Loss MSE: 91800.5324\n",
            "Train Loss RMSE: 302.9860\n",
            "Test Loss MSE: 11211.6191\n",
            "Test RMSE: 105.8849\n",
            "Test R²: 0.9911\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 112/100\n",
            "\n",
            "Epoch 885/1000\n",
            "Train Loss MSE: 65844.2461\n",
            "Train Loss RMSE: 256.6013\n",
            "Test Loss MSE: 201053.8281\n",
            "Test RMSE: 448.3903\n",
            "Test R²: 0.8395\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 113/100\n",
            "\n",
            "Epoch 886/1000\n",
            "Train Loss MSE: 24180.2627\n",
            "Train Loss RMSE: 155.5000\n",
            "Test Loss MSE: 55817.1367\n",
            "Test RMSE: 236.2565\n",
            "Test R²: 0.9555\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 114/100\n",
            "\n",
            "Epoch 887/1000\n",
            "Train Loss MSE: 69675.2185\n",
            "Train Loss RMSE: 263.9606\n",
            "Test Loss MSE: 13841.2002\n",
            "Test RMSE: 117.6486\n",
            "Test R²: 0.9890\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 115/100\n",
            "\n",
            "Epoch 888/1000\n",
            "Train Loss MSE: 414289.6937\n",
            "Train Loss RMSE: 643.6534\n",
            "Test Loss MSE: 436458.3125\n",
            "Test RMSE: 660.6499\n",
            "Test R²: 0.6517\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 116/100\n",
            "\n",
            "Epoch 889/1000\n",
            "Train Loss MSE: 19041.4841\n",
            "Train Loss RMSE: 137.9909\n",
            "Test Loss MSE: 76376.6562\n",
            "Test RMSE: 276.3633\n",
            "Test R²: 0.9390\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 117/100\n",
            "\n",
            "Epoch 890/1000\n",
            "Train Loss MSE: 26939.5957\n",
            "Train Loss RMSE: 164.1329\n",
            "Test Loss MSE: 234408.2656\n",
            "Test RMSE: 484.1573\n",
            "Test R²: 0.8129\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 118/100\n",
            "\n",
            "Epoch 891/1000\n",
            "Train Loss MSE: 444877.0660\n",
            "Train Loss RMSE: 666.9911\n",
            "Test Loss MSE: 250962.3125\n",
            "Test RMSE: 500.9614\n",
            "Test R²: 0.7997\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 119/100\n",
            "\n",
            "Epoch 892/1000\n",
            "Train Loss MSE: 54133.4183\n",
            "Train Loss RMSE: 232.6659\n",
            "Test Loss MSE: 93628.8047\n",
            "Test RMSE: 305.9882\n",
            "Test R²: 0.9253\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 120/100\n",
            "\n",
            "Epoch 893/1000\n",
            "Train Loss MSE: 25336.7218\n",
            "Train Loss RMSE: 159.1751\n",
            "Test Loss MSE: 34283.1523\n",
            "Test RMSE: 185.1571\n",
            "Test R²: 0.9726\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 121/100\n",
            "\n",
            "Epoch 894/1000\n",
            "Train Loss MSE: 117325.8647\n",
            "Train Loss RMSE: 342.5286\n",
            "Test Loss MSE: 15547.2285\n",
            "Test RMSE: 124.6885\n",
            "Test R²: 0.9876\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 122/100\n",
            "\n",
            "Epoch 895/1000\n",
            "Train Loss MSE: 22693.5263\n",
            "Train Loss RMSE: 150.6437\n",
            "Test Loss MSE: 67367.9141\n",
            "Test RMSE: 259.5533\n",
            "Test R²: 0.9462\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 123/100\n",
            "\n",
            "Epoch 896/1000\n",
            "Train Loss MSE: 25844.2642\n",
            "Train Loss RMSE: 160.7615\n",
            "Test Loss MSE: 554812.7500\n",
            "Test RMSE: 744.8575\n",
            "Test R²: 0.5572\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 124/100\n",
            "\n",
            "Epoch 897/1000\n",
            "Train Loss MSE: 23542.5336\n",
            "Train Loss RMSE: 153.4358\n",
            "Test Loss MSE: 376091.4062\n",
            "Test RMSE: 613.2629\n",
            "Test R²: 0.6999\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 125/100\n",
            "\n",
            "Epoch 898/1000\n",
            "Train Loss MSE: 66390.0361\n",
            "Train Loss RMSE: 257.6626\n",
            "Test Loss MSE: 89178.3672\n",
            "Test RMSE: 298.6275\n",
            "Test R²: 0.9288\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 126/100\n",
            "\n",
            "Epoch 899/1000\n",
            "Train Loss MSE: 115682.0116\n",
            "Train Loss RMSE: 340.1206\n",
            "Test Loss MSE: 737016.9375\n",
            "Test RMSE: 858.4969\n",
            "Test R²: 0.4118\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 127/100\n",
            "\n",
            "Epoch 900/1000\n",
            "Train Loss MSE: 38227.0370\n",
            "Train Loss RMSE: 195.5174\n",
            "Test Loss MSE: 681721.5000\n",
            "Test RMSE: 825.6643\n",
            "Test R²: 0.4559\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 128/100\n",
            "\n",
            "Epoch 901/1000\n",
            "Train Loss MSE: 375927.6463\n",
            "Train Loss RMSE: 613.1294\n",
            "Test Loss MSE: 890465.6875\n",
            "Test RMSE: 943.6449\n",
            "Test R²: 0.2894\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 129/100\n",
            "\n",
            "Epoch 902/1000\n",
            "Train Loss MSE: 20981.7494\n",
            "Train Loss RMSE: 144.8508\n",
            "Test Loss MSE: 64217.1406\n",
            "Test RMSE: 253.4110\n",
            "Test R²: 0.9488\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 130/100\n",
            "\n",
            "Epoch 903/1000\n",
            "Train Loss MSE: 40103.9753\n",
            "Train Loss RMSE: 200.2598\n",
            "Test Loss MSE: 5987.9961\n",
            "Test RMSE: 77.3821\n",
            "Test R²: 0.9952\n",
            "Learning Rate: 1.00e-02\n",
            "✓ New best model! Improvement: 773.6245\n",
            "\n",
            "Epoch 904/1000\n",
            "Train Loss MSE: 34490.9133\n",
            "Train Loss RMSE: 185.7173\n",
            "Test Loss MSE: 9238.1768\n",
            "Test RMSE: 96.1154\n",
            "Test R²: 0.9926\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 1/100\n",
            "\n",
            "Epoch 905/1000\n",
            "Train Loss MSE: 128263.3741\n",
            "Train Loss RMSE: 358.1388\n",
            "Test Loss MSE: 5679.8413\n",
            "Test RMSE: 75.3647\n",
            "Test R²: 0.9955\n",
            "Learning Rate: 1.00e-02\n",
            "✓ New best model! Improvement: 308.1548\n",
            "\n",
            "Epoch 906/1000\n",
            "Train Loss MSE: 24472.5668\n",
            "Train Loss RMSE: 156.4371\n",
            "Test Loss MSE: 75521.2109\n",
            "Test RMSE: 274.8112\n",
            "Test R²: 0.9397\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 1/100\n",
            "\n",
            "Epoch 907/1000\n",
            "Train Loss MSE: 15706.3895\n",
            "Train Loss RMSE: 125.3251\n",
            "Test Loss MSE: 277462.1250\n",
            "Test RMSE: 526.7467\n",
            "Test R²: 0.7786\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 2/100\n",
            "\n",
            "Epoch 908/1000\n",
            "Train Loss MSE: 39260.9707\n",
            "Train Loss RMSE: 198.1438\n",
            "Test Loss MSE: 446351.6250\n",
            "Test RMSE: 668.0955\n",
            "Test R²: 0.6438\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 3/100\n",
            "\n",
            "Epoch 909/1000\n",
            "Train Loss MSE: 179195.7139\n",
            "Train Loss RMSE: 423.3151\n",
            "Test Loss MSE: 544459.4375\n",
            "Test RMSE: 737.8749\n",
            "Test R²: 0.5655\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 4/100\n",
            "\n",
            "Epoch 910/1000\n",
            "Train Loss MSE: 74514.1644\n",
            "Train Loss RMSE: 272.9728\n",
            "Test Loss MSE: 11097.2158\n",
            "Test RMSE: 105.3433\n",
            "Test R²: 0.9911\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 5/100\n",
            "\n",
            "Epoch 911/1000\n",
            "Train Loss MSE: 54296.5504\n",
            "Train Loss RMSE: 233.0162\n",
            "Test Loss MSE: 10639.7900\n",
            "Test RMSE: 103.1494\n",
            "Test R²: 0.9915\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 6/100\n",
            "\n",
            "Epoch 912/1000\n",
            "Train Loss MSE: 24519.7156\n",
            "Train Loss RMSE: 156.5877\n",
            "Test Loss MSE: 852282.6875\n",
            "Test RMSE: 923.1916\n",
            "Test R²: 0.3198\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 7/100\n",
            "\n",
            "Epoch 913/1000\n",
            "Train Loss MSE: 13523.4846\n",
            "Train Loss RMSE: 116.2905\n",
            "Test Loss MSE: 258554.2500\n",
            "Test RMSE: 508.4823\n",
            "Test R²: 0.7937\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 8/100\n",
            "\n",
            "Epoch 914/1000\n",
            "Train Loss MSE: 498969.9425\n",
            "Train Loss RMSE: 706.3780\n",
            "Test Loss MSE: 120932.1562\n",
            "Test RMSE: 347.7530\n",
            "Test R²: 0.9035\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 9/100\n",
            "\n",
            "Epoch 915/1000\n",
            "Train Loss MSE: 51594.4655\n",
            "Train Loss RMSE: 227.1442\n",
            "Test Loss MSE: 34172.7969\n",
            "Test RMSE: 184.8589\n",
            "Test R²: 0.9727\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 10/100\n",
            "\n",
            "Epoch 916/1000\n",
            "Train Loss MSE: 138694.6856\n",
            "Train Loss RMSE: 372.4174\n",
            "Test Loss MSE: 15315.0244\n",
            "Test RMSE: 123.7539\n",
            "Test R²: 0.9878\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 11/100\n",
            "\n",
            "Epoch 917/1000\n",
            "Train Loss MSE: 216643.2069\n",
            "Train Loss RMSE: 465.4495\n",
            "Test Loss MSE: 717945.6250\n",
            "Test RMSE: 847.3167\n",
            "Test R²: 0.4270\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 12/100\n",
            "\n",
            "Epoch 918/1000\n",
            "Train Loss MSE: 66641.2910\n",
            "Train Loss RMSE: 258.1497\n",
            "Test Loss MSE: 7785.7344\n",
            "Test RMSE: 88.2368\n",
            "Test R²: 0.9938\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 13/100\n",
            "\n",
            "Epoch 919/1000\n",
            "Train Loss MSE: 46435.4573\n",
            "Train Loss RMSE: 215.4889\n",
            "Test Loss MSE: 26364.5938\n",
            "Test RMSE: 162.3718\n",
            "Test R²: 0.9790\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 14/100\n",
            "\n",
            "Epoch 920/1000\n",
            "Train Loss MSE: 37703.4308\n",
            "Train Loss RMSE: 194.1737\n",
            "Test Loss MSE: 16726.7969\n",
            "Test RMSE: 129.3321\n",
            "Test R²: 0.9867\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 15/100\n",
            "\n",
            "Epoch 921/1000\n",
            "Train Loss MSE: 26878.4168\n",
            "Train Loss RMSE: 163.9464\n",
            "Test Loss MSE: 62319.8086\n",
            "Test RMSE: 249.6394\n",
            "Test R²: 0.9503\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 16/100\n",
            "\n",
            "Epoch 922/1000\n",
            "Train Loss MSE: 76974.5485\n",
            "Train Loss RMSE: 277.4429\n",
            "Test Loss MSE: 229833.9062\n",
            "Test RMSE: 479.4100\n",
            "Test R²: 0.8166\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 17/100\n",
            "\n",
            "Epoch 923/1000\n",
            "Train Loss MSE: 22934.7506\n",
            "Train Loss RMSE: 151.4422\n",
            "Test Loss MSE: 42596.3867\n",
            "Test RMSE: 206.3889\n",
            "Test R²: 0.9660\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 18/100\n",
            "\n",
            "Epoch 924/1000\n",
            "Train Loss MSE: 169301.2478\n",
            "Train Loss RMSE: 411.4623\n",
            "Test Loss MSE: 317670.3438\n",
            "Test RMSE: 563.6225\n",
            "Test R²: 0.7465\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 19/100\n",
            "\n",
            "Epoch 925/1000\n",
            "Train Loss MSE: 139897.8608\n",
            "Train Loss RMSE: 374.0292\n",
            "Test Loss MSE: 198726.0625\n",
            "Test RMSE: 445.7870\n",
            "Test R²: 0.8414\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 20/100\n",
            "\n",
            "Epoch 926/1000\n",
            "Train Loss MSE: 23741.4011\n",
            "Train Loss RMSE: 154.0824\n",
            "Test Loss MSE: 12087.5547\n",
            "Test RMSE: 109.9434\n",
            "Test R²: 0.9904\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 21/100\n",
            "\n",
            "Epoch 927/1000\n",
            "Train Loss MSE: 60851.6152\n",
            "Train Loss RMSE: 246.6812\n",
            "Test Loss MSE: 15151.7998\n",
            "Test RMSE: 123.0926\n",
            "Test R²: 0.9879\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 22/100\n",
            "\n",
            "Epoch 928/1000\n",
            "Train Loss MSE: 56188.0928\n",
            "Train Loss RMSE: 237.0403\n",
            "Test Loss MSE: 14689.2031\n",
            "Test RMSE: 121.1990\n",
            "Test R²: 0.9883\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 23/100\n",
            "\n",
            "Epoch 929/1000\n",
            "Train Loss MSE: 52679.4711\n",
            "Train Loss RMSE: 229.5201\n",
            "Test Loss MSE: 145197.8281\n",
            "Test RMSE: 381.0483\n",
            "Test R²: 0.8841\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 24/100\n",
            "\n",
            "Epoch 930/1000\n",
            "Train Loss MSE: 22501.7139\n",
            "Train Loss RMSE: 150.0057\n",
            "Test Loss MSE: 501685.7500\n",
            "Test RMSE: 708.2978\n",
            "Test R²: 0.5996\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 25/100\n",
            "\n",
            "Epoch 931/1000\n",
            "Train Loss MSE: 809551.8623\n",
            "Train Loss RMSE: 899.7510\n",
            "Test Loss MSE: 6617.8574\n",
            "Test RMSE: 81.3502\n",
            "Test R²: 0.9947\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 26/100\n",
            "\n",
            "Epoch 932/1000\n",
            "Train Loss MSE: 182128.5228\n",
            "Train Loss RMSE: 426.7652\n",
            "Test Loss MSE: 150557.7031\n",
            "Test RMSE: 388.0177\n",
            "Test R²: 0.8798\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 27/100\n",
            "\n",
            "Epoch 933/1000\n",
            "Train Loss MSE: 246079.4383\n",
            "Train Loss RMSE: 496.0639\n",
            "Test Loss MSE: 38286.5469\n",
            "Test RMSE: 195.6695\n",
            "Test R²: 0.9694\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 28/100\n",
            "\n",
            "Epoch 934/1000\n",
            "Train Loss MSE: 714108.1857\n",
            "Train Loss RMSE: 845.0492\n",
            "Test Loss MSE: 81320.8750\n",
            "Test RMSE: 285.1682\n",
            "Test R²: 0.9351\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 29/100\n",
            "\n",
            "Epoch 935/1000\n",
            "Train Loss MSE: 19916.2765\n",
            "Train Loss RMSE: 141.1250\n",
            "Test Loss MSE: 23278.9336\n",
            "Test RMSE: 152.5744\n",
            "Test R²: 0.9814\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 30/100\n",
            "\n",
            "Epoch 936/1000\n",
            "Train Loss MSE: 39322.3151\n",
            "Train Loss RMSE: 198.2986\n",
            "Test Loss MSE: 11875.8691\n",
            "Test RMSE: 108.9765\n",
            "Test R²: 0.9905\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 31/100\n",
            "\n",
            "Epoch 937/1000\n",
            "Train Loss MSE: 901968.8118\n",
            "Train Loss RMSE: 949.7204\n",
            "Test Loss MSE: 14196.5225\n",
            "Test RMSE: 119.1492\n",
            "Test R²: 0.9887\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 32/100\n",
            "\n",
            "Epoch 938/1000\n",
            "Train Loss MSE: 35251.2625\n",
            "Train Loss RMSE: 187.7532\n",
            "Test Loss MSE: 65888.3594\n",
            "Test RMSE: 256.6873\n",
            "Test R²: 0.9474\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 33/100\n",
            "\n",
            "Epoch 939/1000\n",
            "Train Loss MSE: 56169.3642\n",
            "Train Loss RMSE: 237.0008\n",
            "Test Loss MSE: 318917.6875\n",
            "Test RMSE: 564.7280\n",
            "Test R²: 0.7455\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 34/100\n",
            "\n",
            "Epoch 940/1000\n",
            "Train Loss MSE: 984618.5252\n",
            "Train Loss RMSE: 992.2795\n",
            "Test Loss MSE: 16606.7969\n",
            "Test RMSE: 128.8674\n",
            "Test R²: 0.9867\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 35/100\n",
            "\n",
            "Epoch 941/1000\n",
            "Train Loss MSE: 756266.1270\n",
            "Train Loss RMSE: 869.6356\n",
            "Test Loss MSE: 28347.5898\n",
            "Test RMSE: 168.3674\n",
            "Test R²: 0.9774\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 36/100\n",
            "\n",
            "Epoch 942/1000\n",
            "Train Loss MSE: 39805.2779\n",
            "Train Loss RMSE: 199.5126\n",
            "Test Loss MSE: 24789.0977\n",
            "Test RMSE: 157.4455\n",
            "Test R²: 0.9802\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 37/100\n",
            "\n",
            "Epoch 943/1000\n",
            "Train Loss MSE: 26736.1927\n",
            "Train Loss RMSE: 163.5121\n",
            "Test Loss MSE: 58801.8594\n",
            "Test RMSE: 242.4909\n",
            "Test R²: 0.9531\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 38/100\n",
            "\n",
            "Epoch 944/1000\n",
            "Train Loss MSE: 20148.1681\n",
            "Train Loss RMSE: 141.9442\n",
            "Test Loss MSE: 71787.0000\n",
            "Test RMSE: 267.9310\n",
            "Test R²: 0.9427\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 39/100\n",
            "\n",
            "Epoch 945/1000\n",
            "Train Loss MSE: 944457.2722\n",
            "Train Loss RMSE: 971.8319\n",
            "Test Loss MSE: 12306.7666\n",
            "Test RMSE: 110.9359\n",
            "Test R²: 0.9902\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 40/100\n",
            "\n",
            "Epoch 946/1000\n",
            "Train Loss MSE: 303824.2486\n",
            "Train Loss RMSE: 551.2025\n",
            "Test Loss MSE: 9863.9326\n",
            "Test RMSE: 99.3173\n",
            "Test R²: 0.9921\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 41/100\n",
            "\n",
            "Epoch 947/1000\n",
            "Train Loss MSE: 76670.3783\n",
            "Train Loss RMSE: 276.8942\n",
            "Test Loss MSE: 474088.5000\n",
            "Test RMSE: 688.5408\n",
            "Test R²: 0.6216\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 42/100\n",
            "\n",
            "Epoch 948/1000\n",
            "Train Loss MSE: 45195.1387\n",
            "Train Loss RMSE: 212.5915\n",
            "Test Loss MSE: 109169.9141\n",
            "Test RMSE: 330.4087\n",
            "Test R²: 0.9129\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 43/100\n",
            "\n",
            "Epoch 949/1000\n",
            "Train Loss MSE: 28970.8168\n",
            "Train Loss RMSE: 170.2082\n",
            "Test Loss MSE: 30436.2168\n",
            "Test RMSE: 174.4598\n",
            "Test R²: 0.9757\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 44/100\n",
            "\n",
            "Epoch 950/1000\n",
            "Train Loss MSE: 42067.4844\n",
            "Train Loss RMSE: 205.1036\n",
            "Test Loss MSE: 62091.8438\n",
            "Test RMSE: 249.1824\n",
            "Test R²: 0.9504\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 45/100\n",
            "\n",
            "Epoch 951/1000\n",
            "Train Loss MSE: 21941.3375\n",
            "Train Loss RMSE: 148.1261\n",
            "Test Loss MSE: 67130.8750\n",
            "Test RMSE: 259.0963\n",
            "Test R²: 0.9464\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 46/100\n",
            "\n",
            "Epoch 952/1000\n",
            "Train Loss MSE: 67441.9573\n",
            "Train Loss RMSE: 259.6959\n",
            "Test Loss MSE: 7461.6475\n",
            "Test RMSE: 86.3808\n",
            "Test R²: 0.9940\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 47/100\n",
            "\n",
            "Epoch 953/1000\n",
            "Train Loss MSE: 788509.7486\n",
            "Train Loss RMSE: 887.9807\n",
            "Test Loss MSE: 24703.9062\n",
            "Test RMSE: 157.1748\n",
            "Test R²: 0.9803\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 48/100\n",
            "\n",
            "Epoch 954/1000\n",
            "Train Loss MSE: 23230.3321\n",
            "Train Loss RMSE: 152.4150\n",
            "Test Loss MSE: 131170.7969\n",
            "Test RMSE: 362.1751\n",
            "Test R²: 0.8953\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 49/100\n",
            "\n",
            "Epoch 955/1000\n",
            "Train Loss MSE: 17784.5345\n",
            "Train Loss RMSE: 133.3587\n",
            "Test Loss MSE: 137890.7812\n",
            "Test RMSE: 371.3365\n",
            "Test R²: 0.8900\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 50/100\n",
            "\n",
            "Epoch 956/1000\n",
            "Train Loss MSE: 412321.7456\n",
            "Train Loss RMSE: 642.1228\n",
            "Test Loss MSE: 87360.8984\n",
            "Test RMSE: 295.5688\n",
            "Test R²: 0.9303\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 51/100\n",
            "\n",
            "Epoch 957/1000\n",
            "Train Loss MSE: 30295.7850\n",
            "Train Loss RMSE: 174.0568\n",
            "Test Loss MSE: 5176.7422\n",
            "Test RMSE: 71.9496\n",
            "Test R²: 0.9959\n",
            "Learning Rate: 1.00e-02\n",
            "✓ New best model! Improvement: 503.0991\n",
            "\n",
            "Epoch 958/1000\n",
            "Train Loss MSE: 22698.0141\n",
            "Train Loss RMSE: 150.6586\n",
            "Test Loss MSE: 9170.8252\n",
            "Test RMSE: 95.7644\n",
            "Test R²: 0.9927\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 1/100\n",
            "\n",
            "Epoch 959/1000\n",
            "Train Loss MSE: 71728.1970\n",
            "Train Loss RMSE: 267.8212\n",
            "Test Loss MSE: 203904.8125\n",
            "Test RMSE: 451.5582\n",
            "Test R²: 0.8373\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 2/100\n",
            "\n",
            "Epoch 960/1000\n",
            "Train Loss MSE: 389572.9036\n",
            "Train Loss RMSE: 624.1578\n",
            "Test Loss MSE: 197536.5469\n",
            "Test RMSE: 444.4508\n",
            "Test R²: 0.8424\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 3/100\n",
            "\n",
            "Epoch 961/1000\n",
            "Train Loss MSE: 27448.7613\n",
            "Train Loss RMSE: 165.6767\n",
            "Test Loss MSE: 779450.0625\n",
            "Test RMSE: 882.8647\n",
            "Test R²: 0.3780\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 4/100\n",
            "\n",
            "Epoch 962/1000\n",
            "Train Loss MSE: 35915.6921\n",
            "Train Loss RMSE: 189.5144\n",
            "Test Loss MSE: 25652.7656\n",
            "Test RMSE: 160.1648\n",
            "Test R²: 0.9795\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 5/100\n",
            "\n",
            "Epoch 963/1000\n",
            "Train Loss MSE: 855405.8000\n",
            "Train Loss RMSE: 924.8815\n",
            "Test Loss MSE: 228259.6250\n",
            "Test RMSE: 477.7652\n",
            "Test R²: 0.8178\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 6/100\n",
            "\n",
            "Epoch 964/1000\n",
            "Train Loss MSE: 25315.3349\n",
            "Train Loss RMSE: 159.1079\n",
            "Test Loss MSE: 8093.3911\n",
            "Test RMSE: 89.9633\n",
            "Test R²: 0.9935\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 7/100\n",
            "\n",
            "Epoch 965/1000\n",
            "Train Loss MSE: 28149.6000\n",
            "Train Loss RMSE: 167.7784\n",
            "Test Loss MSE: 101392.7266\n",
            "Test RMSE: 318.4222\n",
            "Test R²: 0.9191\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 8/100\n",
            "\n",
            "Epoch 966/1000\n",
            "Train Loss MSE: 97833.0937\n",
            "Train Loss RMSE: 312.7828\n",
            "Test Loss MSE: 26805.6719\n",
            "Test RMSE: 163.7244\n",
            "Test R²: 0.9786\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 9/100\n",
            "\n",
            "Epoch 967/1000\n",
            "Train Loss MSE: 41507.2298\n",
            "Train Loss RMSE: 203.7332\n",
            "Test Loss MSE: 52704.9570\n",
            "Test RMSE: 229.5756\n",
            "Test R²: 0.9579\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 10/100\n",
            "\n",
            "Epoch 968/1000\n",
            "Train Loss MSE: 114730.4965\n",
            "Train Loss RMSE: 338.7189\n",
            "Test Loss MSE: 119427.8984\n",
            "Test RMSE: 345.5834\n",
            "Test R²: 0.9047\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 11/100\n",
            "\n",
            "Epoch 969/1000\n",
            "Train Loss MSE: 48175.0637\n",
            "Train Loss RMSE: 219.4882\n",
            "Test Loss MSE: 220450.0000\n",
            "Test RMSE: 469.5210\n",
            "Test R²: 0.8241\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 12/100\n",
            "\n",
            "Epoch 970/1000\n",
            "Train Loss MSE: 32243.2231\n",
            "Train Loss RMSE: 179.5640\n",
            "Test Loss MSE: 297139.5000\n",
            "Test RMSE: 545.1050\n",
            "Test R²: 0.7629\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 13/100\n",
            "\n",
            "Epoch 971/1000\n",
            "Train Loss MSE: 20177.9404\n",
            "Train Loss RMSE: 142.0491\n",
            "Test Loss MSE: 6704.0308\n",
            "Test RMSE: 81.8781\n",
            "Test R²: 0.9946\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 14/100\n",
            "\n",
            "Epoch 972/1000\n",
            "Train Loss MSE: 21443.7910\n",
            "Train Loss RMSE: 146.4370\n",
            "Test Loss MSE: 70835.3359\n",
            "Test RMSE: 266.1491\n",
            "Test R²: 0.9435\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 15/100\n",
            "\n",
            "Epoch 973/1000\n",
            "Train Loss MSE: 98658.8905\n",
            "Train Loss RMSE: 314.1001\n",
            "Test Loss MSE: 31660.3730\n",
            "Test RMSE: 177.9336\n",
            "Test R²: 0.9747\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 16/100\n",
            "\n",
            "Epoch 974/1000\n",
            "Train Loss MSE: 67088.3669\n",
            "Train Loss RMSE: 259.0142\n",
            "Test Loss MSE: 31130.4590\n",
            "Test RMSE: 176.4383\n",
            "Test R²: 0.9752\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 17/100\n",
            "\n",
            "Epoch 975/1000\n",
            "Train Loss MSE: 88780.2580\n",
            "Train Loss RMSE: 297.9602\n",
            "Test Loss MSE: 32315.5723\n",
            "Test RMSE: 179.7653\n",
            "Test R²: 0.9742\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 18/100\n",
            "\n",
            "Epoch 976/1000\n",
            "Train Loss MSE: 49931.7460\n",
            "Train Loss RMSE: 223.4541\n",
            "Test Loss MSE: 31105.5898\n",
            "Test RMSE: 176.3678\n",
            "Test R²: 0.9752\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 19/100\n",
            "\n",
            "Epoch 977/1000\n",
            "Train Loss MSE: 195697.8002\n",
            "Train Loss RMSE: 442.3774\n",
            "Test Loss MSE: 289409.2188\n",
            "Test RMSE: 537.9677\n",
            "Test R²: 0.7690\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 20/100\n",
            "\n",
            "Epoch 978/1000\n",
            "Train Loss MSE: 14991.0040\n",
            "Train Loss RMSE: 122.4378\n",
            "Test Loss MSE: 95483.7031\n",
            "Test RMSE: 309.0044\n",
            "Test R²: 0.9238\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 21/100\n",
            "\n",
            "Epoch 979/1000\n",
            "Train Loss MSE: 223558.0364\n",
            "Train Loss RMSE: 472.8192\n",
            "Test Loss MSE: 53467.3828\n",
            "Test RMSE: 231.2302\n",
            "Test R²: 0.9573\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 22/100\n",
            "\n",
            "Epoch 980/1000\n",
            "Train Loss MSE: 57339.0977\n",
            "Train Loss RMSE: 239.4558\n",
            "Test Loss MSE: 11552.8789\n",
            "Test RMSE: 107.4843\n",
            "Test R²: 0.9908\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 23/100\n",
            "\n",
            "Epoch 981/1000\n",
            "Train Loss MSE: 799749.7156\n",
            "Train Loss RMSE: 894.2873\n",
            "Test Loss MSE: 23302.3184\n",
            "Test RMSE: 152.6510\n",
            "Test R²: 0.9814\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 24/100\n",
            "\n",
            "Epoch 982/1000\n",
            "Train Loss MSE: 972153.6122\n",
            "Train Loss RMSE: 985.9785\n",
            "Test Loss MSE: 142762.1562\n",
            "Test RMSE: 377.8388\n",
            "Test R²: 0.8861\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 25/100\n",
            "\n",
            "Epoch 983/1000\n",
            "Train Loss MSE: 41724.6550\n",
            "Train Loss RMSE: 204.2661\n",
            "Test Loss MSE: 122659.8828\n",
            "Test RMSE: 350.2283\n",
            "Test R²: 0.9021\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 26/100\n",
            "\n",
            "Epoch 984/1000\n",
            "Train Loss MSE: 466875.2834\n",
            "Train Loss RMSE: 683.2827\n",
            "Test Loss MSE: 7356.7588\n",
            "Test RMSE: 85.7715\n",
            "Test R²: 0.9941\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 27/100\n",
            "\n",
            "Epoch 985/1000\n",
            "Train Loss MSE: 24672.7590\n",
            "Train Loss RMSE: 157.0756\n",
            "Test Loss MSE: 43240.9883\n",
            "Test RMSE: 207.9447\n",
            "Test R²: 0.9655\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 28/100\n",
            "\n",
            "Epoch 986/1000\n",
            "Train Loss MSE: 28972.2217\n",
            "Train Loss RMSE: 170.2123\n",
            "Test Loss MSE: 117495.4375\n",
            "Test RMSE: 342.7761\n",
            "Test R²: 0.9062\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 29/100\n",
            "\n",
            "Epoch 987/1000\n",
            "Train Loss MSE: 31197.7960\n",
            "Train Loss RMSE: 176.6290\n",
            "Test Loss MSE: 225354.2344\n",
            "Test RMSE: 474.7149\n",
            "Test R²: 0.8202\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 30/100\n",
            "\n",
            "Epoch 988/1000\n",
            "Train Loss MSE: 22593.1457\n",
            "Train Loss RMSE: 150.3102\n",
            "Test Loss MSE: 101630.7344\n",
            "Test RMSE: 318.7958\n",
            "Test R²: 0.9189\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 31/100\n",
            "\n",
            "Epoch 989/1000\n",
            "Train Loss MSE: 38770.8787\n",
            "Train Loss RMSE: 196.9032\n",
            "Test Loss MSE: 52115.8438\n",
            "Test RMSE: 228.2889\n",
            "Test R²: 0.9584\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 32/100\n",
            "\n",
            "Epoch 990/1000\n",
            "Train Loss MSE: 33380.2434\n",
            "Train Loss RMSE: 182.7026\n",
            "Test Loss MSE: 847258.0000\n",
            "Test RMSE: 920.4662\n",
            "Test R²: 0.3238\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 33/100\n",
            "\n",
            "Epoch 991/1000\n",
            "Train Loss MSE: 31410.8434\n",
            "Train Loss RMSE: 177.2310\n",
            "Test Loss MSE: 17725.6543\n",
            "Test RMSE: 133.1377\n",
            "Test R²: 0.9859\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 34/100\n",
            "\n",
            "Epoch 992/1000\n",
            "Train Loss MSE: 91583.0900\n",
            "Train Loss RMSE: 302.6270\n",
            "Test Loss MSE: 8276.6182\n",
            "Test RMSE: 90.9759\n",
            "Test R²: 0.9934\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 35/100\n",
            "\n",
            "Epoch 993/1000\n",
            "Train Loss MSE: 16636.8086\n",
            "Train Loss RMSE: 128.9838\n",
            "Test Loss MSE: 900949.5625\n",
            "Test RMSE: 949.1836\n",
            "Test R²: 0.2810\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 36/100\n",
            "\n",
            "Epoch 994/1000\n",
            "Train Loss MSE: 27155.4392\n",
            "Train Loss RMSE: 164.7891\n",
            "Test Loss MSE: 109620.2344\n",
            "Test RMSE: 331.0895\n",
            "Test R²: 0.9125\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 37/100\n",
            "\n",
            "Epoch 995/1000\n",
            "Train Loss MSE: 56898.9620\n",
            "Train Loss RMSE: 238.5350\n",
            "Test Loss MSE: 65594.8359\n",
            "Test RMSE: 256.1149\n",
            "Test R²: 0.9477\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 38/100\n",
            "\n",
            "Epoch 996/1000\n",
            "Train Loss MSE: 24622.2276\n",
            "Train Loss RMSE: 156.9147\n",
            "Test Loss MSE: 18810.8789\n",
            "Test RMSE: 137.1528\n",
            "Test R²: 0.9850\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 39/100\n",
            "\n",
            "Epoch 997/1000\n",
            "Train Loss MSE: 39815.0962\n",
            "Train Loss RMSE: 199.5372\n",
            "Test Loss MSE: 10525.8369\n",
            "Test RMSE: 102.5955\n",
            "Test R²: 0.9916\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 40/100\n",
            "\n",
            "Epoch 998/1000\n",
            "Train Loss MSE: 319008.9831\n",
            "Train Loss RMSE: 564.8088\n",
            "Test Loss MSE: 181228.9062\n",
            "Test RMSE: 425.7099\n",
            "Test R²: 0.8554\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 41/100\n",
            "\n",
            "Epoch 999/1000\n",
            "Train Loss MSE: 17568.4788\n",
            "Train Loss RMSE: 132.5461\n",
            "Test Loss MSE: 479129.0000\n",
            "Test RMSE: 692.1914\n",
            "Test R²: 0.6176\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 42/100\n",
            "\n",
            "Epoch 1000/1000\n",
            "Train Loss MSE: 67407.9557\n",
            "Train Loss RMSE: 259.6304\n",
            "Test Loss MSE: 55149.3750\n",
            "Test RMSE: 234.8390\n",
            "Test R²: 0.9560\n",
            "Learning Rate: 1.00e-02\n",
            "Patience: 43/100\n",
            "\n",
            "Training completed!\n",
            "Best model from epoch 957\n",
            "Best test R²: 0.9959\n",
            "Best test RMSE: 71.9496\n",
            "\n",
            "Best model info:\n",
            "{'epoch': 956, 'train_loss': np.float64(30295.784974791786), 'train_rmse': np.float64(174.05684409063548), 'test_loss': 5176.7421875, 'test_rmse': np.float64(71.94958087091265), 'test_r2': 0.9958686828613281, 'learning_rate': 0.01}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on: {device}\")\n",
        "\n",
        "X_train_tensor = X_train_tensor_norm\n",
        "y_train_tensor = y_train_tensor\n",
        "X_test_tensor = X_test_tensor_norm\n",
        "y_test_tensor = y_test_tensor\n",
        "\n",
        "# Dataset e loader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)\n",
        "\n",
        "# Optimizer com weight decay para regularização adicional\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                           lr=1e-2,\n",
        "                           weight_decay=1e-5)  # L2 regularization\n",
        "\n",
        "# Scheduler mais sofisticado\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                       mode='min',\n",
        "                                                       patience=40,  # Aumentado\n",
        "                                                       factor=0.7,   # Redução mais suave\n",
        "                                                       min_lr=1e-6,\n",
        "                                                       verbose=True)\n",
        "\n",
        "# Tracking melhorado\n",
        "history = {'train_loss': [], 'test_loss': [], 'test_r2': [], 'lr': []}\n",
        "best_test_loss = float('inf')\n",
        "best_weights = None\n",
        "max_patience = 100  # Aumentado baseado na sua análise\n",
        "patience = 0\n",
        "n_epochs = 1000\n",
        "\n",
        "print(f\"Starting training for {n_epochs} epochs...\")\n",
        "print(f\"Early stopping patience: {max_patience}\")\n",
        "print(f\"Initial learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{n_epochs}\")\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    epoch_train_losses = []\n",
        "\n",
        "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping para estabilidade\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        epoch_train_losses.append(loss.item())\n",
        "\n",
        "    # Média das losses do epoch\n",
        "    epoch_train_loss = np.mean(epoch_train_losses)\n",
        "    train_rmse = np.sqrt(epoch_train_loss)\n",
        "\n",
        "    print(f\"Train Loss MSE: {epoch_train_loss:.4f}\")\n",
        "    print(f\"Train Loss RMSE: {train_rmse:.4f}\")\n",
        "\n",
        "    # Evaluation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_test_tensor.to(device))\n",
        "        test_loss = loss_fn(y_pred, y_test_tensor.to(device)).item()\n",
        "\n",
        "        test_rmse = np.sqrt(test_loss)\n",
        "\n",
        "        # R² calculation\n",
        "        ss_res = torch.sum((y_test_tensor.to(device) - y_pred) ** 2)\n",
        "        ss_tot = torch.sum((y_test_tensor.to(device) - torch.mean(y_test_tensor.to(device))) ** 2)\n",
        "        test_r2 = (1 - ss_res / ss_tot).item()\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    #scheduler.step(test_loss)\n",
        "    new_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Armazenar histórico\n",
        "    history['train_loss'].append(epoch_train_loss)\n",
        "    history['test_loss'].append(test_loss)\n",
        "    history['test_r2'].append(test_r2)\n",
        "    history['lr'].append(current_lr)\n",
        "\n",
        "    print(f\"Test Loss MSE: {test_loss:.4f}\")\n",
        "    print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "    print(f\"Test R²: {test_r2:.4f}\")\n",
        "    print(f\"Learning Rate: {current_lr:.2e}\")\n",
        "\n",
        "    if new_lr != current_lr:\n",
        "        print(f\"Learning rate reduced to: {new_lr:.2e}\")\n",
        "\n",
        "    # Early stopping com melhor modelo\n",
        "    if test_loss < best_test_loss:\n",
        "        improvement = best_test_loss - test_loss\n",
        "        best_test_loss = test_loss\n",
        "        best_weights = model.state_dict().copy()\n",
        "        save_info_best_model = {\n",
        "            'epoch': epoch,\n",
        "            'train_loss': epoch_train_loss,\n",
        "            'train_rmse': train_rmse,\n",
        "            'test_loss': test_loss,\n",
        "            'test_rmse': test_rmse,\n",
        "            'test_r2': test_r2,\n",
        "            'learning_rate': current_lr\n",
        "        }\n",
        "        patience = 0\n",
        "        print(f\"✓ New best model! Improvement: {improvement:.4f}\")\n",
        "    else:\n",
        "        patience += 1\n",
        "        print(f\"Patience: {patience}/{max_patience}\")\n",
        "\n",
        "        if patience >= max_patience and test_loss < 80:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch+1}!\")\n",
        "            print(f\"Best test loss: {best_test_loss:.4f}\")\n",
        "            break\n",
        "\n",
        "# Carregar o melhor modelo\n",
        "model.load_state_dict(best_weights)\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Best model from epoch {save_info_best_model['epoch']+1}\")\n",
        "print(f\"Best test R²: {save_info_best_model['test_r2']:.4f}\")\n",
        "print(f\"Best test RMSE: {save_info_best_model['test_rmse']:.4f}\")\n",
        "\n",
        "# Salvar informações do melhor modelo\n",
        "print(\"\\nBest model info:\")\n",
        "print(save_info_best_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_info_best_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhI17UIyUIX-",
        "outputId": "23c73d66-fd6f-4168-a8e5-ff5cf01beeda"
      },
      "id": "DhI17UIyUIX-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 956,\n",
              " 'train_loss': np.float64(30295.784974791786),\n",
              " 'train_rmse': np.float64(174.05684409063548),\n",
              " 'test_loss': 5176.7421875,\n",
              " 'test_rmse': np.float64(71.94958087091265),\n",
              " 'test_r2': 0.9958686828613281,\n",
              " 'learning_rate': 0.01}"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# i want to know how good is the model in the test dataset\n",
        "# load weigths in the model\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/best_model_state_dict.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnbIcnlKef9a",
        "outputId": "dafb2754-3d7c-4ebb-80fd-8376dd9d64de"
      },
      "id": "gnbIcnlKef9a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OptimizedModule(\n",
              "  (_orig_mod): LinearMostSimple(\n",
              "    (l1): Linear(in_features=22, out_features=18, bias=True)\n",
              "    (norm): BatchNorm1d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU()\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (l2): Linear(in_features=18, out_features=12, bias=True)\n",
              "    (norm1): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU()\n",
              "    (l3): Linear(in_features=12, out_features=6, bias=True)\n",
              "    (norm2): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu2): ReLU()\n",
              "    (l4): Linear(in_features=6, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XJrorFCpBDqd",
      "metadata": {
        "id": "XJrorFCpBDqd"
      },
      "source": [
        "#5 - Comparação e treinamento entre modelos não DNN\n",
        "\n",
        "\n",
        "*   Os modelos em holdout foram treinados da mesma forma que MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AZEfadqPgKLs",
      "metadata": {
        "id": "AZEfadqPgKLs"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class RegressionComparison:\n",
        "    def __init__(self, X, y, test_size=0.2, random_state=42, normalize=False):\n",
        "        \"\"\"\n",
        "        Inicializa a comparação de algoritmos de regressão\n",
        "\n",
        "        Parameters:\n",
        "        X: features (já preparadas e normalizadas)\n",
        "        y: target variable\n",
        "        test_size: proporção para teste\n",
        "        random_state: seed para reproducibilidade\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.test_size = test_size\n",
        "        self.random_state = random_state\n",
        "        self.results = {}\n",
        "        self.grid_search_results = {}\n",
        "\n",
        "        # Split inicial\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=random_state\n",
        "        )\n",
        "\n",
        "        # Verifica se quer normalizar\n",
        "        if normalize:\n",
        "            self.normalize_data()\n",
        "\n",
        "    def normalize_data(self):\n",
        "        \"\"\"Normaliza os dados\"\"\"\n",
        "        scaler = StandardScaler()\n",
        "        self.X_train = scaler.fit_transform(self.X_train)\n",
        "        self.X_test = scaler.transform(self.X_test)\n",
        "\n",
        "    def rmse(self, y_true, y_pred):\n",
        "        \"\"\"Calcula RMSE\"\"\"\n",
        "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "    def mse(self, y_true, y_pred):\n",
        "        \"\"\"Calcula MSE\"\"\"\n",
        "        return mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    def holdout_simple(self):\n",
        "        \"\"\"Holdout Simples - treina e testa uma vez\"\"\"\n",
        "        print(\"=== SIMPLE HOLDOUT ===\")\n",
        "\n",
        "        models = {\n",
        "            'Linear Regression': LinearRegression(),\n",
        "            'Decision Tree': DecisionTreeRegressor(\n",
        "                random_state=self.random_state),\n",
        "            'Random Forest': RandomForestRegressor(\n",
        "                n_estimators=100,\n",
        "                random_state=self.random_state),\n",
        "            'K-NN': KNeighborsRegressor(n_neighbors=5),\n",
        "            'SVR': SVR(kernel='rbf', C=1.0),\n",
        "            'XGBoost': xgb.XGBRegressor(\n",
        "                random_state=self.random_state,\n",
        "                verbosity=0)\n",
        "        }\n",
        "\n",
        "        holdout_results = {}\n",
        "\n",
        "        for name, model in models.items():\n",
        "            # Treinar\n",
        "            model.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Predizer\n",
        "            y_pred = model.predict(self.X_test)\n",
        "\n",
        "            # Métricas\n",
        "            rmse = self.rmse(self.y_test, y_pred)\n",
        "            mse = self.mse(self.y_test, y_pred)\n",
        "            r2 = r2_score(self.y_test, y_pred)\n",
        "\n",
        "            holdout_results[name] = {\n",
        "                'RMSE': rmse,\n",
        "                'MSE': mse,\n",
        "                'R²': r2\n",
        "            }\n",
        "\n",
        "            print(f\"{name:20} - RMSE: {rmse:.4f}, MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "        self.results['Holdout'] = holdout_results\n",
        "        return holdout_results\n",
        "\n",
        "    def holdout_optimized_models(self):\n",
        "        \"\"\"Holdout com modelos otimizados do Grid Search\"\"\"\n",
        "        if 'Grid Search' not in self.results:\n",
        "            print(\"Grid Search has not been executed yet. Run grid_search_cv() first.\")\n",
        "            return None\n",
        "\n",
        "        print(\"\\n=== HOLDOUT WITH OPTIMIZED MODELS ===\")\n",
        "\n",
        "        holdout_optimized_results = {}\n",
        "\n",
        "        for name, grid_result in self.grid_search_results.items():\n",
        "            # Pegar o melhor modelo do Grid Search\n",
        "            best_model = grid_result['best_model']\n",
        "\n",
        "            # O modelo já foi treinado no Grid Search, mas vamos retreinar\n",
        "            # no conjunto de treino completo para garantir consistência\n",
        "            best_model.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Predizer para o conjunto de treino\n",
        "            y_train_pred = best_model.predict(self.X_train)\n",
        "            # Predizer no conjunto de teste\n",
        "            y_pred = best_model.predict(self.X_test)\n",
        "\n",
        "            # Calcular métricas\n",
        "            rmse_train = self.rmse(self.y_train, y_train_pred)\n",
        "            rmse = self.rmse(self.y_test, y_pred)\n",
        "            mse = self.mse(self.y_test, y_pred)\n",
        "            r2 = r2_score(self.y_test, y_pred)\n",
        "\n",
        "            holdout_optimized_results[name] = {\n",
        "                'RMSE_train': rmse_train,\n",
        "                'RMSE': rmse,\n",
        "                'MSE': mse,\n",
        "                'R²': r2,\n",
        "                'best_params': grid_result['best_params']\n",
        "            }\n",
        "\n",
        "            print(f\"RMSE Train: {rmse_train:4f} - RMSE Test: {rmse:.4f}, MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
        "            print(f\"{'':20}   Best params: {grid_result['best_params']}\")\n",
        "\n",
        "        self.results['Holdout Optimized'] = holdout_optimized_results\n",
        "        return holdout_optimized_results\n",
        "\n",
        "    def grid_search_cv(self, k=5, scoring='neg_mean_squared_error', n_jobs=-1):\n",
        "        \"\"\"Grid Search Cross-Validation para otimização de hiperparâmetros\"\"\"\n",
        "        print(f\"\\n=== GRID SEARCH {k}-FOLD CROSS-VALIDATION ===\")\n",
        "\n",
        "        # Definição dos modelos e seus hiperparâmetros para otimização\n",
        "        models_and_params = {\n",
        "            'Linear Regression': {\n",
        "                'model': LinearRegression(),\n",
        "                'params': {\n",
        "                    # fit_intercept: Se True, calcula o termo de intercepto (bias)\n",
        "                    # Se False, assume que os dados já estão centralizados\n",
        "                    'fit_intercept': [True, False]\n",
        "                }\n",
        "            },\n",
        "            'Decision Tree': {\n",
        "                'model': DecisionTreeRegressor(random_state=self.random_state),\n",
        "                'params': {\n",
        "                    # max_depth: Profundidade máxima da árvore\n",
        "                    # None = sem limite (pode causar overfitting)\n",
        "                    # Valores baixos = underfitting, valores altos = overfitting\n",
        "                    'max_depth': [5, 10, None],\n",
        "\n",
        "                    # min_samples_split: Número mínimo de amostras para dividir um nó interno\n",
        "                    # Valores altos = árvore mais conservadora (menos overfitting)\n",
        "                    # Valores baixos = árvore mais complexa (mais overfitting)\n",
        "                    'min_samples_split': [2, 10],\n",
        "\n",
        "                    # min_samples_leaf: Número mínimo de amostras em uma folha\n",
        "                    # Controla o tamanho mínimo das folhas da árvore\n",
        "                    # Valores altos = generalização melhor\n",
        "                    'min_samples_leaf': [1, 4],\n",
        "\n",
        "                    # criterion: Função para medir a qualidade de uma divisão\n",
        "                    # squared_error: usa variância para regressão\n",
        "                    'criterion': ['squared_error']\n",
        "                }\n",
        "            },\n",
        "            'Random Forest': {\n",
        "                'model': RandomForestRegressor(random_state=self.random_state),\n",
        "                'params': {\n",
        "                    # n_estimators: Número de árvores na floresta\n",
        "                    # Mais árvores = melhor performance mas mais lento\n",
        "                    # Retornos diminuem após certo ponto\n",
        "                    'n_estimators': [50, 100, 200],\n",
        "\n",
        "                    # max_depth: Profundidade máxima de cada árvore\n",
        "                    # None = nós são expandidos até que folhas sejam puras\n",
        "                    # Controla overfitting das árvores individuais\n",
        "                    'max_depth': [5, None],\n",
        "\n",
        "                    # min_samples_split: Mínimo de amostras para dividir nó interno\n",
        "                    # Previne divisões com poucas amostras\n",
        "                    'min_samples_split': [1, 2, 10],\n",
        "\n",
        "                    # max_features: Número de features consideradas para melhor divisão\n",
        "                    # sqrt: raiz quadrada do total de features\n",
        "                    # None: usa todas as features\n",
        "                    # Controla aleatoriedade e generalização\n",
        "                    'max_features': ['sqrt', None]\n",
        "                }\n",
        "            },\n",
        "            'K-NN': {\n",
        "                'model': KNeighborsRegressor(),\n",
        "                'params': {\n",
        "                    # n_neighbors: Número de vizinhos mais próximos\n",
        "                    # Valores baixos = mais sensível a ruído (overfitting)\n",
        "                    # Valores altos = decisão mais suave (underfitting)\n",
        "                    'n_neighbors': [3, 5, 7],\n",
        "\n",
        "                    # weights: Função de peso para vizinhos\n",
        "                    # uniform: todos os vizinhos têm peso igual\n",
        "                    # distance: peso inversamente proporcional à distância\n",
        "                    'weights': ['uniform', 'distance'],\n",
        "\n",
        "                    # algorithm: Algoritmo para computar vizinhos mais próximos\n",
        "                    # auto: tenta decidir automaticamente o melhor algoritmo\n",
        "                    'algorithm': ['auto'],\n",
        "\n",
        "                    # p: Parâmetro para métrica de distância Minkowski\n",
        "                    # p=1: distância Manhattan, p=2: distância Euclidiana\n",
        "                    'p': [2]\n",
        "                }\n",
        "            },\n",
        "            'SVR': {\n",
        "                'model': SVR(),\n",
        "                'params': {\n",
        "                    # kernel: Especifica tipo de kernel usado no algoritmo\n",
        "                    # rbf: Radial Basis Function - bom para dados não-lineares\n",
        "                    # linear: para relações lineares, poly: polinomial\n",
        "                    'kernel': ['linear'],\n",
        "\n",
        "                    # C: Parâmetro de regularização\n",
        "                    # Valores baixos = mais regularização (underfitting)\n",
        "                    # Valores altos = menos regularização (overfitting)\n",
        "                    'C': [100],\n",
        "\n",
        "                    # gamma: Coeficiente do kernel para rbf, poly e sigmoid\n",
        "                    # scale: 1 / (n_features * X.var())\n",
        "                    # Valores altos = influência de cada exemplo de treino é baixa\n",
        "                    'gamma': [0.1],\n",
        "\n",
        "                    # epsilon: Especifica epsilon-tube dentro do qual não há penalidade\n",
        "                    # Controla o número de support vectors\n",
        "                    # Valores maiores = modelo mais simples\n",
        "                    'epsilon': [0.2]\n",
        "                }\n",
        "            },\n",
        "            'XGBoost': {\n",
        "                'model': xgb.XGBRegressor(random_state=self.random_state, verbosity=0),\n",
        "                'params': {\n",
        "                    # n_estimators: Número de árvores de gradient boosting\n",
        "                    # Mais árvores = melhor performance mas risco de overfitting\n",
        "                    'n_estimators': [50, 100],\n",
        "\n",
        "                    # max_depth: Profundidade máxima de cada árvore\n",
        "                    # Controla complexidade do modelo\n",
        "                    # Valores altos podem causar overfitting\n",
        "                    'max_depth': [3, 5],\n",
        "\n",
        "                    # learning_rate: Taxa de aprendizado do boosting\n",
        "                    # Valores baixos = treinamento mais lento mas melhor generalização\n",
        "                    # Valores altos = treinamento mais rápido mas risco overfitting\n",
        "                    'learning_rate': [0.01, 0.1, 0.2],\n",
        "\n",
        "                    # subsample: Fração de amostras usadas para treinar cada árvore\n",
        "                    # Valores < 1.0 = stochastic gradient boosting\n",
        "                    # Previne overfitting introduzindo aleatoriedade\n",
        "                    'subsample': [0.9],\n",
        "\n",
        "                    # colsample_bytree: Fração de features usadas para cada árvore\n",
        "                    # Controla aleatoriedade e previne overfitting\n",
        "                    # Valores baixos = mais regularização\n",
        "                    'colsample_bytree': [0.6, 0.9]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        grid_results = {}\n",
        "\n",
        "        for name, model_info in models_and_params.items():\n",
        "            # Calcular número de combinações de hiperparâmetros\n",
        "            param_combinations = 1\n",
        "            for param_values in model_info['params'].values():\n",
        "                param_combinations *= len(param_values)\n",
        "\n",
        "            total_fits = param_combinations * k\n",
        "            print(f\"\\nOptimizing {name}... ({param_combinations} combinations × {k}-fold = {total_fits} fits)\")\n",
        "\n",
        "            # Grid Search com validação cruzada\n",
        "            grid_search = GridSearchCV(\n",
        "                estimator=model_info['model'],\n",
        "                param_grid=model_info['params'],\n",
        "                cv=k,\n",
        "                scoring=scoring,  # neg_mean_squared_error para minimizar MSE\n",
        "                n_jobs=n_jobs,   # -1 usa todos os processadores disponíveis\n",
        "                verbose=1        # Mostra progresso\n",
        "            )\n",
        "\n",
        "            # Treinar no conjunto de treino\n",
        "            grid_search.fit(self.X_train, self.y_train)\n",
        "\n",
        "            # Melhor modelo encontrado\n",
        "            best_model = grid_search.best_estimator_\n",
        "\n",
        "            # Predições no conjunto de teste\n",
        "            y_pred = best_model.predict(self.X_test)\n",
        "\n",
        "            # Calcular métricas no conjunto de teste\n",
        "            rmse = self.rmse(self.y_test, y_pred)\n",
        "            mse = self.mse(self.y_test, y_pred)\n",
        "            r2 = r2_score(self.y_test, y_pred)\n",
        "\n",
        "            # Armazenar resultados\n",
        "            grid_results[name] = {\n",
        "                'best_params': grid_search.best_params_,\n",
        "                'best_cv_score': -grid_search.best_score_,  # Converter de neg_MSE para MSE\n",
        "                'test_RMSE': rmse,\n",
        "                'test_MSE': mse,\n",
        "                'test_R²': r2,\n",
        "                'cv_RMSE': np.sqrt(-grid_search.best_score_),  # RMSE da validação cruzada\n",
        "                'best_model': best_model\n",
        "            }\n",
        "\n",
        "            print(f\"{name:20} - CV RMSE: {np.sqrt(-grid_search.best_score_):.4f}, \"\n",
        "                f\"Test RMSE: {rmse:.4f}, Test R²: {r2:.4f}\")\n",
        "            print(f\"{'':20}   Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "        self.results['Grid Search'] = grid_results\n",
        "        self.grid_search_results = grid_results\n",
        "        return grid_results\n",
        "\n",
        "    def run_all_experiments(self, include_grid_search=True, include_holdout_optimized=True):\n",
        "        \"\"\"Executa todos os experimentos de comparação\"\"\"\n",
        "        print(\"REGRESSION ALGORITHMS COMPARISON\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Dataset shape: X={self.X.shape}, y={self.y.shape}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Holdout simples com parâmetros default\n",
        "        self.holdout_simple()\n",
        "\n",
        "        if include_grid_search:\n",
        "            # Grid Search para otimização de hiperparâmetros\n",
        "            self.grid_search_cv(k=10)\n",
        "\n",
        "            if include_holdout_optimized:\n",
        "                # Holdout com modelos otimizados\n",
        "                self.holdout_optimized_models()\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def get_optimized_models(self):\n",
        "        \"\"\"Retorna os modelos otimizados do Grid Search\"\"\"\n",
        "        if 'Grid Search' not in self.results:\n",
        "            print(\"Grid Search has not been executed yet. Run grid_search_cv() first.\")\n",
        "            return None\n",
        "\n",
        "        optimized_models = {}\n",
        "        for name, result in self.grid_search_results.items():\n",
        "            optimized_models[name] = result['best_model']\n",
        "\n",
        "        return optimized_models\n",
        "\n",
        "    def compare_holdout_methods(self):\n",
        "        \"\"\"Compara os resultados do holdout simples vs holdout otimizado\"\"\"\n",
        "        if 'Holdout' not in self.results or 'Holdout Optimized' not in self.results:\n",
        "            print(\"Both holdout_simple() and holdout_optimized_models() must be executed first\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"COMPARISON: SIMPLE HOLDOUT vs OPTIMIZED HOLDOUT\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Criar dados para comparação\n",
        "        comparison_data = []\n",
        "\n",
        "        for model_name in self.results['Holdout'].keys():\n",
        "            if model_name in self.results['Holdout Optimized']:\n",
        "                simple_rmse = self.results['Holdout'][model_name]['RMSE']\n",
        "                optimized_rmse = self.results['Holdout Optimized'][model_name]['RMSE']\n",
        "                improvement = ((simple_rmse - optimized_rmse) / simple_rmse) * 100\n",
        "\n",
        "                comparison_data.append({\n",
        "                    'Model': model_name,\n",
        "                    'Simple RMSE': simple_rmse,\n",
        "                    'Optimized RMSE': optimized_rmse,\n",
        "                    'Improvement (%)': improvement,\n",
        "                    'Simple R²': self.results['Holdout'][model_name]['R²'],\n",
        "                    'Optimized R²': self.results['Holdout Optimized'][model_name]['R²']\n",
        "                })\n",
        "\n",
        "        # Mostrar comparação detalhada\n",
        "        for data in comparison_data:\n",
        "            print(f\"\\n{data['Model']}:\")\n",
        "            print(f\"  Simple RMSE:     {data['Simple RMSE']:.4f}\")\n",
        "            print(f\"  Optimized RMSE:  {data['Optimized RMSE']:.4f}\")\n",
        "            print(f\"  Improvement:     {data['Improvement (%)']:+.2f}%\")\n",
        "            print(f\"  Simple R²:       {data['Simple R²']:.4f}\")\n",
        "            print(f\"  Optimized R²:    {data['Optimized R²']:.4f}\")\n",
        "\n",
        "        # Encontrar melhores modelos\n",
        "        best_simple = min(comparison_data, key=lambda x: x['Simple RMSE'])\n",
        "        best_optimized = min(comparison_data, key=lambda x: x['Optimized RMSE'])\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"BEST SIMPLE MODEL: {best_simple['Model']} (RMSE: {best_simple['Simple RMSE']:.4f})\")\n",
        "        print(f\"BEST OPTIMIZED MODEL: {best_optimized['Model']} (RMSE: {best_optimized['Optimized RMSE']:.4f})\")\n",
        "\n",
        "        return comparison_data\n",
        "\n",
        "    def compare_methods_summary(self):\n",
        "        \"\"\"Compara os resultados de todos os métodos em uma tabela resumo\"\"\"\n",
        "        if not self.results:\n",
        "            print(\"No experiments have been executed yet.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"COMPARATIVE SUMMARY - BEST MODELS BY METHOD\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        best_models = self.get_best_models()\n",
        "\n",
        "        for method, info in best_models.items():\n",
        "            print(f\"\\n{method}:\")\n",
        "            print(f\"  Best model: {info['model']}\")\n",
        "            print(f\"  RMSE: {info['RMSE']:.4f}\")\n",
        "            print(f\"  MSE: {info['MSE']:.4f}\")\n",
        "            print(f\"  R²: {info['R²']:.4f}\")\n",
        "            if 'best_params' in info:\n",
        "                print(f\"  Parameters: {info['best_params']}\")\n",
        "\n",
        "    def get_best_models(self):\n",
        "        \"\"\"Retorna o melhor modelo de cada método baseado no menor RMSE\"\"\"\n",
        "        best_models = {}\n",
        "\n",
        "        for method, results in self.results.items():\n",
        "            if method == 'Grid Search':\n",
        "                continue  # Grid Search é analisado através do Holdout Optimized\n",
        "\n",
        "            # Encontrar modelo com menor RMSE\n",
        "            best_model_name = min(results.keys(), key=lambda x: results[x]['RMSE'])\n",
        "            best_result = results[best_model_name]\n",
        "\n",
        "            best_models[method] = {\n",
        "                'model': best_model_name,\n",
        "                'RMSE': best_result['RMSE'],\n",
        "                'MSE': best_result['MSE'],\n",
        "                'R²': best_result['R²']\n",
        "            }\n",
        "\n",
        "            # Adicionar parâmetros otimizados se disponível\n",
        "            if 'best_params' in best_result:\n",
        "                best_models[method]['best_params'] = best_result['best_params']\n",
        "\n",
        "        return best_models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rp_jidRzQgo9",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rp_jidRzQgo9",
        "outputId": "20fb1d42-dfda-48bf-f34d-b66192d22b11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REGRESSION ALGORITHMS COMPARISON\n",
            "============================================================\n",
            "Dataset shape: X=(3924, 22), y=(3924,)\n",
            "============================================================\n",
            "=== SIMPLE HOLDOUT ===\n",
            "Linear Regression    - RMSE: 664.7464, MSE: 441887.7830, R²: 0.6473\n",
            "Decision Tree        - RMSE: 428.7085, MSE: 183790.9771, R²: 0.8533\n",
            "Random Forest        - RMSE: 601.4610, MSE: 361755.2818, R²: 0.7113\n",
            "K-NN                 - RMSE: 759.1935, MSE: 576374.7719, R²: 0.5400\n",
            "SVR                  - RMSE: 1114.9864, MSE: 1243194.6650, R²: 0.0079\n",
            "XGBoost              - RMSE: 958.1072, MSE: 917969.4288, R²: 0.2674\n",
            "\n",
            "=== GRID SEARCH 10-FOLD CROSS-VALIDATION ===\n",
            "\n",
            "Optimizing Linear Regression... (2 combinations × 10-fold = 20 fits)\n",
            "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
            "Linear Regression    - CV RMSE: 195.4415, Test RMSE: 664.7779, Test R²: 0.6473\n",
            "                       Best parameters: {'fit_intercept': False}\n",
            "\n",
            "Optimizing Decision Tree... (12 combinations × 10-fold = 120 fits)\n",
            "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
            "Decision Tree        - CV RMSE: 945.9359, Test RMSE: 428.7085, Test R²: 0.8533\n",
            "                       Best parameters: {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
            "\n",
            "Optimizing Random Forest... (36 combinations × 10-fold = 360 fits)\n",
            "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n",
            "Random Forest        - CV RMSE: 1018.2141, Test RMSE: 617.1564, Test R²: 0.6960\n",
            "                       Best parameters: {'max_depth': None, 'max_features': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Optimizing K-NN... (6 combinations × 10-fold = 60 fits)\n",
            "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
            "K-NN                 - CV RMSE: 1036.8033, Test RMSE: 672.5547, Test R²: 0.6390\n",
            "                       Best parameters: {'algorithm': 'auto', 'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\n",
            "\n",
            "Optimizing SVR... (4 combinations × 10-fold = 40 fits)\n",
            "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
          ]
        }
      ],
      "source": [
        "# 2. Inicialize a comparação\n",
        "comparison = RegressionComparison(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Execute todos os experimentos\n",
        "results = comparison.run_all_experiments(\n",
        "    include_grid_search=True,\n",
        "    include_holdout_optimized=True\n",
        ")\n",
        "\n",
        "# 4. Compare os métodos\n",
        "comparison.compare_holdout_methods()\n",
        "\n",
        "# 5. Resumo geral\n",
        "comparison.compare_methods_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fmc73oMZ47P0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmc73oMZ47P0",
        "outputId": "ca801880-2872-4779-ca5e-a6140408e08e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REGRESSION ALGORITHMS COMPARISON\n",
            "============================================================\n",
            "Dataset shape: X=(3924, 22), y=(3924,)\n",
            "============================================================\n",
            "=== SIMPLE HOLDOUT ===\n",
            "Linear Regression    - RMSE: 664.7464, MSE: 441887.7830, R²: 0.6473\n",
            "Decision Tree        - RMSE: 428.7139, MSE: 183795.6127, R²: 0.8533\n",
            "Random Forest        - RMSE: 601.4489, MSE: 361740.7836, R²: 0.7113\n",
            "K-NN                 - RMSE: 744.1072, MSE: 553695.4704, R²: 0.5581\n",
            "SVR                  - RMSE: 1120.4306, MSE: 1255364.7565, R²: -0.0019\n",
            "XGBoost              - RMSE: 958.1072, MSE: 917969.4288, R²: 0.2674\n",
            "\n",
            "=== GRID SEARCH 10-FOLD CROSS-VALIDATION ===\n",
            "\n",
            "Optimizing Linear Regression... (2 combinations × 10-fold = 20 fits)\n",
            "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n",
            "Linear Regression    - CV RMSE: 195.6026, Test RMSE: 664.7464, Test R²: 0.6473\n",
            "                       Best parameters: {'fit_intercept': True}\n",
            "\n",
            "Optimizing Decision Tree... (12 combinations × 10-fold = 120 fits)\n",
            "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
            "Decision Tree        - CV RMSE: 945.9358, Test RMSE: 428.7139, Test R²: 0.8533\n",
            "                       Best parameters: {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
            "\n",
            "Optimizing Random Forest... (16 combinations × 10-fold = 160 fits)\n",
            "Fitting 10 folds for each of 16 candidates, totalling 160 fits\n",
            "Random Forest        - CV RMSE: 1021.2527, Test RMSE: 621.4728, Test R²: 0.6918\n",
            "                       Best parameters: {'max_depth': 5, 'max_features': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "\n",
            "Optimizing K-NN... (6 combinations × 10-fold = 60 fits)\n",
            "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
            "K-NN                 - CV RMSE: 993.1670, Test RMSE: 751.5150, Test R²: 0.5493\n",
            "                       Best parameters: {'algorithm': 'auto', 'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\n",
            "\n",
            "Optimizing SVR... (12 combinations × 10-fold = 120 fits)\n",
            "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
            "SVR                  - CV RMSE: 1393.3130, Test RMSE: 1115.6200, Test R²: 0.0067\n",
            "                       Best parameters: {'C': 10, 'epsilon': 0.2, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "\n",
            "Optimizing XGBoost... (12 combinations × 10-fold = 120 fits)\n",
            "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n",
            "XGBoost              - CV RMSE: 1337.9220, Test RMSE: 987.0951, Test R²: 0.2224\n",
            "                       Best parameters: {'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.9}\n",
            "\n",
            "=== HOLDOUT WITH OPTIMIZED MODELS ===\n",
            "RMSE Train: 123.636418 - RMSE Test: 664.7464, MSE: 441887.7830, R²: 0.6473\n",
            "                       Best params: {'fit_intercept': True}\n",
            "RMSE Train: 0.000000 - RMSE Test: 428.7139, MSE: 183795.6127, R²: 0.8533\n",
            "                       Best params: {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
            "RMSE Train: 415.377303 - RMSE Test: 621.4728, MSE: 386228.4358, R²: 0.6918\n",
            "                       Best params: {'max_depth': 5, 'max_features': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "RMSE Train: 0.000003 - RMSE Test: 751.5150, MSE: 564774.7493, R²: 0.5493\n",
            "                       Best params: {'algorithm': 'auto', 'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\n",
            "RMSE Train: 1392.409715 - RMSE Test: 1115.6200, MSE: 1244607.9493, R²: 0.0067\n",
            "                       Best params: {'C': 10, 'epsilon': 0.2, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "RMSE Train: 143.095393 - RMSE Test: 987.0951, MSE: 974356.6698, R²: 0.2224\n",
            "                       Best params: {'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50, 'subsample': 0.9}\n",
            "\n",
            "================================================================================\n",
            "COMPARISON: SIMPLE HOLDOUT vs OPTIMIZED HOLDOUT\n",
            "================================================================================\n",
            "\n",
            "Linear Regression:\n",
            "  Simple RMSE:     664.7464\n",
            "  Optimized RMSE:  664.7464\n",
            "  Improvement:     +0.00%\n",
            "  Simple R²:       0.6473\n",
            "  Optimized R²:    0.6473\n",
            "\n",
            "Decision Tree:\n",
            "  Simple RMSE:     428.7139\n",
            "  Optimized RMSE:  428.7139\n",
            "  Improvement:     +0.00%\n",
            "  Simple R²:       0.8533\n",
            "  Optimized R²:    0.8533\n",
            "\n",
            "Random Forest:\n",
            "  Simple RMSE:     601.4489\n",
            "  Optimized RMSE:  621.4728\n",
            "  Improvement:     -3.33%\n",
            "  Simple R²:       0.7113\n",
            "  Optimized R²:    0.6918\n",
            "\n",
            "K-NN:\n",
            "  Simple RMSE:     744.1072\n",
            "  Optimized RMSE:  751.5150\n",
            "  Improvement:     -1.00%\n",
            "  Simple R²:       0.5581\n",
            "  Optimized R²:    0.5493\n",
            "\n",
            "SVR:\n",
            "  Simple RMSE:     1120.4306\n",
            "  Optimized RMSE:  1115.6200\n",
            "  Improvement:     +0.43%\n",
            "  Simple R²:       -0.0019\n",
            "  Optimized R²:    0.0067\n",
            "\n",
            "XGBoost:\n",
            "  Simple RMSE:     958.1072\n",
            "  Optimized RMSE:  987.0951\n",
            "  Improvement:     -3.03%\n",
            "  Simple R²:       0.2674\n",
            "  Optimized R²:    0.2224\n",
            "\n",
            "==================================================\n",
            "BEST SIMPLE MODEL: Decision Tree (RMSE: 428.7139)\n",
            "BEST OPTIMIZED MODEL: Decision Tree (RMSE: 428.7139)\n",
            "\n",
            "================================================================================\n",
            "COMPARATIVE SUMMARY - BEST MODELS BY METHOD\n",
            "================================================================================\n",
            "\n",
            "Holdout:\n",
            "  Best model: Decision Tree\n",
            "  RMSE: 428.7139\n",
            "  MSE: 183795.6127\n",
            "  R²: 0.8533\n",
            "\n",
            "Holdout Optimized:\n",
            "  Best model: Decision Tree\n",
            "  RMSE: 428.7139\n",
            "  MSE: 183795.6127\n",
            "  R²: 0.8533\n",
            "  Parameters: {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
          ]
        }
      ],
      "source": [
        "# Neste caso apenas KNN e SVR vão ter alguma melhora pois temos os dados normalizados\n",
        "# 2. Inicialize a comparação\n",
        "comparison = RegressionComparison(X, y, test_size=0.2, random_state=42, normalize=True)\n",
        "\n",
        "# 3. Execute todos os experimentos\n",
        "results = comparison.run_all_experiments(\n",
        "    include_grid_search=True,\n",
        "    include_holdout_optimized=True\n",
        ")\n",
        "\n",
        "# 4. Compare os métodos\n",
        "comparison.compare_holdout_methods()\n",
        "\n",
        "# 5. Resumo geral\n",
        "comparison.compare_methods_summary()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11a02b90"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Função de visualização\n",
        "def plot_dif(nome_modelo, y_pred, y_true):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(y_true.values[:100], label='Real', marker='o', linestyle='-', alpha=0.7)\n",
        "    plt.plot(y_pred[:100], label='Predito', marker='x', linestyle='--', alpha=0.7)\n",
        "    plt.title(f\"{nome_modelo} Real vs. Predito (100 amostras)\")\n",
        "    plt.xlabel(\"Índice da Amostra\")\n",
        "    plt.ylabel(\"Qtd. Veículos\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "id": "11a02b90",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "3f317fc9",
        "outputId": "b19d1d47-5713-4bfc-b5d0-40f8b1d9c2ed"
      },
      "source": [
        "# Make predictions with the loaded MLP model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    y_pred_mlp = model(X_test_tensor_norm).cpu().numpy()\n",
        "\n",
        "# Flatten the predictions to match the shape of y_test\n",
        "y_pred_mlp = y_pred_mlp.flatten()\n",
        "\n",
        "# Plot the difference for the MLP model\n",
        "plot_dif(\"MLP (Loaded Model)\", y_pred_mlp, y_test)\n",
        "\n",
        "# Create the comparison DataFrame\n",
        "df_comparacao = pd.DataFrame({\n",
        "    'Qtd. Veículos (Real)': y_test,\n",
        "    'Qtd. Veículos (Predito)': y_pred_mlp\n",
        "})\n",
        "\n",
        "# Display the comparison DataFrame\n",
        "display(df_comparacao.head())\n",
        "\n",
        "# Calculate descriptive statistics\n",
        "numeric_cols = ['Qtd. Veículos (Real)', 'Qtd. Veículos (Predito)']\n",
        "medidas_tendencia = pd.DataFrame(index=numeric_cols)\n",
        "\n",
        "for col in numeric_cols:\n",
        "    medidas_tendencia.loc[col, 'Média'] = df_comparacao[col].mean()\n",
        "    medidas_tendencia.loc[col, 'Mediana'] = df_comparacao[col].median()\n",
        "    medidas_tendencia.loc[col, 'Moda'] = df_comparacao[col].mode().iloc[0] if not df_comparacao[col].mode().empty else None\n",
        "    medidas_tendencia.loc[col, 'Mínimo'] = df_comparacao[col].min()\n",
        "    medidas_tendencia.loc[col, 'Máximo'] = df_comparacao[col].max()\n",
        "\n",
        "display(medidas_tendencia)"
      ],
      "id": "3f317fc9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      Qtd. Veículos (Real)  Qtd. Veículos (Predito)\n",
              "3649                   4.0                 4.709765\n",
              "463                    7.0                 4.228819\n",
              "2764                   2.0                 5.988323\n",
              "2384                  15.0                 7.932024\n",
              "2091                  20.0                 9.119263"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fd31d618-e0d6-4279-b5ff-083401845c95\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Qtd. Veículos (Real)</th>\n",
              "      <th>Qtd. Veículos (Predito)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3649</th>\n",
              "      <td>4.0</td>\n",
              "      <td>4.709765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>7.0</td>\n",
              "      <td>4.228819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>2.0</td>\n",
              "      <td>5.988323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2384</th>\n",
              "      <td>15.0</td>\n",
              "      <td>7.932024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2091</th>\n",
              "      <td>20.0</td>\n",
              "      <td>9.119263</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd31d618-e0d6-4279-b5ff-083401845c95')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fd31d618-e0d6-4279-b5ff-083401845c95 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fd31d618-e0d6-4279-b5ff-083401845c95');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-55b97131-4550-42f7-b41a-5c37f60782ec\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-55b97131-4550-42f7-b41a-5c37f60782ec')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-55b97131-4550-42f7-b41a-5c37f60782ec button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(medidas_tendencia)\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Qtd. Ve\\u00edculos (Real)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.635443667528429,\n        \"min\": 2.0,\n        \"max\": 20.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          7.0,\n          20.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Qtd. Ve\\u00edculos (Predito)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4.228819370269775,\n          9.1192626953125,\n          5.988323211669922\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                             Média   Mediana       Moda    Mínimo  \\\n",
              "Qtd. Veículos (Real)     93.388535  4.000000   1.000000  1.000000   \n",
              "Qtd. Veículos (Predito)  70.152779  6.579868  17.449841  0.638062   \n",
              "\n",
              "                               Máximo  \n",
              "Qtd. Veículos (Real)     30241.000000  \n",
              "Qtd. Veículos (Predito)  24258.685547  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-caaf9e08-79a9-408a-a5f4-f6010a9daaa6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Média</th>\n",
              "      <th>Mediana</th>\n",
              "      <th>Moda</th>\n",
              "      <th>Mínimo</th>\n",
              "      <th>Máximo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Qtd. Veículos (Real)</th>\n",
              "      <td>93.388535</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>30241.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Qtd. Veículos (Predito)</th>\n",
              "      <td>70.152779</td>\n",
              "      <td>6.579868</td>\n",
              "      <td>17.449841</td>\n",
              "      <td>0.638062</td>\n",
              "      <td>24258.685547</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-caaf9e08-79a9-408a-a5f4-f6010a9daaa6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-caaf9e08-79a9-408a-a5f4-f6010a9daaa6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-caaf9e08-79a9-408a-a5f4-f6010a9daaa6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-035349fe-467e-4703-9e69-2c607b1d8d34\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-035349fe-467e-4703-9e69-2c607b1d8d34')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-035349fe-467e-4703-9e69-2c607b1d8d34 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_c4aa2e25-ae51-488c-9099-32b55441a78a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('medidas_tendencia')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c4aa2e25-ae51-488c-9099-32b55441a78a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('medidas_tendencia');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "medidas_tendencia",
              "summary": "{\n  \"name\": \"medidas_tendencia\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"M\\u00e9dia\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.430160920935105,\n        \"min\": 70.15277862548828,\n        \"max\": 93.38853503184713,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          70.15277862548828,\n          93.38853503184713\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Mediana\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.8242423812718145,\n        \"min\": 4.0,\n        \"max\": 6.579868316650391,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          6.579868316650391,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Moda\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11.63179379926957,\n        \"min\": 1.0,\n        \"max\": 17.449840545654297,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          17.449840545654297,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"M\\u00ednimo\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2559291511496721,\n        \"min\": 0.6380615234375,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.6380615234375,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"M\\u00e1ximo\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4230.13511699498,\n        \"min\": 24258.685546875,\n        \"max\": 30241.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          24258.685546875,\n          30241.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6 - Criando o arquivo CSV(deve ser rodado para gerar o CSV)\n"
      ],
      "metadata": {
        "id": "xGMHHjvALWsM"
      },
      "id": "xGMHHjvALWsM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Download e carregamento do melhor modelo\n",
        "!gdown --fuzzy \"https://drive.google.com/file/d/1Msj-jml3WnTtk4QIi7uyT85EZuzOk7ke/view?usp=sharing\"\n",
        "\n",
        "# Melhor modelo\n",
        "model.load_state_dict(torch.load('/content/best_model_state_dict.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "z9LdXdKrL03r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "069748a6-615d-49ad-a68c-939c34308f1d"
      },
      "id": "z9LdXdKrL03r",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Msj-jml3WnTtk4QIi7uyT85EZuzOk7ke\n",
            "To: /content/best_model_state_dict.pth\n",
            "\r  0% 0.00/11.1k [00:00<?, ?B/s]\r100% 11.1k/11.1k [00:00<00:00, 40.5MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OptimizedModule(\n",
              "  (_orig_mod): LinearMostSimple(\n",
              "    (l1): Linear(in_features=22, out_features=18, bias=True)\n",
              "    (norm): BatchNorm1d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU()\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (l2): Linear(in_features=18, out_features=12, bias=True)\n",
              "    (norm1): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU()\n",
              "    (l3): Linear(in_features=12, out_features=6, bias=True)\n",
              "    (norm2): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu2): ReLU()\n",
              "    (l4): Linear(in_features=6, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_grouped_comp_mod = df_grouped_comp.copy()\n",
        "\n",
        "# Substitui o valores \"#DIV/0!\" por 0\n",
        "df_grouped_comp_mod = df_grouped_comp_mod.replace('#DIV/0!', 0)\n",
        "\n",
        "# Salvando o cod do municipio\n",
        "df_grouped_comp_cod = df_grouped_comp_mod['Cod_municipio'].copy()\n",
        "\n",
        "# Tratamento final dos dados\n",
        "for col in df_grouped_comp_mod.columns:\n",
        "    df_grouped_comp_mod[col] = pd.to_numeric(df_grouped_comp_mod[col]).astype('float64')\n",
        "\n",
        "# Removendo a coluna cod municipio\n",
        "df_grouped_comp_mod = df_grouped_comp_mod.drop(columns=['Cod_municipio'])\n",
        "\n",
        "# Normalizar dados, a normalização considera apenas o dataset de treino\n",
        "df_grouped_comp_norm = x_scaler.transform(df_grouped_comp_mod)\n",
        "\n",
        "# Transformando para torch\n",
        "df_grouped_comp_tensor = torch.tensor(df_grouped_comp_norm, dtype=torch.float32).to('cuda')\n",
        "\n",
        "# Predizer com o modelo\n",
        "y_pred_mlp = model(df_grouped_comp_tensor).cpu().detach().numpy()\n",
        "\n",
        "# Criar dataframe com apenas o cod municipio e o valor predito\n",
        "df_pred = pd.DataFrame({'Cod_municipio': df_grouped_comp_cod, 'Qtd_veiculos': y_pred_mlp.flatten()})\n",
        "\n",
        "# Transformar cod municipio para int\n",
        "df_pred['Cod_municipio'] = df_pred['Cod_municipio'].astype(int)\n",
        "\n",
        "# Gerar arquivo csv\n",
        "df_pred.to_csv('predicoes-modelo-dnn(parte-extra).csv', index=False)"
      ],
      "metadata": {
        "id": "0QXa0aUAmA3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0451e3c-dc52-4aa6-b21a-7eddc7afe238"
      },
      "id": "0QXa0aUAmA3N",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0626 23:01:51.087000 297 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "oABS6IdWihVV",
        "Rp41VhQ87c0A",
        "247zhUjboLkO",
        "3s8nsm6DoCkb",
        "XJrorFCpBDqd",
        "xGMHHjvALWsM"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}